"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7034],{40115:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/README","metadata":{"permalink":"/journal/README","source":"@site/../../journal/README.md","title":"Private Note or Documents","description":"Why Backend and DevOps Roles May Become One in the Future | HackerNoon","date":"2024-05-30T15:46:47.000Z","tags":[],"readingTime":4.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"tmp","permalink":"/journal/tmp"}},"content":"[Why Backend and DevOps Roles May Become One in the Future | HackerNoon](https://hackernoon.com/why-backend-and-devops-roles-may-become-one-in-the-future)\\n\\n[How to Put GitHub on Resume in 2023: Complete Guide & Tips](https://www.hiration.com/blog/github-on-resume/)\\n\\n## Job\\n\\nprone to errors\\n\\nbackend engineer\\ninfrastructure\\n\\nVocabulary:\\n- experienced\\n- precedence\\n\\n- promiscuous\\n- reimburse\\n- homogenous\\n- monotonous\\n- snoop\\n\\n## Terminology\\n\\n[Arithmetic](https://en.wikipedia.org/wiki/Arithmetic)\\n\\n## Tech Vocabulary\\n\\n- trivial program\\n- trial program\\n- experienced engineer\\n- division and multiplication **take precedence**\\n\\n## Mathematics Vocabulary\\n\\n- associative\\n- commutative\\n   a \xd7 b = b \xd7 a\\n   a + b = b + a\\n   a ^ b = b ^ a\\n- division\\n   `/`: divide\\n  `%`: mod\\n  dividend / divisor = quotient\\n  dividend % divisor = remainder\\n  dividend / divisor = (quotient) R (remainder) : dividend divided by the divisor has a quotient and a remainder\\n  5 / 3 = 1 R 2: \\n  5 divided by 3 has a quotient of 1 and a remainder of 2\\n\\n## Here\'s a category of topics commonly used in backend software engineering:\\n\\n\\nBackend software engineering interviews often cover a broad range of topics, including data structures, algorithms, system design, database management, networking, and more. Here\'s a categorization of some common terms and topics:\\n\\n1. **Data Structures**:\\n   - Arrays\\n   - Linked Lists\\n   - Stacks\\n   - Queues\\n   - Trees (Binary Trees, Binary Search Trees, AVL Trees, Red-Black Trees, etc.)\\n   - Graphs\\n   - Hash Tables\\n   - Heaps\\n\\n2. **Algorithms**:\\n   - Searching algorithms (Binary search, Linear search, Depth-first search, Breadth-first search)\\n   - Sorting algorithms (Quick sort, Merge sort, Bubble sort, Insertion sort)\\n   - Dynamic programming\\n   - Greedy algorithms\\n   - Recursion\\n   - Graph algorithms (Shortest path algorithms like Dijkstra\'s, Minimum Spanning Tree algorithms like Prim\'s and Kruskal\'s, etc.)\\n\\n3. **Database Management:**\\n   - Relational databases (SQL)\\n   - Non-relational databases (NoSQL)\\n   - ACID properties, transactions, and concurrency control\\n   - Indexing, normalization and denormalization\\n   - Query optimization and performance tuning\\n   - Database Migration\\n\\n4. **Architectural:**\\n   - Microservices\\n\\n5. **Security:**:\\n   - Authentication and authorization\\n   - Encryption\\n   - Man-in-the-middle attacks\\n\\n6. **System Design:**\\n   - Scalability\\n   - Load balancing\\n   - Caching strategies\\n   - Database sharding\\n   - Replication and fault tolerance\\n   - Microservices architecture\\n   - API design\\n   - Message queues and asynchronous processing\\n   - Proxy servers\\n\\n7. **Server:**\\n   - HTTP protocol, RESTful APIs\\n   - Web server concepts (e.g., Nginx, Apache)\\n   - Authentication and authorization (e.g., OAuth, JWT)\\n   - Session management and cookies\\n   - Web security best practices (e.g., SQL injection, XSS)\\n   - ORM\\n     - active record\\n     - data mapper\\n\\n8. **Concurrency and Parallelism:**\\n   - Multi-threading and synchronization\\n   - Locking mechanisms (e.g., mutexes, semaphores)\\n   - Concurrent data structures (e.g., concurrent hash maps, queues)\\n   - Parallel processing frameworks (e.g., MapReduce)\\n\\n9. **Programming Languages and Frameworks:**\\n   - Proficiency in at least one backend programming language (e.g., Java, Python, Go, Node.js)\\n   - Frameworks and libraries commonly used in backend development (e.g., Spring, Django, Express.js)\\n   - Understanding of asynchronous programming and event-driven architectures\\n\\n10. **Python:**\\n   - Data Structure\\n      - list\\n         - `append(v)`, `pop()`: O(1)\\n         - `insert(0, v)`, `pop(0)`: O(n)\\n      - dict\\n      - deque\\n         - `append(v)`, `pop()`: O(1)\\n         - `appendleft(v)`=`insert(0, v)`, `popleft()`=`pop(0)`: O(1)\\n   - Typing\\n   - Interactive shells\\n      - rich\\n      - IPython\\n      - bpython\\n      - ptpython\\n\\n11. **DevOps:**\\n   - Cloud platforms (e.g., AWS, Azure, Google Cloud Platform)\\n   - CI/CD pipelines\\n   - Containerization (e.g., Docker, Kubernetes)\\n   - Monitoring and logging solutions (e.g., Prometheus, ELK stack)\\n\\n12. **Testing:**\\n   - Unit testing\\n   - Integration testing\\n   - End-to-end testing\\n\\n13. **Debugging:**  \\n   - Debugging techniques and tools (e.g., logging, debugginggers)\\n   - Performance profiling and optimization\\n\\n14. **Software Development Practices:**\\n   - Version control systems (e.g., Git)\\n   - Continuous integration and continuous deployment (CI/CD)\\n   - Agile methodologies (e.g., Scrum, Kanban)\\n   - Code review processes and best practices\\n\\n15. **Cloud Computing and DevOps:**\\n   - Cloud platforms (e.g., AWS, Azure, Google Cloud Platform)\\n   - Infrastructure as Code (IaC) tools (e.g., Terraform, CloudFormation)\\n   - Containerization (e.g., Docker, Kubernetes)\\n   - Monitoring and logging solutions (e.g., Prometheus, ELK stack)\\n\\n16. **Other Topics:**\\n    - Design patterns (e.g., singleton, factory, observer)\\n    - Object-oriented design principles\\n    - Memory management and garbage collection\\n    - Networking concepts (e.g., TCP/IP, DNS)\\n\\n17. **Soft Skills:**\\n    - Collaboration\\n    - Problem-solving\\n    - Communication skills\\n    - Teamwork\\n\\nPreparation in these areas can significantly enhance your performance in backend software engineering interviews.\\n\\n## Tech interview\\n\\nhttps://www.techinterviewhandbook.org/\\n\\n## back-end developer interview questions and answers \\n\\nhttps://www.turing.com/interview-questions/back-end\\n\\nhttps://www.interviewkickstart.com/interview-questions/back-end-developer-interview-questions\\n\\nhttps://blog.hubspot.com/website/backend-interview-questions\\n\\n## Is C good choice of language for the interview?\\n\\nhttps://leetcode.com/discuss/general-discussion/536401/is-c-good-choice-of-language-to-have-a-technical-interview\\n\\nhttps://www.linkedin.com/pulse/c-vs-cjavapython-interviews-tushar-dwivedi\\n\\n\\n## Moving company\\n- Finding **a cheap yet reliable** shipping or moving company can be challenging.\\n- Moving from Hong Kong to Canada: Any Recommendations for the Best Affordable Moving Company?\\n\\nrelocation from Hong Kong to Canada\\nremovals to Canada\\nOrganization and a good moving plan are **indispensable** parts of every move\\n\\n\\nhttps://www.transworldrelocation.com/zh-hk/services\\nhttps://www.sevenseasworldwide.com/\\nhttps://www.crownrelo.com/hong-kong/zh-hk/get-a-quote\\n\\n## GPT 3.5 vs GPT 4.0\\n\\n3.5 is used for general purpose, creating an essay.\\n4.0 is more analytical and more logical and more precise.\\n\\nI use 3.5 to create general letters, essays, posts, etc.\\n4.0 is more for deeper questions & outputs.\\n\\n3.5 is much faster than 4.0.\\n\\n\\n[In-Depth Comparison: GPT-4 vs GPT-3.5 \u2013 KanariesDiscord](https://docs.kanaries.net/articles/compare-gpt-4-gpt-3)\\n\\n\\nforemost\\n\\nC is a good language to learn for a career. \\n\\n. It is a foundational/**primitive** language that is still widely used in areas such as operating system, compiler/interpreter and low-level libraries.\\n\\n\\nThanks for listening to my rant.\\n\\n\\n[Ben Hoyt\u2019s Resume/CV](https://benhoyt.com/cv/)\\n\\n## Tech\\n\\nhttps://youtube.com/clip/UgkxedRh7NJJliritCfi-oVUzunSBZavWahd?si=x2W5qQvvHp_IvrR1"},{"id":"/tmp","metadata":{"permalink":"/journal/tmp","source":"@site/../../journal/tmp.md","title":"tmp","description":"https://stackoverflow.com/questions/52429984/docker-compose-build-environment-variable","date":"2024-05-30T15:46:47.000Z","tags":[],"readingTime":0.385,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Private Note or Documents","permalink":"/journal/README"},"nextItem":{"title":"Thursday, April 25, 2024","permalink":"/journal/2024/04/25/"}},"content":"https://stackoverflow.com/questions/52429984/docker-compose-build-environment-variable\\n\\nuse case:\\nbecause the frontend applications now always  are needed to build into the page app or static page app then run on the client side, if you want to use env variables to customize the app for different usage such as a different server address for dev and prod use, you can only use the env variables in the building stage. for quick spinning up the app, you shouldn\'t invoke building when every time running"},{"id":"/2024/04/25/","metadata":{"permalink":"/journal/2024/04/25/","source":"@site/../../journal/2024-04-25.md","title":"Thursday, April 25, 2024","description":"Travel mugs for dishwasher safer","date":"2024-04-25T00:00:00.000Z","tags":[],"readingTime":1.085,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"tmp","permalink":"/journal/tmp"},"nextItem":{"title":"Friday, April 19, 2024","permalink":"/journal/2024/04/19/"}},"content":"## Travel mugs for dishwasher safer\\n\\nBut there is nothing that will bring you down off the high of a good meal faster than seeing the dishes stacked up in your sink. If you have a dishwasher in that moment, it can be your **saving grace**, but it can not do it all.\\n\\nA sink full of plates and glasses can get loaded into the standard dishwasher and come out looking spotless. However, there are still plenty of items that you should be washing by hand. Cast iron or non stick pans can get ruined in dishwashers, wooden spoons can warp and crack, and knives can be damaged and dull.\\n\\n### Insulted travel mug\\n\\nAlso on the list is your favorite **insulted travel mug**. \\n\\nThe reason for this, according to Report, is that the hight water pressure and extreme heat cycles can damage their precious vacuum seal -- and if the seal is damaged, your favorite travel mug will lose its insulting properties.\\n\\n### Stainless steel mug\\n\\nThe **Contigo** Travel Mug User Guide states the following:\\n- Lid and any UNPAINTED stainless steel body are top-rack dishwasher safe. HAND-WASH ONLY any PAINTED stainless steel body.\\n\\n\\nlongevity\\n\\n## FAQs\\n\\n### Is the dishwashing powder safe for the travel mug?\\n\\n\\n[Why You Shouldn\'t Put Travel Mugs In The Dishwasher](https://www.tastingtable.com/861624/why-you-shouldnt-put-travel-mugs-in-the-dishwasher/)"},{"id":"/2024/04/19/","metadata":{"permalink":"/journal/2024/04/19/","source":"@site/../../journal/2024-04-19.md","title":"Friday, April 19, 2024","description":"Docker compose for development and production","date":"2024-04-19T00:00:00.000Z","tags":[],"readingTime":3.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Thursday, April 25, 2024","permalink":"/journal/2024/04/25/"},"nextItem":{"title":"Thursday, April 18, 2024","permalink":"/journal/2024/04/18/"}},"content":"## Docker compose for development and production\\n\\nUnderstanding the nuances between development (`dev`) and production (`prod`) environments is crucial for efficient operations:\\n\\n1. **Database Infrastructure:**\\n   - In development, engineers typically rely on local MySQL or MongoDB instances. However, for production-grade reliability and scalability, businesses often leverage managed database services provided by leading cloud platforms. As a result, the configuration of environment variables for database connections varies between the two environments.\\n\\n2. **Resource Allocation:**\\n   - Development machines typically possess limited computing resources compared to production servers. Consequently, it\'s essential to optimize resource allocation during development by minimizing unnecessary services or configurations. This ensures efficient performance and cost-effectiveness.\\n\\n3. **Building Strategies:**\\n   - During development, engineers build and run Docker images locally to streamline the development process. Conversely, for production, a structured approach is adopted. This involves building and publishing images through Continuous Integration (CI) pipelines. Subsequently, the pre-built images are pulled from a registry for execution in the production environment.\\n\\nGiven these considerations, a one-size-fits-all approach, such as simply overriding configurations from a base Docker Compose file, may not suffice to address the unique requirements of each environment.\\n\\nIn summary, while Docker Compose serves as a valuable tool for local development and testing, its utility in production deployment may be limited. To meet the evolving needs of a growing application, businesses are encouraged to explore advanced deployment solutions, such as cloud-based Kubernetes pods, for scalable and resilient operations.\\n\\n\\n[Docker Compose best practices for dev and prod | Hacker News](https://news.ycombinator.com/item?id=32484008)\\n\\n## Deploying a local Docker compose project for production\\n\\none common way to do it without CD/CI pipeline, is to:\\n\\n1. build development images by using different/override docker-compose files and use .env that can be adapted for every environment.\\n2. push stable images to a container registry\\n3. docker pull the images on the prod.\\n4. run the containers\\n\\n[Reddit - Dive into anything](https://www.reddit.com/r/docker/comments/s0fn94/how_to_deploy_a_local_docker_compose_project_to_a/)\\n\\n## How to automate deployment from development to production using Docker Compose\\n\\nOrganizing development and production configurations in Docker Compose can be achieved using various strategies, and the choice depends on your specific requirements and preferences. Here are two common approaches:\\n\\n1. **Using Two Docker Compose Files:**\\n   - **Development:** Create a `docker-compose.yml` file tailored for development purposes. This file can include options for volumes, environment variables, ports, and other settings that facilitate the development workflow, such as live code reloading.\\n   - **Production:** Create a separate `docker-compose.prod.yml` file for production settings. This file should include configurations optimized for production, such as specifying production-ready images and environment variables suited for the production environment.\\n\\n   To deploy in production, you can use the `-f` flag to specify the production Docker Compose file:\\n   ```sh\\n   docker-compose -f docker-compose.prod.yml up -d\\n   ```\\n\\n   This approach offers clear separation between development and production configurations and makes it easier to manage different settings for each environment.\\n\\n2. **Using Different Environment Files:**\\n   - Maintain a single Docker Compose file (`docker-compose.yml`) that defines the services and their basic configurations.\\n   - Use different environment files for development (`docker-compose.override.yml`) and production (`docker-compose.prod.yml`). These environment files can override configurations defined in the base `docker-compose.yml`.\\n   - In the development environment file, you can specify volumes, environment variables for development-specific settings, and other configurations suited for development.\\n   - In the production environment file, you can specify production-ready image versions, environment variables for production, and any other configurations optimized for the production environment.\\n\\n   To run in development:\\n   ```sh\\n   docker-compose up -d\\n   ```\\n\\n   To run in production:\\n   ```sh\\n   docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\\n   ```\\n\\n   This approach keeps all configurations in a single Docker Compose file, with environment-specific settings separated into different files. It provides flexibility and simplifies deployment by allowing you to specify the environment at runtime.\\n\\nBoth approaches have their merits, and the choice depends on factors such as the complexity of your project, team preferences, and deployment workflows. Whichever approach you choose, ensure that your configurations are well-organized, easy to maintain, and provide the necessary flexibility for both development and production environments.\\n\\n## Setting up CI/CD with Docker Compose"},{"id":"/2024/04/18/","metadata":{"permalink":"/journal/2024/04/18/","source":"@site/../../journal/2024-04-18.md","title":"Thursday, April 18, 2024","description":"Main features when writing in Word","date":"2024-04-18T00:00:00.000Z","tags":[],"readingTime":0.235,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Friday, April 19, 2024","permalink":"/journal/2024/04/19/"},"nextItem":{"title":"Thursday, April 17, 2024","permalink":"/journal/2024/04/17/"}},"content":"## Main features when writing in Word\\n\\nBasic features to learn in using Word:\\n\\n1. TOC\\n1. Number the headings and subheadings:\\n  1. create new **Multilevel List** for heading 1, heading 2, heading 3, etc.\\n  2. [MS Word: How-to use heading styles and automated numbering - YouTube](https://www.youtube.com/watch?v=lYvcwGc5FQs)"},{"id":"/2024/04/17/","metadata":{"permalink":"/journal/2024/04/17/","source":"@site/../../journal/2024-04-17.md","title":"Thursday, April 17, 2024","description":"PDF vs Word","date":"2024-04-17T00:00:00.000Z","tags":[],"readingTime":1.745,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Thursday, April 18, 2024","permalink":"/journal/2024/04/18/"},"nextItem":{"title":"Tuesday, April 16, 2024","permalink":"/journal/2024/04/16/"}},"content":"## PDF vs Word\\n\\nPDF: Fixed layout, non-editable.\\nWord: Editable, variable layout.\\n\\n## PDF vs JPEG image\\n\\nIndeed, **PDF** is primarily optimized for text-based content with vector graphics, while images, particularly in formats like **JPEG**, excel at representing complex visual content like photographs or intricate illustrations. If the content is highly visual or complex, using an image format like **JPEG** may be more appropriate, even if it results in a larger file size.\\n\\n## Which is more efficient to store text? PDF or JPEG?\\n\\nIn terms of file size efficiency, **PDF** is generally smaller as it stores text as **vector data** or **text macros**, whereas **JPEG** stores text as **pixels**. So, for the same text on the same layout, a PDF would typically be smaller in file size compared to a JPEG image.\\n\\n## Layout view\\n\\nIn a **layout view**, from large to small:\\n\\n- **Window:** A graphical area on a screen that displays content or user interface elements. It can contain one or more panes, panels, or tabs, depending on the application\'s design.\\n\\n- **Pane:** A division within a window that separates and displays different sections of content. Panes can be resizable and can contain various elements such as panels, tabs, or other UI components.\\n\\n- **Panel:** A specific area within a pane that typically contains controls, information, or tools related to a particular aspect of the application or document being viewed.\\n\\n- **Tab:** A navigational element often used to switch between multiple documents, views, or sections within a single window. Each tab typically represents a separate content area or document within the same window.\\n\\nOther terms related to layout views might include:\\n- **Frame:** A designated area within a window or pane where content can be displayed or arranged.\\n- **Viewport:** The visible area of a document or interface within a window or pane, especially when the content is larger than the available display space.\\n\\n```sh\\n[Window]\\n   |\\n   |--[Pane]\\n   |    |\\n   |    |--[Panel]\\n   |    :\\n   |    \\n   |--[Pane]\\n   |    |\\n   |    |--[Panel]\\n   |    |\\n   |    |--[Panel]\\n   |         |\\n   |         |--[Tab]\\n   |         |\\n   |         |--[Tab]\\n   |         :\\n   :\\n\\n```"},{"id":"/2024/04/16/","metadata":{"permalink":"/journal/2024/04/16/","source":"@site/../../journal/2024-04-16.md","title":"Tuesday, April 16, 2024","description":"iPhone Photo Backup in External Storage & macOS Photos App Management","date":"2024-04-16T00:00:00.000Z","tags":[],"readingTime":0.91,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Thursday, April 17, 2024","permalink":"/journal/2024/04/17/"},"nextItem":{"title":"Monday, April 15, 2024","permalink":"/journal/2024/04/15/"}},"content":"## iPhone Photo Backup in External Storage & macOS Photos App Management\\n\\nBackup photos into macOS Photos Library in external storage, while you can manage the macOS Photos Library\\n\\nHere is how I streamline photo management in my iphone:\\n1. Prepare your storage device: it\'s recommended to format the storage device for Mac: APFS format.\\n1. Create Photos Library on External SSD: a Photos Library folder contains all data needed to manage the photos\\n2. Move photos in iphone by the macOS Photos app: These photos will be backup to the Photos Library on External SSD\\n\\n[Move your Photos library to save space on your Mac - Apple Support](https://support.apple.com/en-us/108345)\\n\\n\\n## Backup Photo into NAS\\n\\n1. Setup a home NAS.\\n2. Upload your original photos from iPhone to a NAS folder.\\n3. A background sync application will organize these photos into separate folders based on years/months.\\n  1. The app can read metadata in photos.\\n  2. The app can copy full data in photos.\\n  3. The app can skip copying if finding same file.\\n  4. The app can keep the photo file created/modified timestamp."},{"id":"/2024/04/15/","metadata":{"permalink":"/journal/2024/04/15/","source":"@site/../../journal/2024-04-15.md","title":"Monday, April 15, 2024","description":"Cylinder-Head-Sector (CHS)","date":"2024-04-15T00:00:00.000Z","tags":[],"readingTime":6.715,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Tuesday, April 16, 2024","permalink":"/journal/2024/04/16/"},"nextItem":{"title":"Friday, April 12, 2024","permalink":"/journal/2024/04/12/"}},"content":"## Cylinder-Head-Sector (CHS)\\n\\n[Cylinder, head, and sector of a hard drive](https://en.wikipedia.org/wiki/Cylinder-head-sector)\\n\\n## Check **S.M.A.R.T.** information of a disk in macOS\\n\\n```sh\\nbrew install smartmontools\\n\\nsmartctl -a disk0s3\\n```\\n\\n\\n## Repair storage drive in Windows\\n\\nRepairing an storage drive in PowerShell usually involves checking and repairing its file system. PowerShell provides cmdlets for performing file system checks and repairs. Here\'s a basic approach to repair an external drive using PowerShell:\\n\\n1. **Check Disk for Errors**: Use the `Repair-Volume` cmdlet to check the external drive for errors. This cmdlet scans the file system and fixes any issues it finds.\\n\\n```powershell\\nRepair-Volume -DriveLetter E\\n```\\n\\nReplace `E` with the drive letter of your external drive.\\n\\n2. **Check File System Integrity**: You can also use `chkdsk` (Check Disk) command-line tool via PowerShell to check and repair file system errors. Run the following command:\\n\\n```powershell\\nchkdsk /f /r /x E:\\n```\\n\\nReplace `E:` with the drive letter of your external drive.\\n\\n- `/f`: Fixes errors on the disk.\\n- `/r`: Locates bad sectors and recovers readable information.\\n- `/x`: Forces the volume to dismount first if necessary. \\n\\nPlease note that repairing an external drive could result in data loss, so it\'s recommended to back up important data before proceeding. Additionally, you might need to run PowerShell with administrative privileges (`Run as administrator`) for these commands to work properly.\\n\\n## What are bad sectors in the disk?\\n\\nBad sectors, also known as bad blocks, are sections on a disk drive that are physically damaged or unable to reliably store data. They can occur on various types of storage media, including hard disk drives (HDDs), solid-state drives (SSDs), USB drives, SD cards, and more. \\n\\nThere are two types of bad sectors:\\n\\n1. **Logical Bad Blocks**: Logical bad blocks occur due to software issues or file system errors. These blocks may become inaccessible or unusable due to file system corruption, improper shutdowns, or other software-related issues. Running disk repair tools or performing file system checks can often resolve logical bad blocks.\\n\\n2. **Physical Bad Blocks**: Physical bad blocks are caused by physical damage to the disk surface or storage medium. These blocks cannot reliably store data and may lead to data loss or corruption. Physical bad blocks may occur due to factors such as manufacturing defects, wear and tear over time, exposure to environmental factors like heat or moisture, or physical shocks.\\n\\nIt\'s important to address bad blocks on a disk drive as they can lead to data loss, system instability, or hardware failure. Regularly scanning for and repairing bad blocks, as well as maintaining up-to-date backups of important data, can help mitigate the risks associated with disk drive issues.\\n\\n## How to repair bad sectors?\\n\\nBad sectors, also known as bad blocks, are sections of a disk drive\'s surface that are physically damaged or malfunctioning. When a sector becomes bad, it may no longer reliably store data, leading to potential data loss or corruption. Repairing bad sectors involves attempting to either recover or mark these problematic sectors to prevent data from being written to them in the future.\\n\\nThere are two main approaches to repairing bad sectors:\\n\\n1. **Recovery**: In some cases, data stored in a bad sector can still be recovered. Disk repair tools may attempt to read the data from a bad sector multiple times to recover as much information as possible. If successful, the data can then be relocated to a good sector on the disk.\\n\\n2. **Marking**: If data recovery is not possible or if the sector is too damaged to reliably store data, the bad sector can be marked as unusable. This process involves informing the disk\'s file system that the sector is bad, preventing data from being written to it in the future. The disk\'s firmware may also remap the bad sector to a spare sector on the disk if available.\\n\\nIt\'s important to note that while marking bad sectors can prevent further data loss, it does not repair the physical damage to the disk. Over time, additional bad sectors may develop, especially on aging or damaged disk drives. Regularly scanning for and repairing bad sectors, as well as maintaining up-to-date backups of important data, can help mitigate the risks associated with disk drive issues.\\n\\n## Data recovery tools\\n\\nYes, there are several free data recovery tools available that offer similar functionality to EaseUS Data Recovery Wizard. Here are a few options:\\n\\n1. **DMDE**:\\n\\n2. **Recuva**: Recuva is a popular and user-friendly data recovery tool developed by Piriform (now owned by CCleaner). It can recover deleted files from hard drives, SSDs, USB drives, memory cards, and more. Recuva offers both a free version and a paid version with additional features.\\n\\n3. **PhotoRec**: PhotoRec is an open-source data recovery utility that specializes in recovering lost files, including photos, videos, documents, and archives, from a wide range of storage devices. It\'s available for Windows, macOS, and Linux.\\n\\n4. **TestDisk**: TestDisk is a powerful open-source tool for recovering lost partitions and fixing disk-related issues. It works alongside PhotoRec and can help recover lost partitions and repair damaged file systems.\\n\\n5. **Disk Drill**: Disk Drill offers a free version that allows you to recover up to 500 MB of data for free. It supports file recovery from various storage devices, including hard drives, SSDs, USB drives, and memory cards. Disk Drill is available for Windows and macOS.\\n\\n6. **Puran File Recovery**: Puran File Recovery is a lightweight and easy-to-use data recovery tool that can recover deleted files from hard drives, USB drives, memory cards, and other storage devices. It offers a simple interface and is available for Windows.\\n\\nThese are just a few examples of free data recovery tools available. Each tool has its own set of features, limitations, and compatibility with different storage devices and file systems. It\'s essential to research and choose the tool that best fits your specific data recovery needs. Additionally, remember to always use data recovery tools with caution and avoid writing new data to the storage device until you\'ve completed the recovery process to minimize the risk of overwriting lost data.\\n\\n## Disk Repair Tools\\n\\n### Disk Drill\\n\\nDisk Drill is a legitimate data recovery application developed by CleverFiles. It\'s widely used and has generally positive reviews from users and technology publications. The application is known for its ease of use and effectiveness in recovering deleted files from various storage devices such as hard drives, USB drives, memory cards, and more.\\n\\nHowever, it\'s essential to download Disk Drill from the official CleverFiles website or reputable app stores to ensure that you\'re getting the legitimate version of the software. Like any software, there\'s always a risk of downloading from unofficial sources, which may lead to malware or other security issues.\\n\\nAlways be cautious when downloading and installing software, and ensure that you\'re using a reputable source to obtain the application. Additionally, it\'s advisable to read user reviews and research the software before downloading to ensure that it meets your needs and is trustworthy.\\n\\n### DiskGenius\\n\\nDiskGenius is a reputable disk management and data recovery software developed by Eassos Ltd. It offers a wide range of features, including partition management, data backup, data recovery, disk cloning, and more. \\n\\nDiskGenius has generally positive reviews from users and technology publications, and it\'s known for its effectiveness in recovering lost or deleted files, repairing disk errors, and managing disk partitions. \\n\\nHowever, like any software, its reliability can depend on various factors, including the specific use case, the condition of the disk being worked on, and user experience. It\'s essential to download DiskGenius from the official website or reputable sources to ensure that you\'re getting the legitimate version of the software.\\n\\nBefore using any data recovery or disk management software, it\'s a good idea to research user reviews, check for any known issues or limitations, and ensure that the software meets your specific needs. Additionally, always make sure to back up your important data before performing any disk-related operations to prevent data loss.\\n\\n\\n## Resources\\n\\n[How to Check and Repair Bad Sectors for Hard Drives or USB Drives?](https://www.diskgenius.com/how-to/bad-sector-repair-software.php)"},{"id":"/2024/04/12/","metadata":{"permalink":"/journal/2024/04/12/","source":"@site/../../journal/2024-04-12.md","title":"Friday, April 12, 2024","description":"Docker Build Args vs Runtime Env Variables: Optimizing Frontend Deployment","date":"2024-04-12T00:00:00.000Z","tags":[],"readingTime":0.725,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Monday, April 15, 2024","permalink":"/journal/2024/04/15/"},"nextItem":{"title":"Sunday, April 7, 2024","permalink":"/journal/2024/04/07/"}},"content":"## Docker Build Args vs Runtime Env Variables: Optimizing Frontend Deployment\\n\\nDuring the building stage, environment variables are passed as arguments to the Docker image, influencing the build process. Then, during container execution, environment variables are set within the running container, ensuring customization without the need for rebuilding the image each time the application is spun up.\\n\\n```dockerfile\\nFROM mhart/alpine-node:10\\nARG NODE_ENV\\nENV NODE_ENV $NODE_ENV\\nADD . /app\\nWORKDIR /app\\n\\n# install dependencies\\nCOPY app/package.json app/package-lock.json ./\\nRUN npm install\\nRUN yarn build\\n\\nEXPOSE 7000\\n\\nCMD [\\"yarn\\", \\"start\\"]\\n```\\n\\n```yaml\\nbuild:\\n  context: .\\n  dockerfile: Dockerfile-preprod\\n  args:\\n    - NODE_ENV=${NODE_ENV}\\n```\\n\\nThe use case is:\\n\\nFor frontend apps, building into either page or static formats is standard, followed by client-side execution. To customize with env variables, like server addresses for dev or prod, they\'re set during building. Avoiding rebuilding for every runtime spin-up ensures swift deployment.\\n\\n\\nhttps://stackoverflow.com/questions/52429984/docker-compose-build-environment-variable"},{"id":"/2024/04/07/","metadata":{"permalink":"/journal/2024/04/07/","source":"@site/../../journal/2024-04-07.md","title":"Sunday, April 7, 2024","description":"TOTP application","date":"2024-04-07T00:00:00.000Z","tags":[],"readingTime":4.89,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Friday, April 12, 2024","permalink":"/journal/2024/04/12/"},"nextItem":{"title":"Saturday, April 6, 2024","permalink":"/journal/2024/04/06/"}},"content":"## TOTP application\\n\\nTOTP stands for Time-Based One-Time Password. It\'s a form of two-factor authentication that generates a unique, temporary password that is used along with a regular password for added security. TOTP codes are usually generated by mobile apps or hardware tokens.\\n\\nHere\'s how a TOTP application typically works:\\n\\n1. **Setup**: Initially, you\'ll need to set up two-factor authentication on the service or platform you want to secure. This usually involves enabling TOTP authentication and scanning a QR code or entering a secret key into your TOTP application.\\n\\n2. **Generating Codes**: Once set up, the TOTP application (such as Google Authenticator, Authy, or Microsoft Authenticator) will continuously generate new, time-based codes. These codes are typically 6 to 8 digits long and change every 30 seconds or so.\\n\\n3. **Authentication**: When logging into the service or platform, you\'ll be prompted to enter a TOTP code along with your regular password. You\'ll retrieve the current code from your TOTP application and enter it within the allotted time window.\\n\\n4. **Validation**: The service or platform will then validate the code you entered against the expected code generated by their system. If they match, you\'ll be granted access.\\n\\n5. **Continuous Use**: Every time you log in, you\'ll need to provide a new TOTP code from your application. This adds an extra layer of security because even if someone were to obtain your regular password, they wouldn\'t be able to access your account without the TOTP code.\\n\\nRemember, it\'s important to keep your TOTP application and the device it\'s installed on secure to prevent unauthorized access to your accounts. Additionally, it\'s a good idea to have backup methods of authentication in case you lose access to your TOTP application or device. This might include backup codes provided by the service or platform, or alternative authentication methods like SMS or email verification.\\n\\n## TOTP URI scheme\\n\\nThe text \\"otpauth://totp\\" typically serves as a URI scheme used to represent TOTP (Time-Based One-Time Password) parameters in a standardized format. This format is commonly used for sharing TOTP configuration data between applications, such as when setting up two-factor authentication on a new device.\\n\\n```sh\\notpauth://totp/Example:alice@example.com?secret=JBSWY3DPEHPK3PXP&issuer=Example&algorithm=SHA256&digits=6&period=30\\n```\\n\\nFollowing \\"otpauth://totp\\", there is typically additional information encoded in the URI, including:\\n\\n1. **Label**: This identifies what the TOTP code is for, such as the name of the service or account.\\n2. **Issuer (Optional)**: This specifies the provider or issuer of the TOTP code.\\n3. **Secret**: This is a unique secret key used to generate the TOTP codes.\\n4. **Algorithm (Optional)**: This specifies the algorithm used to generate the codes, usually HMAC-SHA1, HMAC-SHA256, or HMAC-SHA512.\\n5. **Digits (Optional)**: This specifies the number of digits in the generated TOTP codes, typically 6 or 8.\\n6. **Period (Optional)**: This specifies the time period (in seconds) for which a TOTP code is valid, usually 30 seconds.\\n7. **Counter (Optional)**: This is an alternative to the time-based approach, specifying a counter value for generating TOTP codes.\\n\\nThis URI would be interpreted by a TOTP-compatible application to set up a TOTP configuration for an account named \\"Example\\" belonging to \\"alice@example.com\\". It specifies a secret key, SHA-256 algorithm, 6-digit codes, and a 30-second period.\\n\\nYou can use this URI to easily configure a TOTP application by scanning a QR code or manually entering the information into the app. This helps streamline the setup process for two-factor authentication on various platforms.\\n\\n\\n## How TOTP application generate TOTP codes?\\n\\nTOTP application has no need to know the issuer ip address.\\n\\nThe TOTP application just use URI (`otpauth://TOTP/...`), as the URI contains all the necessary information for the TOTP application to generate TOTP codes.\\n\\nHere\'s a simplified explanation of how you can generate TOTP codes based on the shared secret:\\n\\n1. **Convert the secret from base32 encoding**: The secret provided in the URI (`JBSWY3DPEHPK3PXP`) is typically base32 encoded. You\'ll need to decode it to its raw binary form.\\n\\n2. **Determine the current time**: TOTP codes are time-based, so you need to determine the current time in the same time unit as specified in the URI (`30 seconds` in this case). This is typically Unix time (number of seconds since January 1, 1970).\\n\\n3. **Calculate the counter**: The counter value is derived from the current time divided by the time period specified in the URI. This represents the number of time steps that have occurred since the TOTP epoch.\\n\\n4. **Hash the counter with the secret**: Use the HMAC-SHA algorithm (specified in the URI) to hash the counter value with the shared secret. This produces a hash value.\\n\\n5. **Extract the dynamic truncation offset**: TOTP uses a dynamic truncation offset to extract a 4-byte dynamic binary code from the hash. This offset is determined by the last 4 bits of the hash value.\\n\\n6. **Generate the OTP**: Take the dynamic binary code and convert it to a numeric code. This is usually done by taking the last 6 or 8 bits of the dynamic code and converting it to a decimal number.\\n\\n7. **Format the OTP**: If necessary, format the OTP code to the specified number of digits (6 digits in this case).\\n\\nHere\'s an example implementation in Python using the `pyotp` library:\\n\\n```python\\nimport time\\nimport base64\\nimport hmac\\nimport hashlib\\nimport struct\\nimport pyotp\\n\\nuri = \\"otpauth://totp/Example:alice@example.com?secret=JBSWY3DPEHPK3PXP&issuer=Example&algorithm=SHA256&digits=6&period=30\\"\\n\\n# Parse URI to extract parameters\\nparams = pyotp.parse_uri(uri)\\nsecret = base64.b32decode(params[\\"secret\\"])\\ndigits = params[\\"digits\\"]\\nperiod = params[\\"period\\"]\\n\\n# Generate TOTP code\\nepoch = time.time()\\ncounter = int(epoch) // period\\ncounter_bytes = struct.pack(\\">Q\\", counter)\\nhash_value = hmac.new(secret, counter_bytes, hashlib.sha256).digest()\\noffset = hash_value[-1] & 0x0F\\ndynamic_code = struct.unpack(\\">I\\", hash_value[offset:offset+4])[0] & 0x7FFFFFFF\\notp = str(dynamic_code % (10 ** digits)).zfill(digits)\\n\\nprint(\\"TOTP code:\\", otp)\\n```\\n\\nThis code snippet demonstrates a basic TOTP code generation process. However, it\'s essential to use a trusted library for generating TOTP codes in production scenarios due to security implications. The `pyotp` library is widely used and reputable for this purpose."},{"id":"/2024/04/06/","metadata":{"permalink":"/journal/2024/04/06/","source":"@site/../../journal/2024-04-06.md","title":"Saturday, April 6, 2024","description":"Globally ignore .DS_Store from all the git repos in macOS","date":"2024-04-06T00:00:00.000Z","tags":[],"readingTime":0.42,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Sunday, April 7, 2024","permalink":"/journal/2024/04/07/"},"nextItem":{"title":"Friday, April 5, 2024","permalink":"/journal/2024/04/05/"}},"content":"## Globally ignore `.DS_Store` from all the git repos in `macOS`\\n\\n```sh\\n# remove any existing files from the repo, skipping over ones not in repo\\nfind . -name .DS_Store -print0 | xargs -0 git rm --ignore-unmatch\\n# specify a global exclusion list\\ngit config --global core.excludesfile ~/.gitignore\\n# adding .DS_Store to that list\\necho .DS_Store >> ~/.gitignore\\n```\\n\\n[macos - How can I Remove .DS_Store files from a Git repository? - Stack Overflow](https://stackoverflow.com/questions/107701/how-can-i-remove-ds-store-files-from-a-git-repository)\\n\\n## List Git remote branches\\n\\n```sh\\ngit branch -r\\n```"},{"id":"/2024/04/05/","metadata":{"permalink":"/journal/2024/04/05/","source":"@site/../../journal/2024-04-05.md","title":"Friday, April 5, 2024","description":"What is the relation between filesystem, volume, partition and disk? What are they?","date":"2024-04-05T00:00:00.000Z","tags":[],"readingTime":2.775,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Saturday, April 6, 2024","permalink":"/journal/2024/04/06/"},"nextItem":{"title":"Monday, March 25, 2024","permalink":"/journal/2024/03/25/"}},"content":"## What is the relation between filesystem, volume, partition and disk? What are they?\\n\\nFilesystem, volume, partition, and disk are fundamental components within the realm of computer storage, particularly in the context of hard disk drives (HDDs) and solid-state drives (SSDs). Here\'s an explanation of each and their relationship:\\n\\n1. **Disk**: A disk, in the context of computing, refers to the physical storage device, such as a hard disk drive (HDD) or a solid-state drive (SSD). It consists of one or more platters (in HDDs) or memory chips (in SSDs) on which data is stored.\\n\\n2. **Partition**: A partition is a logical division of a physical disk. It involves dividing the disk into separate sections that the operating system treats as independent units. Each partition can be formatted with a particular filesystem and can contain its own set of files and directories. Partitions are useful for various purposes, such as organizing data, separating operating system files from user files, or hosting multiple operating systems on a single disk.\\n\\n3. **Volume**: A volume is a logical storage unit that can span one or more partitions. It represents a formatted section of a disk that is accessible to the operating system and applications as a single entity. Volumes are typically associated with a specific filesystem, which determines how data is organized and stored on the volume. In simpler terms, a volume is what you see and interact with when you access a storage device (e.g., drive letters like C:, D:, etc., in Windows, or mount points in Unix-like systems).\\n\\n4. **Filesystem**: A filesystem is a method of organizing and storing files and directories on a storage device. It provides a structure for accessing, reading, and writing data stored on the disk. Filesystems define how data is stored, named, and accessed, including aspects such as directory structure, file naming conventions, and metadata storage. Examples of filesystems include NTFS (used in Windows), ext4 (used in many Linux distributions), and APFS (used in macOS).\\n\\nRelationship:\\n\\n- A disk is the physical storage medium.\\n- A disk can be divided into one or more partitions.\\n- Each partition can be formatted with a filesystem.\\n- Each formatted partition represents a volume, which is the accessible storage unit seen by the operating system and users.\\n\\nIn summary, partitions divide disks into manageable sections, volumes represent accessible storage units within those partitions, and filesystems dictate how data is organized and stored within volumes. Together, they form the foundation of storage management in computer systems.\\n\\n## What is the distinction between \\"drive\\" and \\"disk\\"?\\n\\n1. **Drive**: Refers to the entire storage unit, including both the physical disk (in the case of HDDs) or memory chips (in the case of SSDs), as well as any associated hardware components responsible for reading from and writing to the storage medium. This includes not only the memory or storage components themselves but also components such as controllers, interfaces, and connectors.\\n\\n2. **Disk**: Often refers specifically to the physical storage medium itself, whether it\'s the spinning platters inside a hard disk drive (HDD) or the memory chips inside a solid-state drive (SSD). It typically refers to the component where data is stored magnetically (in HDDs) or electronically (in SSDs), excluding other components such as controllers or interfaces.\\n\\nIn common usage, the terms **Drive** and **Disk** are often used interchangeable."},{"id":"/2024/03/25/","metadata":{"permalink":"/journal/2024/03/25/","source":"@site/../../journal/2024-03-25.md","title":"Monday, March 25, 2024","description":"Solid State Drive (SSD) Form Factors","date":"2024-03-25T00:00:00.000Z","tags":[],"readingTime":1.105,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Friday, April 5, 2024","permalink":"/journal/2024/04/05/"},"nextItem":{"title":"Monday, August 14, 2023","permalink":"/journal/2023/08/14/"}},"content":"## Solid State Drive (SSD) Form Factors\\n\\nSSD components:\\n\\n- Controller chips\\n- Memory chips\\n  - **NAND** Flash particles\\n    - SLC (1 bit/cell)\\n    - MLC (2 bits/cell)\\n    - TLC (3 bits/cell)\\n    - QLC (4 bits/cell)\\n  - Manufactures\\n    - Samsung\\n    - SanDisk\\n    - Micron\\n    - SK Hynix\\n    - Toshiba\\n- Interfaces\\n  - SATA (Serial Advanced Technology Attachment)\\n    - Size\\n      - M.2\\n    - Protocols\\n      - AHCI\\n  - PCIe (PCIe 5.0, 4.0, 3.0)\\n    - Size\\n      - M.2\\n    - Protocols\\n      - AHCI (rarely!)\\n      - NVMe\\n\\n> NOTE\\n> Data Transfer Protocols:\\n    - AHCI\\n    - NVMe\\n\\n> NOTE\\n> M.2 is a size format: about the size and shape of a stick of gum.\\n\\nPopular SSD: PCIe 4.0/M.2/NVMe over 7000MB/s\\n\\nPortal SSD:\\n\\n- SSD (mentioned above)\\n- Protective case\\n  - Durable\\n  - Waterproofing\\n  - Drop resistance\\n  - Heat dissipation\\n  - Compact and portable\\n- Bridge chips\\n- USB-C interface (external) to PCIe/M.2/NVMe interface (internal)\\n- USB-C interface (external) to SATA/M.2/AHCI interface (internal)\\n\\n\\n[NVMe vs M.2: Bus, Interface, and Protocol - MiniTool Partition Wizard](https://www.partitionwizard.com/clone-disk/nvme-vs-m-2.html)\\n\\n[3 SSD Terminologies You Need to Know when Buying SSD Drive - MiniTool Partition Wizard](https://www.partitionwizard.com/clone-disk/ssd-terminology.html)\\n\\n## USB Flash Drive\\n\\nBoth USB Flash Drive and SSD use **NAND** flash memory for data storage. The main difference is from other form factors, such as SSD own more advanced and complicate controller chips to read/write data."},{"id":"/2023/08/14/","metadata":{"permalink":"/journal/2023/08/14/","source":"@site/../../journal/2023-08-14.md","title":"Monday, August 14, 2023","description":"- [ ] Write best ChatGPT prompts","date":"2023-08-14T00:00:00.000Z","tags":[],"readingTime":0.045,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Monday, March 25, 2024","permalink":"/journal/2024/03/25/"},"nextItem":{"title":"Friday, August 4, 2023","permalink":"/journal/2023/08/04/"}},"content":"- [ ] Write best ChatGPT prompts\\n\\nhttps://stackoverflow.com/questions/6760685/creating-a-singleton-in-python\\n\\nhttps://codereview.stackexchange.com/questions/31789/progress-report-for-a-long-running-process-using-yield"},{"id":"/2023/08/04/","metadata":{"permalink":"/journal/2023/08/04/","source":"@site/../../journal/2023-08-04.md","title":"Friday, August 4, 2023","description":"- [ ] Authentication and Authorization in Microservices","date":"2023-08-04T00:00:00.000Z","tags":[],"readingTime":0.55,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Monday, August 14, 2023","permalink":"/journal/2023/08/14/"},"nextItem":{"title":"Wednesday, July 26, 2023","permalink":"/journal/2023/07/26/"}},"content":"- [ ] Authentication and Authorization in Microservices\\n\\n1. Authentication and Authorization in each service\\n2. Authentication in a centralized service, and Authorization in each service\\n3. Authentication and Authorization in a centralized service\\n\\n\\n- [ ] Auth Service and User (Profile) Service\\n\\n[Never write a UserService again. Or when to use external Microservices](https://blog.softwaremill.com/never-write-a-userservice-again-d771e10265d)\\n\\n[Microservices Authentication Best Strategy | Aspecto](https://www.aspecto.io/blog/microservices-authentication-strategies-theory-to-practice/)\\n\\n[Authentication and Authorization Concepts for MicroServices \xb7 GitHub](https://gist.github.com/andineck/0ed33faf686560f71234)\\n\\n[design - Microservice Architecture - using Auth Server as a User Resource server](https://softwareengineering.stackexchange.com/questions/366815/microservice-architecture-using-auth-server-as-a-user-resource-server)\\n\\n[How to Run Your Own Decentralized Authentication Service Using AuthN](https://www.freecodecamp.org/news/how-to-run-your-own-decentralized-authentication-service-using-authn/)\\n\\n- [ ] Implement event-driven architecture microservices using Redis\\n\\n[Using Redis as an Event Store for Communication Between Microservices](https://redis.com/blog/use-redis-event-store-communication-microservices/)"},{"id":"/2023/07/26/","metadata":{"permalink":"/journal/2023/07/26/","source":"@site/../../journal/2023-07-26.md","title":"Wednesday, July 26, 2023","description":"postgres + node + data model(typescript)","date":"2023-07-26T00:00:00.000Z","tags":[],"readingTime":0.09,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Friday, August 4, 2023","permalink":"/journal/2023/08/04/"},"nextItem":{"title":"Tuesday, July 25, 2023","permalink":"/journal/2023/07/25/"}},"content":"postgres + node + data model(typescript)\\n\\n[Build a Data Access Layer with PostgreSQL and Node.js | AppSignal Blog](https://blog.appsignal.com/2022/06/01/build-a-data-access-layer-with-postgres-and-node.html)"},{"id":"/2023/07/25/","metadata":{"permalink":"/journal/2023/07/25/","source":"@site/../../journal/2023-07-25.md","title":"Tuesday, July 25, 2023","description":"- [ ] Authentication and Authorization in Microservices","date":"2023-07-25T00:00:00.000Z","tags":[],"readingTime":1.405,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Wednesday, July 26, 2023","permalink":"/journal/2023/07/26/"},"nextItem":{"title":"Friday, July 21, 2023","permalink":"/journal/2023/07/21/"}},"content":"- [ ] Authentication and Authorization in Microservices\\n\\nAuthentication in microservices involves two main occasions:\\n\\n1. authentication required when end users communicate with services.\\n2. authentication happens between internal services.\\n3. authentication needed when external services enter internal services.\\n\\nOAuth 2.0 provides the industry-standard protocol for authorizing users in distributed systems. The OAuth framework reduces the burden on developers, eliminating duplications to build their own authentication mechanism in each microservice.\\n\\n[Authentication & Authorization in Microservices Architecture - Part I](https://dev.to/behalf/authentication-authorization-in-microservices-architecture-part-i-2cn0)\\n\\nhttps://softwareengineering.stackexchange.com/questions/366815/microservice-architecture-using-auth-server-as-a-user-resource-server\\n\\n\\nhttps://auth0.com/docs/get-started\\n\\n\\n\\n- [ ] User registration flow in microservice\\n\\n\\n- [ ] Communication between microservices\\n\\nShare user data between micro services\\n\\nUser service and Comment service\\n\\npopulate user data into Comment service, save user data in comment service, update user data in comment service\\n\\nhttps://stackoverflow.com/questions/67543408/microservices-storing-user-data-in-separate-database\\n\\nIdeally, the client communicates with the each service directly, and no interaction between the services!\\n\\nHowever, there is the need for communication between these services.\\n\\nFor example, o what happens if a user deletes his account? What if you delete a TV show? You probably want to trigger some events that will update the data in your comment microservice. In the long run you want to keep everything \\"eventually consistent\\".\\n\\nThe event-driven architecture comes up!\\n\\n- [ ] Data retrieved from two or more services\\n\\nFor example, you send a request from UI saying \\"give me comments with usernames\\", \\n\\nGraphQL interface then first issues a request to comments service, then to user service and finally sends one response with combined data\\n> NOTE: issue a number of requests to various micro-services to collect all the data and return it in only 1 response\\n\\n\\nRest needs to send many.\\n\\n\\nhttps://softwareengineering.stackexchange.com/questions/418153/design-a-correct-microservices-architecture-with-data-relations\\n\\n[Event-Driven Data Management for Microservices - NGINX](https://www.nginx.com/blog/event-driven-data-management-microservices/)"},{"id":"/2023/07/21/","metadata":{"permalink":"/journal/2023/07/21/","source":"@site/../../journal/2023-07-21.md","title":"Friday, July 21, 2023","description":"- [ ]  Mysql, redis, or other db connections pool vs as single connection in Nodejs","date":"2023-07-21T00:00:00.000Z","tags":[],"readingTime":0.215,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Tuesday, July 25, 2023","permalink":"/journal/2023/07/25/"},"nextItem":{"title":"Thursday, July 20, 2023","permalink":"/journal/2023/07/20/"}},"content":"- [ ]  Mysql, redis, or other db connections pool vs as single connection in Nodejs\\n\\nSince Node.js and Redis are both effectively single threaded there is no need to use multiple client instances or any pooling mechanism save for a few exceptions;"},{"id":"/2023/07/20/","metadata":{"permalink":"/journal/2023/07/20/","source":"@site/../../journal/2023-07-20.md","title":"Thursday, July 20, 2023","description":"- [ ] HapiJS","date":"2023-07-20T00:00:00.000Z","tags":[],"readingTime":1.035,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Friday, July 21, 2023","permalink":"/journal/2023/07/21/"},"nextItem":{"title":"Wednesday, July 19, 2023","permalink":"/journal/2023/07/19/"}},"content":"- [ ] HapiJS\\n\\n[Hapi.js \u2014 Project Structure and Best Practices (Part 2)](https://medium.com/the-resonant-web/production-ready-hapi-js-starter-kit-part-2-cba358373017)\\n\\n[Optimizing HapiJS for Benchmarks. In the past year or so, our team\u2026 | by Joel Chen | Walmart Global Tech Blog | Medium](https://medium.com/walmartglobaltech/optimizing-hapijs-for-benchmarks-737f371265e9)\\n\\n- [ ] The confused saying in Microservices: \\"each service should own its own database and no two services should share a database\\"\\n\\nNo golden rule, no fast rules, no best practices suitable for all businesses, only `tradeoff`\\n\\nQ: Need separate database per service?\\nA: Creating a separate database for each service helps to enforce **domain boundaries**.\\n\\n[The Hardest Part About Microservices: Your Data \u2013 Software Blog](https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/)\\n\\n- [ ] Nodejs development practices\\n\\nSet default configs: author name, author email, author url, the license, and the version. \\n\\n```sh\\nnpm set init.author.name \\"Your name\\"\\nnpm set init.author.email \\"your@email.com\\"\\nnpm set init.author.url \\"https://your-url.com\\"\\nnpm set init.license \\"MIT\\"\\nnpm set init.version \\"1.0.0\\"\\n```\\n\\n```sh\\nfunction node-project {\\n  git init\\n  npx license $(npm get init.license) -o \\"$(npm get init.author.name)\\" > LICENSE\\n  npx gitignore node\\n  npx covgen \\"$(npm get init.author.email)\\"\\n  npm init -y\\n  git add -A\\n  git commit -m \\"Initial commit\\"\\n}\\n```\\n\\n\\nSetting up efficient workflows with ESLint, Prettier and TypeScript in vscode.\\n\\n[Setting up efficient workflows with ESLint, Prettier and TypeScript - JavaScript inDepth](https://indepth.dev/posts/1282/setting-up-efficient-workflows-with-eslint-prettier-and-typescript)"},{"id":"/2023/07/19/","metadata":{"permalink":"/journal/2023/07/19/","source":"@site/../../journal/2023-07-19.md","title":"Wednesday, July 19, 2023","description":"- [ ] I still prefer os.path over Pathlib, as follows","date":"2023-07-19T00:00:00.000Z","tags":[],"readingTime":1.43,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Thursday, July 20, 2023","permalink":"/journal/2023/07/20/"},"nextItem":{"title":"Sunday, July 16, 2023","permalink":"/journal/2023/07/16/"}},"content":"- [ ] I still prefer **os.path** over **Pathlib**, as follows\\n\\n1. Consistency, I\'m used to use path string as an argument between functions and I think **Pathlib** is not flexible enough to handle arguments\\n2. Pure and Function, Although **Pathlib** brings many useful features like `glob`, `stem`, and so on. I still like the concept of simplicity that don\'t put all things together!\\n\\n\\n- [ ] Trim `$` for clipboard copy in **Docusaurus** in code block bash.\\n\\n[Ignore `$ ` for clipboard copy \xb7 Issue #1745 \xb7 facebook/docusaurus \xb7 GitHub](https://github.com/facebook/docusaurus/issues/1745)\\n\\n- [x] Some common issues I often hit when using **git**\\n\\nConfigure username/password for different repos or remotes\\n\\nGlobal configuration\\n\\n```sh\\ngit config --global --list\\ngit config --local --list\\n```\\n\\n\\n**GIT** two popular authentication methods:\\n\\n- ssh key\\n\\n[How to Authenticate Your Git to GitHub with SSH Keys](https://hackernoon.com/how-to-authenticate-your-git-to-github-with-ssh-keys)\\n\\n- git credentials\\n\\nStore username/password instead of ssh for multiple remotes\\n\\nTo enable git credentials\\n\\n```sh\\n# local\\ngit config credential.helper store\\n# global\\ngit config --global credential.helper store\\n```\\n\\nEach credential is stored in `~/.git-credentials` file on its own line as a URL like:\\n\\n```sh\\nhttps://<USERNAME>:<PASSWORD>@github.com\\n```\\n\\nConfigure credentials,\\n\\n```sh\\n# Global\\ngit config --global credential.https://github.com.username <your_username>\\n\\n# Or \\ngit config --local user.name <your_username>\\ngit config --local user.email <your_useremail>\\n# Then git pull or git push to let it cache your username/password after it prompt you to input password in the first time\\n```\\n\\n\\nAlternatively, we can directly edit our global Git config file `~/.gitconfig`,\\n\\n```sh\\n[credential \\"https://github.com\\"]\\n\\tusername = <username>\\n```\\n\\n[Git - Config Username & Password - Store Credentials - ShellHacks](https://www.shellhacks.com/git-config-username-password-store-credentials/)\\n\\n[Configuring Git Credentials](https://www.baeldung.com/ops/git-configure-credentials)\\n\\n- [ ] Programming Algorithms\\n\\nTop Algorithms Every Programmer Should Know\\n\\n[What is Algorithm | Introduction to Algorithms - GeeksforGeeks](https://www.geeksforgeeks.org/introduction-to-algorithms/)"},{"id":"/2023/07/16/","metadata":{"permalink":"/journal/2023/07/16/","source":"@site/../../journal/2023-07-16.md","title":"Sunday, July 16, 2023","description":"Admission program requirements | University of Ottawa","date":"2023-07-16T00:00:00.000Z","tags":[],"readingTime":0.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Wednesday, July 19, 2023","permalink":"/journal/2023/07/19/"},"nextItem":{"title":"Thursday, July 14, 2023","permalink":"/journal/2023/07/14/"}},"content":"[Admission program requirements | University of Ottawa](https://www.uottawa.ca/study/graduate-studies/program-specific-requirements)\\n\\n[Faculty of Graduate Studies | University of Calgary](https://grad.ucalgary.ca/future-students/explore-programs)\\n\\n[Graduate Programs - University of Alberta](https://calendar.ualberta.ca/content.php?catoid=39&navoid=12434)\\n\\n[Temporary Foreign Workers - Job Bank](https://www.jobbank.gc.ca/temporary-foreign-workers)\\n\\n20 Common Resume Buzzwords (and What to Use Instead)"},{"id":"/2023/07/14/","metadata":{"permalink":"/journal/2023/07/14/","source":"@site/../../journal/2023-07-14.md","title":"Thursday, July 14, 2023","description":"Best practice:","date":"2023-07-14T00:00:00.000Z","tags":[],"readingTime":1.27,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Sunday, July 16, 2023","permalink":"/journal/2023/07/16/"},"nextItem":{"title":"Tuesday, July 4, 2023","permalink":"/journal/2023/07/04/"}},"content":"Best practice:\\n\\n- [x] Update markdown metadata such as datetime when saving files\\n    [Introduction | Front Matter](https://frontmatter.codes/docs)\\n\\n- [x] Docusaurus refer code snippets from GitHub repositories\\n    [GitHub - saucelabs/docusaurus-theme-github-codeblock: A Docusaurus v2 plugin that supports referencing code examples from public GitHub repositories.](https://github.com/saucelabs/docusaurus-theme-github-codeblock)\\n\\n    ```js reference\\n    https://github.com/saucelabs/docusaurus-theme-github-codeblock/blob/main/src/theme/ReferenceCodeBlock/index.tsx#L105-L108\\n    ```\\n\\n    ```js reference\\n    https://github.com/liviaerxin/liviaerxin.github.io/blob/560ce03e8dbf5d32b197ccf307ca36af25b5dacd/code-snippets/XKeyIn.cpp#L55-L72\\n    ```\\n\\n- [x] Test-Driven Development mindset involving CI, CD, documentation, iterative deliveries\\n\\n- [x] Create a local volume to bind a specific local folder, only available in `Linux` below.\\n\\n```sh\\ndocker volume create --opt type=none --opt o=bind --opt device=/data/volumes/testvol testvol\\n```\\n\\n```sh\\n\u279c  ~ docker inspect testvol\\n[\\n    {\\n        \\"CreatedAt\\": \\"2023-07-13T04:36:16Z\\",\\n        \\"Driver\\": \\"local\\",\\n        \\"Labels\\": {},\\n        \\"Mountpoint\\": \\"/var/lib/docker/volumes/testvol/_data\\",\\n        \\"Name\\": \\"testvol\\",\\n        \\"Options\\": {\\n            \\"device\\": \\"/data/volumes/testvol\\",\\n            \\"o\\": \\"bind\\",\\n            \\"type\\": \\"none\\"\\n        },\\n        \\"Scope\\": \\"local\\"\\n    }\\n```\\n\\nIn default, the created volume will just sit on `/var/lib/docker/volumes`\\n\\n```sh\\ndocker volume create defaultvol\\n```\\n\\n```sh\\n\u279c  ~ docker volume inspect defaultvol\\n[\\n    {\\n        \\"CreatedAt\\": \\"2023-07-13T04:51:57Z\\",\\n        \\"Driver\\": \\"local\\",\\n        \\"Labels\\": null,\\n        \\"Mountpoint\\": \\"/var/lib/docker/volumes/defaultvol/_data\\",\\n        \\"Name\\": \\"defaultvol\\",\\n        \\"Options\\": null,\\n        \\"Scope\\": \\"local\\"\\n    }\\n\\n```\\n\\n\\n- [ ] Proxies Server: **Traefik** vs **NGINIX**\\n\\nProxies have become an essential networking component and are frequently used with many popular internet services. Proxy servers facilitate requests and responses between end-users and web servers, providing helpful features that augment routing control, privacy, and security. NGINX and Traefik are the most popular tools currently offering proxy functionality. Both solutions can support traditional server-based deployments and containerized application environments, such as Kubernetes. This article will examine both tools in-depth and cover their pros, cons, and distinguishing features.\\n\\n[Traefik vs NGINX: Use Case Comparison](https://www.kubecost.com/kubernetes-devops-tools/traefik-vs-nginx/)"},{"id":"/2023/07/04/","metadata":{"permalink":"/journal/2023/07/04/","source":"@site/../../journal/2023-07-04.md","title":"Tuesday, July 4, 2023","description":"Resumable upload","date":"2023-07-04T00:00:00.000Z","tags":[],"readingTime":0.36,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Thursday, July 14, 2023","permalink":"/journal/2023/07/14/"},"nextItem":{"title":"Monday, July 3, 2023","permalink":"/journal/2023/07/03/"}},"content":"Resumable upload\\n\\n[app_resumable_upload](../code-snippets/python/app_resumable_upload.py)\\n[app_resumable_upload](../code-snippets/python/app_tusd.py)\\n\\n\\n[Implementations | tus.io](https://tus.io/implementations)\\n\\n[Resumable file upload](https://javascript.info/resume-upload)\\n\\n[GitHub - tus/tus-js-client: A pure JavaScript client for the tus resumable upload protocol](https://github.com/tus/tus-js-client)\\n\\n[GitHub - tus/tusd: Reference server implementation in Go of tus: the open protocol for resumable file uploads](https://github.com/tus/tusd)\\n\\nIO, StreamIO, FileIO\\n\\nhigh-level used by asyncio.io in socket/tcp/http:   \\n[Streams \u2014 Python 3.11.4 documentation](https://docs.python.org/3/library/asyncio-stream.html#streamreader)\\n\\nstarlette.Request.stream = http Request Body\\n\\nlow-level:  \\n[io \u2014 Core tools for working with streams \u2014 Python 3.11.4 documentation](https://docs.python.org/3/library/io.html#io.RawIOBase)"},{"id":"/2023/07/03/","metadata":{"permalink":"/journal/2023/07/03/","source":"@site/../../journal/2023-07-03.md","title":"Monday, July 3, 2023","description":"As a backend engineer, there are several core skills that are important for success in the field. These skills include:","date":"2023-07-03T00:00:00.000Z","tags":[],"readingTime":2.79,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Tuesday, July 4, 2023","permalink":"/journal/2023/07/04/"}},"content":"As a backend engineer, there are several core skills that are important for success in the field. These skills include:\\n\\n1. Programming languages: Proficiency in one or more programming languages is crucial for backend development. Common languages for backend engineering include Python, Java, C#, Ruby, and JavaScript (Node.js). It\'s important to have a strong understanding of data structures, algorithms, and object-oriented programming concepts.\\n\\n2. Web frameworks: Familiarity with backend web frameworks is essential. Depending on the language you work with, you should be proficient in frameworks such as Django (Python), Spring (Java), ASP.NET (C#), Ruby on Rails (Ruby), or Express.js (Node.js). These frameworks provide tools and libraries for building robust web applications and services.\\n\\n3. Databases and query languages: Backend engineers often work with databases to store and retrieve data. Understanding relational databases like MySQL, PostgreSQL, or Oracle, as well as NoSQL databases like MongoDB or Redis, is valuable. Additionally, knowledge of SQL (Structured Query Language) for database querying is important.\\n\\n4. API development and integration: Backend engineers frequently design and build APIs (Application Programming Interfaces) to enable communication between different systems and services. You should have experience in designing and implementing RESTful APIs and be familiar with tools like Swagger or OpenAPI.\\n\\n5. Server management and deployment: Understanding server management and deployment processes is essential. Familiarity with cloud platforms like AWS (Amazon Web Services), Azure, or Google Cloud, as well as containerization technologies like Docker and orchestration tools like Kubernetes, is valuable.\\n\\n6. Security and scalability: Backend engineers need to have a strong understanding of security principles to develop secure applications. Knowledge of authentication and authorization mechanisms, data encryption, and handling user input securely is important. Additionally, understanding scalability concepts and techniques for handling high traffic and load balancing can be beneficial.\\n\\n7. Testing and debugging: Proficiency in testing and debugging is crucial to ensure the reliability and stability of backend systems. Knowledge of unit testing frameworks, integration testing, and debugging tools is important to identify and fix issues efficiently.\\n\\n8. Version control systems: Proficiency in version control systems like Git is essential for collaboration and managing code repositories. Understanding branching, merging, and pull requests is important to work effectively in a team.\\n\\n9. Continuous Integration and Continuous Deployment (CI/CD): Familiarity with CI/CD practices and tools like Jenkins, Travis CI, or CircleCI is valuable. Knowledge of automated testing, build pipelines, and deployment workflows is important to streamline development processes.\\n\\n10. Problem-solving and analytical thinking: Backend engineers often face complex problems that require analytical thinking and problem-solving skills. The ability to analyze requirements, break down problems into manageable tasks, and devise efficient solutions is highly valuable.\\n\\nRemember, the specific skills required may vary depending on the organization, industry, and technology stack being used. It\'s important to stay updated with emerging technologies and trends in backend development to remain competitive in the job market.\\n\\n\\nWhy Google Search Journey?\\n\\nGoogle Search Journey groups pages from your search histories by topic or intent, providing a more helpful user experience than just showing a chronological list of pages.\\n\\nSearch can, at times, become a rabbit hole. A user may start out looking to book a trip but then get distracted by work or life- or doing several related searches for things they\'ll need for that trip - and totally forget to book the actual trip.\\n\\nPractice programming skills\\n\\n- leetcode\\n- codewars"}]}}')}}]);