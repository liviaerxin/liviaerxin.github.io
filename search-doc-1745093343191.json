{"searchDocs":[{"title":"Algorithms","type":0,"sectionRef":"#","url":"/docs/algorithms","content":"","keywords":"algorithm","version":"Next"},{"title":"Boyer–Moore majority vote algorithm​","type":1,"pageTitle":"Algorithms","url":"/docs/algorithms#boyermoore-majority-vote-algorithm","content":" Just run one loop to find the majority in an array.  Time complexity: O(n)Space complexity: O(1)  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Algorithms","url":"/docs/algorithms#resources","content":"","version":"Next","tagName":"h2"},{"title":"ARM64/AArch64 Assembly Cheat Sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64","content":"","keywords":"AArch64 ARM64 Cheat sheet","version":"Next"},{"title":"Registers​","type":1,"pageTitle":"ARM64/AArch64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64#registers","content":" Register\tLow 32-bits\tCalling conventionGeneral-purpose registers: x0\tw0 x1\tw1 x2\tw2 Special-purpose registers: xzr\twzr\tZero register sp\t-\tStack pointer  ","version":"Next","tagName":"h2"},{"title":"Data type​","type":1,"pageTitle":"ARM64/AArch64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64#data-type","content":" Definition size\tDefinition instruction8 bit\tbyte 16 bit\thword 32 bit\tword 64 bit\tdword  ","version":"Next","tagName":"h2"},{"title":"Load from immediate​","type":1,"pageTitle":"ARM64/AArch64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64#load-from-immediate","content":" movz/mov + movk  Load the 64-bit integer 0x1a2b3c4d1a2b3c4d from the immediate,  // Load the 64-bit integer `0x1a2b3c4d1a2b3c4d` from the immediate movz x1, #0x3c4d movk x1, #0x1a2b, lsl #16 movk x1, #0x3c4d, lsl #32 movk x1, #0x1a2b, lsl #48   ","version":"Next","tagName":"h2"},{"title":"Load from label​","type":1,"pageTitle":"ARM64/AArch64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64#load-from-label","content":" Load instruction\tPurposeldr x0, [x1]\tload 64-bit ldr w0, [x1]\tload 32-bit ldrh w0, [x1]\tload 16-bit ldrb w0, [x1]\tload 8-bit  Assume the 32-bit data in .data section,  .data int32_var: .word 0x1a2b3c4d   adr: shift by byte(±1M, one instruction), the assembler will do:  calculate the PC-relative offset from the current adr instruction to the label int32_var in bytes.encode the offset in the adr instruction.  adr x20, int32_var ldr x2, [x20]   adrp + add: shift by 4KB page(±4G, two instructions), the assembler will do:  calculate the PC-relative offset from the current adr instruction to the label int32_var in page. calculate the PC-relative offset in bytes.divide the byte offset using 4096(or right shift 12 bits), now the quotient is page offset encode the page offset in the adrp instruction.encode the lower 12 bits in the add instruction.  adrp\tx20, int32_var add x20, x20, :lo12:int32_var ldr x2, [x20]   or more simply,  adrp\tx20, int32_var ldr x2, [x20, :lo12:int32_var]   in macOS m1,  adrp\tx20, int32_var@PAGE add x20, x20, int32_var@PAGEOFF ldr x2, [x20]   ","version":"Next","tagName":"h2"},{"title":"Store​","type":1,"pageTitle":"ARM64/AArch64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64#store","content":" ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"ARM64/AArch64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-arm64#resources","content":" ios-resources/bits/arm64.md at master · Siguza/ios-resources · GitHub  asm_book/section_1/regs/ldr.md at main · pkivolowitz/asm_book · GitHub  Exploring AArch64 assembler – Chapter 5  https://peterdn.com/post/2020/08/22/hello-world-in-arm64-assembly/  https://gpanders.com/blog/exploring-mach-o-part-1/  https://iitd-plos.github.io/col718/ref/arm-instructionset.pdf  https://modexp.wordpress.com/2018/10/30/arm64-assembly/#registers  https://stackoverflow.com/questions/41906688/what-are-the-semantics-of-adrp-and-adrl-instructions-in-arm-assembly ","version":"Next","tagName":"h2"},{"title":"Cheatsheet ImageMagick","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-imagemagick","content":"","keywords":"Cheatsheet ImageMagick","version":"Next"},{"title":"Convert a PNG file to a JPG file​","type":1,"pageTitle":"Cheatsheet ImageMagick","url":"/docs/cheat-sheet/cheatsheet-imagemagick#convert-a-png-file-to-a-jpg-file","content":" convert input.png \\ -density 150 \\ -define jpeg:extent=500KB \\ output.jpg   convert input.png \\ -density 150 \\ -quality 50 \\ output.jpg   ","version":"Next","tagName":"h2"},{"title":"Convert a PDF to a series of PNG files​","type":1,"pageTitle":"Cheatsheet ImageMagick","url":"/docs/cheat-sheet/cheatsheet-imagemagick#convert-a-pdf-to-a-series-of-png-files","content":" convert \\ &lt;your-PDF-file&gt;.pdf \\ -density 150 \\ -trim \\ -verbose \\ page%d.png   ","version":"Next","tagName":"h2"},{"title":"Convert a PDF to a series of JPG files​","type":1,"pageTitle":"Cheatsheet ImageMagick","url":"/docs/cheat-sheet/cheatsheet-imagemagick#convert-a-pdf-to-a-series-of-jpg-files","content":" convert \\ -density 150 \\ -trim \\ &lt;your-PDF-file&gt;.pdf \\ -quality 100 \\ -flatten \\ -sharpen 0x1.0 \\ -verbose \\ page%d.jpg   ","version":"Next","tagName":"h2"},{"title":"Compress a PDF​","type":1,"pageTitle":"Cheatsheet ImageMagick","url":"/docs/cheat-sheet/cheatsheet-imagemagick#compress-a-pdf","content":" convert -density 120 -quality 100 -compress jpeg original.pdf output.pdf   ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Cheatsheet ImageMagick","url":"/docs/cheat-sheet/cheatsheet-imagemagick#resources","content":"","version":"Next","tagName":"h2"},{"title":"Docker Cheat Sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-docker","content":"","keywords":"docker docker compose cheatsheet","version":"Next"},{"title":"Docker​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#docker","content":" ","version":"Next","tagName":"h2"},{"title":"Test image busybox​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#test-image-busybox","content":" Famous busybox image that provide many common UNIX utilities for testing.  docker run -it --rm --privileged busybox sh   ","version":"Next","tagName":"h3"},{"title":"Find the IP address of Docker container​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#find-the-ip-address-of-docker-container","content":" docker inspect \\ -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \\ nfs   ","version":"Next","tagName":"h3"},{"title":"Delete all containers(include all status of running, stopped, created)​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#delete-all-containersinclude-all-status-of-running-stopped-created","content":" docker rm -f $(docker ps -a -q)   ","version":"Next","tagName":"h3"},{"title":"Delete the container by image name​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#delete-the-container-by-image-name","content":" docker rm -f $(docker ps -a -q --filter ancestor=&lt;image name here!&gt;)   drmi() { docker rm -f $(docker ps -a | awk -v i=&quot;^$1.*&quot; '{if($2~i){print$1}}'); }   ","version":"Next","tagName":"h3"},{"title":"Delete all volumes​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#delete-all-volumes","content":" docker volume rm $(docker volume ls -q)   ","version":"Next","tagName":"h3"},{"title":"Keep container running for for testing and debugging​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#keep-container-running-for-for-testing-and-debugging","content":" # Use -t(-tty) docker run --rm -d -t busybox   docker run --rm -d busybox tail -f /dev/null   docker run --rm -d busybox sleep infinity   ","version":"Next","tagName":"h3"},{"title":"Docker Compose​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#docker-compose","content":" ","version":"Next","tagName":"h2"},{"title":"Check docker-compose.yml rendering​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#check-docker-composeyml-rendering","content":" docker compose --env-file .env --env-file .prod.env config   ","version":"Next","tagName":"h3"},{"title":"Rebuild image and restart a service which you specified​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#rebuild-image-and-restart-a-service-which-you-specified","content":" docker-compose up --no-deps web-app -d   ","version":"Next","tagName":"h3"},{"title":"Remove a service​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#remove-a-service","content":" docker-compose rm -s -v web-auth   ","version":"Next","tagName":"h3"},{"title":"env_file in docker-compose.yml​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#env_file-in-docker-composeyml","content":" env_file equals environment functions.If you use both the env_file and environment attribute, environment variables set by environment take precedence.env_file not used for variables substitution in docker-compose.yml fileenv_file is not the same as --env-file used in docker-compose --env-file cli. --env-file .env.prod will be used for interpolation for docker-compose.yml file.  Variables resolved from env_file but not taking effect in docker-compose.yaml  docker compose --env-file .env --env-file .prod.env up -d   ","version":"Next","tagName":"h3"},{"title":"Exclude sub folders when mounting a local folder​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#exclude-sub-folders-when-mounting-a-local-folder","content":" It's very useful when developing a node project. That will help only mount the local source codes while excluding the local node_modules folder only in the container.  version: &quot;3&quot; services: node: image: &quot;node:16&quot; working_dir: /home/node/app volumes: - ./:/home/node/app - /home/node/app/node_modules expose: - &quot;8081&quot; command: &quot;npm start&quot;   ","version":"Next","tagName":"h3"},{"title":"Dockerfile​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#dockerfile","content":" ","version":"Next","tagName":"h2"},{"title":"Download from GitHub release using wget​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#download-from-github-release-using-wget","content":" tgz  RUN GH_USER=REPLACE_WITH_USER \\ GH_REPO=REPLACE_WITH_REPO \\ GH_BRANCH=REPLACE_WITH_BRANCH \\ wget https://github.com/${GH_USER}/${GH_REPO}/archive/refs/tags/${GH_BRANCH}.tar.gz -O &quot;${GH_REPO}-${GH_BRANCH}.tar.gz&quot; \\ &amp;&amp; tar -xzvf ./&quot;${GH_REPO}-${GH_BRANCH}.tar.gz&quot; \\ &amp;&amp; rm ./&quot;${GH_REPO}-${GH_BRANCH}.tar.gz&quot;   zip  RUN GH_USER=REPLACE_WITH_USER \\ GH_REPO=REPLACE_WITH_REPO \\ GH_BRANCH=REPLACE_WITH_BRANCH \\ wget https://github.com/${GH_USER}/${GH_REPO}/archive/refs/tags/${GH_BRANCH}.zip \\ -O &quot;${GH_REPO}-${GH_BRANCH}.zip&quot; &amp;&amp; \\ unzip ./&quot;${GH_REPO}-${GH_BRANCH}.zip&quot; &amp;&amp; \\ rm ./&quot;${GH_REPO}-${GH_BRANCH}.zip&quot;   ","version":"Next","tagName":"h3"},{"title":"Download from GitHub release using curl​","type":1,"pageTitle":"Docker Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-docker#download-from-github-release-using-curl","content":" curl -L https://github.com/yarnpkg/yarn/releases/download/v0.23.4/ya‌​rn-v0.23.4.tar.gz &gt; yarn.tar.gz  ","version":"Next","tagName":"h3"},{"title":"x64 Assembly Cheat Sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-assembly-x64","content":"","keywords":"x64 NASM cheat sheet","version":"Next"},{"title":"Registers​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#registers","content":" Register\tLow 32-bits\tLow 16-bits\tLow 8-bit\tHigh 8-bits\tCalling convention\tCallee-saved?General-purpose: %rax\t%eax\t%ax\t%al\t%ah\tReturn value\tNo %rbx\t%ebx\t%bx\t%bl\t%bh\t-\tYes %rcx\t%ecx\t%cx\t%cl\t%ch\t4th argument\tYes %rdx\t%edx\t%dx\t%dl\t%dh\t3th argument\tYes %rsi\t%esi\t%si\t%sil\t-\t2st argument\tNo %rdi\t%edi\t%di\t%dil\t-\t1st argument\tNo %r8\t%r8d\t%r8w\t%r8b\t-\t5th argument\tNo %r9\t%r9d\t%r9w\t%r9b\t-\t6th argument\tNo %r10\t%r10d\t%r10w\t%r10b\t-\t-\tNo %r11\t%r11d\t%r11w\t%r11b\t-\t-\tNo %r12\t%r12d\t%r12w\t%r12b\t-\t-\tYes %r13\t%r13d\t%r13w\t%r13b\t-\t-\tYes %r14\t%r14d\t%r14w\t%r14b\t-\t-\tYes %r15\t%r15d\t%r15w\t%r15b\t-\t-\tYes Special-purpose: %rsp\t%esp\t%sp\t%spl\t–\tStack pointer\tYes %rbp\t%ebp\t%bp\t%bpl\t–\tBase pointer\tYes %rip\t%eip\t%ip\t-\t–\tInstruction pointer\t- %rflags\t%eflags\t%flags\t-\t–\tFlags and condition codes\tNo  ","version":"Next","tagName":"h2"},{"title":"Data Type​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#data-type","content":" Definition size\tNASM\t-\tGAS\tsuffix8 bit\tdb\tBYTE\tbyte\tb 16 bit\tdw\tWORD\tshort/word/2byte\tw 32 bit\tdd\tDWORD\tlong/int/4byte\tl 64 bit\tddq/do\tQWORD\tquad/8byte\tq float\tdd\t-\t- double\tdq\t-\t- extended precision\tdt\t-\t- string\t-\t-\tascii/asciz\t-  .data int8 .db 0x7f msg .db 0x7f, 'E', 'L', 'F', 1, 1, 1, 0   .data int8 .byte 0x7f msg .byte 0x7f, 'E', 'L', 'F', 1, 1, 1, 0 ms .asciz &quot;ELF&quot; ms .ascii &quot;ELF&quot;, 0x0   ","version":"Next","tagName":"h2"},{"title":"Memory and Addressing Modes​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#memory-and-addressing-modes","content":" mov eax, [ebx]\t; Move the 4 bytes in memory at the address contained in EBX into EAX mov [var], ebx\t; Move the contents of EBX into the 4 bytes at memory address var. (Note, var is a 32-bit constant). mov eax, [esi-4]\t; Move 4 bytes at memory address ESI + (-4) into EAX mov [esi+eax], cl\t; Move the contents of CL into the byte at address ESI+EAX mov edx, [esi+4*ebx] ; Move the 4 bytes of data at address ESI+4*EBX into EDX   mov (%ebx), %eax\t/* Load 4 bytes from the memory address in EBX into EAX. */ mov %ebx, var(,1)\t/* Move the contents of EBX into the 4 bytes at memory address var. (Note, var is a 32-bit constant). */ mov -4(%esi), %eax\t/* Move 4 bytes at memory address ESI + (-4) into EAX. */ mov %cl, (%esi,%eax,1) /* Move the contents of CL into the byte at address ESI+EAX. */ mov (%esi,%ebx,4), %edx /* Move the 4 bytes of data at address ESI+4*EBX into EDX. */   ","version":"Next","tagName":"h2"},{"title":"Size Directives in mov​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#size-directives-in-mov","content":" mov BYTE [ebx], 2\t; Move 2 into the single byte at the address stored in EBX. mov WORD [ebx], 2\t; Move the 16-bit integer representation of 2 into the 2 bytes starting at the address in EBX. mov DWORD [ebx], 2 ; Move the 32-bit integer representation of 2 into the 4 bytes starting at the address in EBX.   movb $2, (%ebx)\t/* Move 2 into the single byte at the address stored in EBX. */ movw $2, (%ebx)\t/* Move the 16-bit integer representation of 2 into the 2 bytes starting at the address in EBX. */ movl $2, (%ebx) /* Move the 32-bit integer representation of 2 into the 4 bytes starting at the address in EBX. */   movsbl %al, %edx # copy 1-byte %al, sign-extend into 4-byte %edx movzbl %al, %edx # copy 1-byte %al, zero-extend into 4-byte %edx   ","version":"Next","tagName":"h2"},{"title":"Common instructions​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#common-instructions","content":" ","version":"Next","tagName":"h2"},{"title":"Mov and lea​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#mov-and-lea","content":" mov src, dst # general form of instruction dst = src mov $0, %eax # %eax = 0 movb %al, 0x409892 # write to address 0x409892 low-byte of %eax mov 8(%rsp), %eax # %eax = value read from address %rsp + 8 lea 0x20(%rsp), %rdi # %rdi = %rsp + 0x20 (no dereference!) lea (%rdi,%rdx,1), %rax # %rax = %rdi + %rdx   ","version":"Next","tagName":"h3"},{"title":"Stack operation​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#stack-operation","content":" push %rbx # push value of %rbx onto stack pushq $0x3 # push immediate value 3 onto stack sub $0x10, %rsp # adjust stack pointer to set aside 16 more bytes pop %rax # pop topmost value from stack into register %rax add $0x10, %rsp # adjust stack point to remove topmost 16 bytes   ","version":"Next","tagName":"h2"},{"title":"Calling Convention​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#calling-convention","content":" mov $0x3, %rdi # first arg is passed in %rdi mov $0x7, %rsi # second arg is passed in %rsi callq binky # transfers control to function binky   ","version":"Next","tagName":"h2"},{"title":"Program structure​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#program-structure","content":" global &lt;entry&gt; -&gt; exposes entry pointextern &lt;function&gt; -&gt; declares a function in another linked .o file (e.g. C function, other asm file)section &lt;sectiontype&gt; -&gt; sets section, usually: .text -&gt; program code.data -&gt; data  The program entry point of a standalone program is the label _start. When compiled with gcc, C provides _start, which inits and then jumps to main, which should then be implemented by the program.  ","version":"Next","tagName":"h2"},{"title":"Syscalls​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#syscalls","content":" put syscall number in EAX (e.g. on Linux: 60 for exit, 1 for write to stdout)put arguments in the registers (see above) like when calling a C functionexecute the syscall instruction  ","version":"Next","tagName":"h2"},{"title":"Calling C functions​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#calling-c-functions","content":" ","version":"Next","tagName":"h2"},{"title":"Assemble​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#assemble","content":" Assemble: nasm -felf64 -o &lt;object&gt; &lt;filename&gt;Link with ld: ld -o &lt;output&gt; &lt;object&gt;Link with gcc: gcc -o &lt;output&gt; &lt;object&gt;  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"x64 Assembly Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-assembly-x64#resources","content":" Assembly 1: Basics – CS 61 2018  CS107 Guide to x86-64  x64 NASM Cheat Sheet · GitHub  nasmtutorial  gasexamples  Guide to x86 Assembly ","version":"Next","tagName":"h2"},{"title":"Python pip cheat sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-python-pip","content":"","keywords":"Cheatsheet Python","version":"Next"},{"title":"pip examples​","type":1,"pageTitle":"Python pip cheat sheet","url":"/docs/cheat-sheet/cheatsheet-python-pip#pip-examples","content":" pip install git+https://github.com/owner/repo@0.1 pip install git+https://github.com/owner/repo@releases/tag/v3.7.1   ","version":"Next","tagName":"h2"},{"title":"requirements.txt​","type":1,"pageTitle":"Python pip cheat sheet","url":"/docs/cheat-sheet/cheatsheet-python-pip#requirementstxt","content":" ","version":"Next","tagName":"h2"},{"title":"Specify extra index url in requirements.txt​","type":1,"pageTitle":"Python pip cheat sheet","url":"/docs/cheat-sheet/cheatsheet-python-pip#specify-extra-index-url-in-requirementstxt","content":" package-one==1.9.4 --extra-index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio``   ","version":"Next","tagName":"h3"},{"title":"Specify github source in requirements.txt​","type":1,"pageTitle":"Python pip cheat sheet","url":"/docs/cheat-sheet/cheatsheet-python-pip#specify-github-source-in-requirementstxt","content":" Specify commit hash​  package-one==1.9.4 package-two @ git+https://github.com/owner/repo@41b95ec package-three==1.0.1   Specify branch name​  package-one==1.9.4 package-two @ git+https://github.com/owner/repo@main package-three==1.0.1   Specify tag​  package-one==1.9.4 package-two @ git+https://github.com/owner/repo@0.1 package-three==1.0.1   Specify release​  package-one==1.9.4 package-two @ git+https://github.com/owner/repo@releases/tag/v3.7.1 package-three==1.0.1   ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Python pip cheat sheet","url":"/docs/cheat-sheet/cheatsheet-python-pip#resources","content":"","version":"Next","tagName":"h2"},{"title":"Building ETL Data Pipeline for ML Training with Airflow","type":0,"sectionRef":"#","url":"/docs/building-etl-in-ml-with-airflow","content":"","keywords":"etl ml python Airflow","version":"Next"},{"title":"Project Structure​","type":1,"pageTitle":"Building ETL Data Pipeline for ML Training with Airflow","url":"/docs/building-etl-in-ml-with-airflow#project-structure","content":" Here's the expected directory layout:  /raw_data/ images/ 1.jpg 2.jpg annotations.json   After processing, images will be split and labeled like this:  /processed_data/ images/ 1.jpg 2.jpg annotations.json train/ images/1.jpg labels/1.txt val/ images/ labels/ yolov5_data.yaml   ","version":"Next","tagName":"h2"},{"title":"Airflow DAG: ml_data_preparation_dag.py​","type":1,"pageTitle":"Building ETL Data Pipeline for ML Training with Airflow","url":"/docs/building-etl-in-ml-with-airflow#airflow-dag-ml_data_preparation_dagpy","content":" from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime import os import shutil import json from PIL import Image RAW_IMAGES_DIR = &quot;/raw_data/images&quot; RAW_ANNOTATIONS_PATH = &quot;/raw_data/annotations.json&quot; WORKING_IMAGES_DIR = &quot;/tmp/xxx_data/images&quot; WORKING_ANNOTATIONS_PATH = &quot;/tmp/xxx_data/annotations.json&quot; PROCESSED_DIR = &quot;/processed_data&quot; PROCESSED_IMAGES_DIR = os.path.join(PROCESSED_DIR, &quot;images&quot;) PROCESSED_ANNOTATIONS_PATH = os.path.join(PROCESSED_DIR, &quot;annotations.json&quot;) TARGET_SIZE = (416, 416) TRAIN_DIR = os.path.join(PROCESSED_DIR, &quot;train&quot;) VAL_DIR = os.path.join(PROCESSED_DIR, &quot;val&quot;) TRAIN_IMAGES_DIR = os.path.join(TRAIN_DIR, &quot;images&quot;) TRAIN_LABELS_DIR = os.path.join(TRAIN_DIR, &quot;labels&quot;) VAL_IMAGES_DIR = os.path.join(VAL_DIR, &quot;images&quot;) VAL_LABELS_DIR = os.path.join(VAL_DIR, &quot;labels&quot;) DATA_YOLO_CONFIG = os.path.join(PROCESSED_DIR, &quot;yolov5_data.yaml&quot;) # Get the new data def extract_data(): os.makedirs(WORKING_DIR, exist_ok=True) os.makedirs(PROCESSED_IMAGES_DIR, exist_ok=True) # Assume that I get data from local RAW_IMAGES_DIR, every time we can get new data then update it into PROCESSED_DIR shutil.copy(RAW_IMAGES_DIR, WORKING_IMAGES_DIR) shutil.copy(RAW_ANNOTATIONS_PATH, WORKING_ANNOTATIONS_PATH) # Resize data def transform_data(): # Assume: the new added image filename not conflict with open(WORKING_ANNOTATIONS_PATH, 'r') as f: coco = json.load(f) image_id_map = {} for img in coco['images']: img_path = os.path.join(WORKING_IMAGES_DIR, img['file_name']) new_img_path = os.path.join(PROCESSED_IMAGES_DIR, img['file_name']) with Image.open(img_path) as im: orig_width, orig_height = im.size resized = im.resize(TARGET_SIZE) resized.save(new_img_path) scale_x = TARGET_SIZE[0] / orig_width scale_y = TARGET_SIZE[1] / orig_height img['width'], img['height'] = TARGET_SIZE image_id_map[img['id']] = (scale_x, scale_y) for ann in coco['annotations']: scale_x, scale_y = image_id_map[ann['image_id']] x, y, w, h = ann['bbox'] ann['bbox'] = [ round(x * scale_x, 2), round(y * scale_y, 2), round(w * scale_x, 2), round(h * scale_y, 2) ] # Update the annotation with open(PROCESSED_ANNOTATIONS_PATH, 'a') as f: old_coco = json.load(f) old_coco.update(coco) json.dump(old_coco, f) # Add a `yolov5` compatible data def load_data(): images_dir = PROCESSED_IMAGES_DIR ann_path = PROCESSED_ANNOTATIONS_PATH os.makedirs(TRAIN_IMAGES_DIR, exist_ok=True) os.makedirs(TRAIN_LABELS_DIR, exist_ok=True) os.makedirs(VAL_IMAGES_DIR, exist_ok=True) os.makedirs(VAL_LABELS_DIR, exist_ok=True) with open(ann_path) as f: coco = json.load(f) # Step 1: Build image ID to filename map img_id_to_filename = {img['id']: img['file_name'] for img in coco['images']} # Step 2: Extract classes dynamically categories = {cat['id']: cat['name'] for cat in coco['categories']} class_name_to_id = {v: i for i, v in enumerate(sorted(set(categories.values())))} class_id_map = {cat_id: class_name_to_id[name] for cat_id, name in categories.items()} class_names = list(class_name_to_id.keys()) # Step 3: Build label files in YOLO format labels = {img_id: [] for img_id in img_id_to_filename} for ann in coco['annotations']: image_id = ann['image_id'] category_id = ann['category_id'] x, y, w, h = ann['bbox'] img = next(i for i in coco['images'] if i['id'] == image_id) img_w, img_h = img['width'], img['height'] x_center = (x + w / 2) / img_w y_center = (y + h / 2) / img_h w /= img_w h /= img_h class_id = class_id_map[category_id] labels[image_id].append(f&quot;{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}&quot;) # Step 4: Split train/val image_ids = list(img_id_to_filename.keys()) random.shuffle(image_ids) split = int(0.8 * len(image_ids)) train_ids, val_ids = image_ids[:split], image_ids[split:] for subset, subset_ids in [('train', train_ids), ('val', val_ids)]: for img_id in subset_ids: filename = img_id_to_filename[img_id] shutil.copy(os.path.join(PROCESSED_IMAGES_DIR, filename), os.path.join(PROCESSED_DIR, subset, &quot;images&quot;, filename)) label_path = os.path.join(PROCESSED_DIR, subset, &quot;labels&quot;, f&quot;{Path(filename).stem}.txt&quot;) with open(label_path, 'w') as f: f.write('\\n'.join(labels[img_id])) # Step 5: Write data.yaml data_yaml = { 'train': TRAIN_IMAGES_DIR, 'val': VAL_IMAGES_DIR, 'nc': len(class_names), 'names': class_names } with open(DATA_YOLO_CONFIG, 'w') as f: yaml.dump(data_yaml, f)   ","version":"Next","tagName":"h2"},{"title":"YOLOv5 Training with Python API​","type":1,"pageTitle":"Building ETL Data Pipeline for ML Training with Airflow","url":"/docs/building-etl-in-ml-with-airflow#yolov5-training-with-python-api","content":" After transformation, we split the dataset and kick off training with YOLOv5 (using the official PyTorch implementation). This step:  Extracts class names from COCO.Generates YOLO .txt label files.Splits data 80/20.Trains a model using YOLOv5’s Python API.  PROCESSED_DIR = &quot;/processed_data&quot; DATA_YOLO_CONFIG = os.path.join(PROCESSED_DIR, &quot;yolov5_data.yaml&quot;) TARGET_SIZE = (416, 416) def train_model(): import os import json import shutil import random import yaml from yolov5 import train train.run( imgsz=416, batch_size=8, epochs=5, data=DATA_YOLO_CONFIG, weights='yolov5s.pt', name='custom_yolov5_model', project=os.path.join(PROCESSED_DIR, &quot;runs&quot;), exist_ok=True )   ","version":"Next","tagName":"h2"},{"title":"Airflow DAG Pipeline​","type":1,"pageTitle":"Building ETL Data Pipeline for ML Training with Airflow","url":"/docs/building-etl-in-ml-with-airflow#airflow-dag-pipeline","content":" Plug all three steps into Airflow:  with DAG( dag_id=&quot;ml_data_preparation_dag&quot;, schedule_interval=None, start_date=datetime(2024, 1, 1), catchup=False, tags=[&quot;ml&quot;, &quot;data-prep&quot;, &quot;coco&quot;] ) as dag: extract_task = PythonOperator( task_id=&quot;extract_data&quot;, python_callable=extract_data ) transform_task = PythonOperator( task_id=&quot;transform_data&quot;, python_callable=transform_data ) load_task = PythonOperator( task_id=&quot;transform_data&quot;, python_callable=load_data ) train_task = PythonOperator( task_id=&quot;train_model&quot;, python_callable=train_model ) extract_task &gt;&gt; transform_task &gt;&gt; load_task &gt;&gt; train_task   ","version":"Next","tagName":"h2"},{"title":"Results​","type":1,"pageTitle":"Building ETL Data Pipeline for ML Training with Airflow","url":"/docs/building-etl-in-ml-with-airflow#results","content":" After running the DAG:  Your images and labels are properly resized and split.Your YOLOv5 model is trained with updated bounding boxes.All steps are repeatable and traceable via Airflow.  ","version":"Next","tagName":"h2"},{"title":"What’s Next?​","type":1,"pageTitle":"Building ETL Data Pipeline for ML Training with Airflow","url":"/docs/building-etl-in-ml-with-airflow#whats-next","content":" Add evaluation and mAP tracking.Schedule retraining weekly.push trained weights to S3 or serve them with FastAPI. ","version":"Next","tagName":"h2"},{"title":"Traefik Cheat Sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-traefik","content":"","keywords":"Cheatsheet Traefik","version":"Next"},{"title":"Redirect root path / to a subpath​","type":1,"pageTitle":"Traefik Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-traefik#redirect-root-path--to-a-subpath","content":" The goal is to redirect root path / to sub path /mtr that's an ingress for a web service:  http://127.0.0.1 -&gt; http://127.0.0.1/mtrhttp://127.0.0.1 -&gt; http://127.0.0.1/mtrhttps://127.0.0.1/something -&gt; no redirect  It works for Traefik 2.0  services: traefik: image: traefik:v2.10 command: - --api.insecure=true - --providers.docker=true - --providers.docker.exposedbydefault=false - --entrypoints.web.address=:80 ports: - 80:80 - 8080:8080 # Web UI Port volumes: - /var/run/docker.sock:/var/run/docker.sock:ro labels: - traefik.enable=true # Redirection from `http://xxx.com` to `http://xxx.com/foo` - traefik.http.routers.domain.entrypoints=web - traefik.http.routers.domain.rule=Path(`/`) - traefik.http.routers.domain.service=noop@internal - traefik.http.routers.domain.middlewares=to-foo@docker - traefik.http.middlewares.to-foo.redirectregex.permanent=true - traefik.http.middlewares.to-foo.redirectregex.regex=^http://([^/]+)/?$ - traefik.http.middlewares.to-foo.redirectregex.replacement=http://$${1}/foo foo: image: traefik/whoami:v1.10 hostname: foo.com labels: - traefik.enable=true # just to ingress `http://xxx.com/foo` - traefik.http.routers.foo.entrypoints=web - traefik.http.routers.foo.rule=PathPrefix(`/foo`)   ","version":"Next","tagName":"h2"},{"title":"Route a prefix to a service​","type":1,"pageTitle":"Traefik Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-traefik#route-a-prefix-to-a-service","content":" Match a request with a prefix /bar, strip the prefix and route it to the bar service,  bar: image: traefik/whoami:v1.10 hostname: bar.com labels: - traefik.enable=true # ingress `http://xxx.com/bar/xyz` and send `http://xxx.com/xyz` to `bar` service - traefik.http.routers.bar.entrypoints=web - traefik.http.routers.bar.rule=PathPrefix(`/bar`) - traefik.http.routers.bar.middlewares=bar-strip-prefix@docker - traefik.http.middlewares.bar-strip-prefix.stripprefix.prefixes=/bar   ","version":"Next","tagName":"h2"},{"title":"Specify a custom port for the container​","type":1,"pageTitle":"Traefik Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-traefik#specify-a-custom-port-for-the-container","content":" By default, Traefik used the first exposed port of a container, if a container exposes multiple ports, set traefik.http.services.xxx.loadbalancer.server.port to override that port.  bar12345: image: traefik/whoami:v1.10 hostname: bar12345.com environment: WHOAMI_PORT_NUMBER: 12345 labels: - traefik.enable=true - traefik.http.routers.bar12345.entrypoints=web - traefik.http.routers.bar12345.rule=PathPrefix(`/bar12345`) - traefik.http.routers.bar12345.service=bar12345 # Tell Traefik to use the port 12345 to connect to `bar12345` service - traefik.http.services.bar12345.loadbalancer.server.port=12345   ","version":"Next","tagName":"h2"},{"title":"Specify more than one router and service per container​","type":1,"pageTitle":"Traefik Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-traefik#specify-more-than-one-router-and-service-per-container","content":" In this example, requests are forwarded for http://127.0.0.1/web2ports-a to http://&lt;private IP of container&gt;:8001 in addition to http://127.0.0.1/web2ports-b forwarding to http://&lt;private IP of container&gt;:8002:  web2ports: image: python:3.10 ports: - 8001:8001 - 8002:8002 working_dir: /app volumes: - ./server_whoami.py:/app/server_whoami.py command: &gt; sh -c &quot;python3 server_whoami.py --port 8001 &amp; python3 server_whoami.py --port 8002&quot; labels: - traefik.enable=true # Specify more than one router and service per container - traefik.http.routers.a-router.entrypoints=web - traefik.http.routers.a-router.rule=PathPrefix(`/web2ports-a`) - traefik.http.routers.a-router.service=a-service - traefik.http.services.a-service.loadbalancer.server.port=8001 - traefik.http.routers.b-router.entrypoints=web - traefik.http.routers.b-router.rule=PathPrefix(`/web2ports-b`) - traefik.http.routers.b-router.service=b-service - traefik.http.services.b-service.loadbalancer.server.port=8002   ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Traefik Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-traefik#resources","content":" URL Redirect abc.com to xyz.com - Traefik v2 (latest) - Traefik Labs Community Forum  Traefik redirect / (root) to sub path with Docker labels · GitHub ","version":"Next","tagName":"h2"},{"title":"Keyboard Shortcut Collection","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/keyboard-shortcut-collection","content":"Keyboard Shortcut Collection VS Code Keyboard Shortcuts Macos VS Code Keyboard Shortcuts Windows","keywords":"Keyboard Shortcut Collection","version":"Next"},{"title":"Hello from Docusaurus","type":0,"sectionRef":"#","url":"/docs/doc-with-tags","content":"","keywords":"","version":"Next"},{"title":"Headers​","type":1,"pageTitle":"Hello from Docusaurus","url":"/docs/doc-with-tags#headers","content":" will show up on the table of contents on the upper right  So that your users will know what this page is all about without scrolling down or even without reading too much.  ","version":"Next","tagName":"h2"},{"title":"Only h2 and h3 will be in the TOC by default.​","type":1,"pageTitle":"Hello from Docusaurus","url":"/docs/doc-with-tags#only-h2-and-h3-will-be-in-the-toc-by-default","content":" You can configure the TOC heading levels either per-document or in the theme configuration.  The headers are well-spaced so that the hierarchy is clear.  lists will help youpresent the key pointsthat you want your users to remember and you may nest them multiple times ","version":"Next","tagName":"h2"},{"title":"Share Data between Docker Containers","type":0,"sectionRef":"#","url":"/docs/docker/docker-containers-data-sharing","content":"","keywords":"Docker Containters Data Sharing","version":"Next"},{"title":"Use a volume to bind a local folder​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/docs/docker/docker-containers-data-sharing#use-a-volume-to-bind-a-local-folder","content":" In default, the volume is created by Docker and its corresponding folder resides in Docker managed folder like /var/lib/docker/volumes/:  $ docker create volume xxx   $ docker volume inspect xxx [ { &quot;CreatedAt&quot;: &quot;2024-07-19T14:41:18+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/xxx/_data&quot;, &quot;Name&quot;: &quot;xxx&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; } ]   However, sometimes you would like to bind the volume into a specified local folder(like /data/volumes/testvol) in hosts(only available in Linux)  $ docker volume create --opt type=none --opt o=bind --opt device=/data/volumes/testvol testvol   $ docker inspect testvol [ { &quot;CreatedAt&quot;: &quot;2024-07-13T04:36:16Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/testvol/_data&quot;, &quot;Name&quot;: &quot;testvol&quot;, &quot;Options&quot;: { &quot;device&quot;: &quot;/data/volumes/testvol&quot;, &quot;o&quot;: &quot;bind&quot;, &quot;type&quot;: &quot;none&quot; }, &quot;Scope&quot;: &quot;local&quot; }   In Docker compose yaml,  services: frontend: image: node:lts volumes: - testvol:/home/node/app volumes: db-data: testvol: driver: local driver_opts: type: none o: bind device: /data/volumes/testvol   ","version":"Next","tagName":"h2"},{"title":"Use NFS volume​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/docs/docker/docker-containers-data-sharing#use-nfs-volume","content":" ","version":"Next","tagName":"h2"},{"title":"Use Samba volume​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/docs/docker/docker-containers-data-sharing#use-samba-volume","content":" ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Share Data between Docker Containers","url":"/docs/docker/docker-containers-data-sharing#references","content":" Volumes | Docker Documentation ","version":"Next","tagName":"h2"},{"title":"Set Up NFS Sever in Docker","type":0,"sectionRef":"#","url":"/docs/docker/docker-setup-nfs-sever","content":"","keywords":"Setup NFS Sever","version":"Next"},{"title":"Start Up NFS Server​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/docs/docker/docker-setup-nfs-sever#start-up-nfs-server","content":" Use docker image gists/nfs-server to start up a NFS server container.  In OSX, it's critical to use volume mount and avoid using bind mount as we mentioned above.  In Linux, it's okay to use either volume mount or bind mount.  docker run --rm -d \\ --name nfs \\ --privileged \\ -p 2049:2049 \\ -v /tmp/volume:/nfs-share \\ -e NFS_DIR=/nfs-share \\ -e NFS_OPTION=&quot;rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash&quot; \\ gists/nfs-server   note Before we use an old nfs server image itsthenetwork/nfs-server-alpine which was not maintained more than 4 years and not supported natively in platform linux/arm/v6. docker run -it --rm \\ --name nfs \\ --privileged \\ -v /tmp/volume:/nfs-share \\ -e SHARED_DIRECTORY=/nfs-share \\ -p 2049:2049 \\ itsthenetwork/nfs-server-alpine:latest   note In OSX, due to the docker desktop itself is running in VM, it will cause some error like Operation not supported when binding a local file folder via bind mount even you set 777 mask on the folder. So it's recommended to use volume to bind to /mnt in Samba server in OSX. Furthermore, the Samba server will log such message: error reading meta xattr: Not supported.  Get the ip address of the NFS server, which will be used later to connect the NFS server when mounting in a Docker container client. If you only want to mount the NFS server from the host, you can just know the ip address of your host.  docker inspect \\ -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \\ nfs   Here the output is  172.17.0.2   ","version":"Next","tagName":"h2"},{"title":"Use NFS Client in Docker​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/docs/docker/docker-setup-nfs-sever#use-nfs-client-in-docker","content":" Let's make use of a Docker container to act in a NFS client to access shared data in the running-up NFS server.  note run container as root by option --privileged or --cap-add SYS_ADMIN when permissions denied inside the container:  docker run -it --rm --privileged busybox sh   Inside the container:  note Due to the fsid=0 parameter set in the /etc/exports file, there's no need to specify the folder name when mounting from a client. For example, this works fine even though the folder being mounted and shared is /nfs-share:  # In the container mkdir /mnt # nfs v4 mount -v -o vers=4,loud 172.17.0.2:/ /mnt # create a file to test echo &quot;some text here&quot; &gt; /mnt/file1.txt   Then go to the Host to list directory /data/volume/test, where you will find the file1.txt is sitting.  # In the host cat /data/volume/test/file1.txt   ","version":"Next","tagName":"h2"},{"title":"Use NFS Client With Volume Mount in Docker​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/docs/docker/docker-setup-nfs-sever#use-nfs-client-with-volume-mount-in-docker","content":" Create a NFS volume in Docker  docker volume create --driver local \\ --opt type=nfs \\ --opt o=addr=172.17.0.2,nfsvers=4 \\ --opt device=:/ \\ nfs-volume   docker inspect nfs-volume   Run the container with the created volume nfs-volume.  docker run -it --rm \\ --privileged \\ --name nfs-test \\ -v nfs-volume:/mnt \\ busybox \\ sh   Alternative, you can use the combined one command which will create a volume nfsvolume,  docker run -it --rm \\ --privileged \\ --name nfs-test \\ --mount 'type=volume,source=nfsvolume,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/,&quot;volume-opt=o=addr=172.17.0.2,rw,nfsvers=4,async&quot;,target=/mnt' \\ busybox \\ sh   ","version":"Next","tagName":"h2"},{"title":"Setup a NFS Server and Mount NFS Volume int Docker Compose​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/docs/docker/docker-setup-nfs-sever#setup-a-nfs-server-and-mount-nfs-volume-int-docker-compose","content":" ","version":"Next","tagName":"h2"},{"title":"About NFS Options​","type":1,"pageTitle":"Set Up NFS Sever in Docker","url":"/docs/docker/docker-setup-nfs-sever#about-nfs-options","content":" Understanding the /etc/exports File – The Geek Diary ","version":"Next","tagName":"h2"},{"title":"Network Cheat Sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheet/cheatsheet-network","content":"","keywords":"Network Cheat Sheet","version":"Next"},{"title":"Get IP address​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#get-ip-address","content":" Linux  rms@rms:~$ ip addr   Windows  PS C:\\Users\\Frank&gt; ipconfig   PS C:\\Users\\Frank&gt; netsh interface ip show address   ","version":"Next","tagName":"h2"},{"title":"Get IP address of a specific network interface​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#get-ip-address-of-a-specific-network-interface","content":" Linux  rms@rms:~$ ip addr show enp0s31f6 5: enp0s31f6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:4e:01:fc:39:50 brd ff:ff:ff:ff:ff:ff inet 10.6.64.184/24 brd 10.6.64.255 scope global enp0s31f6 valid_lft forever preferred_lft forever inet6 fe80::24e:1ff:fefc:3950/64 scope link valid_lft forever preferred_lft forever   rms@rms:~$ hostname -I | awk '{print $1}' 10.6.64.184   Windows  PS C:\\Users\\Frank&gt; netsh interface ip show address &quot;Ethernet&quot; Configuration for interface &quot;Ethernet&quot; DHCP enabled: Yes IP Address: 10.6.64.243 Subnet Prefix: 10.6.64.0/24 (mask 255.255.255.0) Default Gateway: 10.6.64.1 Gateway Metric: 0 InterfaceMetric: 25   PS C:\\Users\\Frank&gt; netsh interface ip show address &quot;Ethernet&quot; | findstr &quot;IP Address&quot;   ","version":"Next","tagName":"h3"},{"title":"Show Routing Table​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#show-routing-table","content":" # linux route # osx netstat -rn   The -r flag means to show routes.  The -n flag means to not resolve IPs to hostnames.  ","version":"Next","tagName":"h2"},{"title":"Find Gateway Used for Routing​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#find-gateway-used-for-routing","content":" # linux ip route get 8.8.8.8 # osx route get 8.8.8.8   ","version":"Next","tagName":"h2"},{"title":"Show Routes across Network​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#show-routes-across-network","content":" traceroute # en0 interface traceroute -i en0   ","version":"Next","tagName":"h2"},{"title":"Ping Through Specific Interface​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#ping-through-specific-interface","content":" # linux ping -I en0 sslvpn.astri.org # osx ping -b en0 sslvpn.astri.org   ","version":"Next","tagName":"h2"},{"title":"Find Out Address Used by Which Process​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#find-out-address-used-by-which-process","content":" # osx netstat -avn -p tcp   ","version":"Next","tagName":"h2"},{"title":"Add a Route​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#add-a-route","content":" # osx route -n add 10.0.0.0/24 10.0.0.1 # linux route -n add -net 10.0.0.0/24 gw 10.0.0.1   ","version":"Next","tagName":"h2"},{"title":"FireWall Rule​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#firewall-rule","content":" osx:  # show all information/stats sudo pfctl -sa # show rules sudo pfctl -sr # sanity check edited configuration file sudo pfctl -v -n -f /etc/pf.conf # load pf with new rules sudo pfctl -f /etc/pf.conf # enable pf sudo pfctl -e # disable pf sudo pfctl -d # add information on the fly sudo pfctl -t localsub -T add 127.0.0.0/24 # flush added rules later sudo pfctl -Fa -f /etc/pf.conf sudo pfctl -si sudo pfctl -q   ","version":"Next","tagName":"h2"},{"title":"Get Geolocation of IP Address​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#get-geolocation-of-ip-address","content":" curl ipinfo.io/103.216.223.161   ","version":"Next","tagName":"h2"},{"title":"Packet Analyzer​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#packet-analyzer","content":" ","version":"Next","tagName":"h2"},{"title":"Monitor TCP packet on network interfaces​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#monitor-tcp-packet-on-network-interfaces","content":" # list which interfaces are available for capture tcpdump --list-interfaces # capture all packets in any interface sudo tcpdump --interface any # limit the number of packets captured then stop `-c number` sudo tcpdump -i any -c 5 # disable name resolution with using `-n` and port resolution with `-nn` sudo tcpdump -i any -c 5 -nn # filter packets by protocol, only capture `ICMP` packets sudo tcpdump -i any -c 5 icmp # capture packets related with host `8.8.8.8` sudo tcpdump -i any -c 5 -nn host 8.8.8.8 # capture packets related with port `80` sudo tcpdump -i any -c 5 -nn port 80 # capture packets with source address 192.168.0.1 sudo tcpdump -i any -c 5 -nn src 192.168.0.1 # capture packets with destination address 8.8.8.8 sudo tcpdump -i any -c 5 -nn dst 8.8.8.8 # capture either port `80` or port `443` sudo tcpdump -i enp0s31f6 -nn port 80 or port 443 # display payload sudo tcpdump -i enp0s31f6 port 80 -A -s 0   In the above command:  -i specifies the network interface, and any means all interfaces.-c specify the number of packets to capture, omitting -c to capture packets continuously.-nn display IP address and port as numbers rather than attempting to resolve them to hostname.-A display the packet payload in ASCII format.-s 0 means capturing the entire packet.  ","version":"Next","tagName":"h3"},{"title":"Monitor HTTP message on network interfaces​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#monitor-http-message-on-network-interfaces","content":" tcpflow # monitor on the interface: `enp0s31f6` sudo tcpflow -p -c -i enp0s31f6 port 80 # just show HTTP message of `GET` and `POST` HTTP methods sudo tcpflow -p -c -i enp0s31f6 port 80 | grep -oE '(GET|POST) .* HTTP/1.[01]|Host: .*'   In the above command:  -p disables promiscuous mode-c means only print the output to the console and don’t create files-i specifies the network interfacegrep -o means show only the matching parts of the lines that match the pattern-E means the pattern is an extended regular expression (ERE)  ","version":"Next","tagName":"h3"},{"title":"USB Virtual Ethernet​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#usb-virtual-ethernet","content":" An explanation on the USB virtual ethernet  ","version":"Next","tagName":"h2"},{"title":"Access WSL 2 from local area network(LAN)​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#access-wsl-2-from-local-area-networklan","content":" After enabling systemd in WSL 2, I have to forward the Windows host port to the WSL 2 distribution.  ","version":"Next","tagName":"h2"},{"title":"Find WSL 2 IP address that can be reached from Windows host​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#find-wsl-2-ip-address-that-can-be-reached-from-windows-host","content":" $ ip addr show eth0 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1420 qdisc mq state UP group default qlen 1000 link/ether 00:15:5d:98:b5:99 brd ff:ff:ff:ff:ff:ff inet 172.29.6.23/20 brd 172.29.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::215:5dff:fe98:b599/64 scope link valid_lft forever preferred_lft forever   ","version":"Next","tagName":"h3"},{"title":"Add proxy​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#add-proxy","content":" netsh interface portproxy add v4tov4 listenport=8082 connectport=8082 connectaddress=172.29.6.23,127.0.0.1   ","version":"Next","tagName":"h3"},{"title":"[Optional] Add firewall rule​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#optional-add-firewall-rule","content":" netsh advfirewall firewall add rule name= &quot;Open Port 8082&quot; dir=in action=allow protocol=TCP localport=8082   ","version":"Next","tagName":"h3"},{"title":"Check current proxy​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#check-current-proxy","content":" netsh interface portproxy show all   ","version":"Next","tagName":"h3"},{"title":"Clean up​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#clean-up","content":" netsh interface portproxy delete v4tov4 listenport=8082   netsh advfirewall firewall delete rule name=&quot;Open port 8082&quot;   ","version":"Next","tagName":"h3"},{"title":"Configure Static IP Address​","type":1,"pageTitle":"Network Cheat Sheet","url":"/docs/cheat-sheet/cheatsheet-network#configure-static-ip-address","content":" Ubuntu 17.10 and later uses Netplan as the default network management tool. The previous Ubuntu versions were using ifconfig and its configuration file /etc/network/interfaces to configure the network.  Edit or create a file under /etc/netplan, such as /etc/netplan/01-netcfg.yaml.  /etc/netplan/01-netcfg.yaml network: version: 2 renderer: networkd ethernets: enp0s31f6: dhcp4: no addresses: [10.6.64.12/24] gateway4: 10.6.64.1 nameservers: addresses: [8.8.8.8,8.8.8.4]   Options:  enp0s31f6: configure the network interface enp0s31f6 of the device type ethernets.dhcp4: no: don't obtain IP address from the DHCP server.addresses: [10.6.64.12/24]: use static IP address 10.6.64.12 in subnet 10.6.64.0/24.  Once done, save the file and apply the changes by running the following command:  $ sudo netplan apply   Verify the changes by typing:  $ ip addr show enp0s31f6   note For Ubuntu server which is provisioned with cloud-init, you may need to disable it. To do so, create the following file: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg as, network: {config: disabled}  ","version":"Next","tagName":"h2"},{"title":"Communication Between Docker Containers","type":0,"sectionRef":"#","url":"/docs/docker/docker-containers-communication","content":"","keywords":"","version":"Next"},{"title":"Using --link flag(Legacy)​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/docs/docker/docker-containers-communication#using---link-flaglegacy","content":" Start a postgres db container:  docker run --rm --name postgres-db --detach -e POSTGRES_PASSWORD=mysecretpassword postgres   Run a postgres client container to connect the db container with user postgres and password mysecretpassword:  docker run -it --rm --link postgres-db:db postgres psql -h db -U postgres psql (14.3) Type &quot;help&quot; for help. postgres=# SELECT 1; ?column? ---------- 1 (1 row)   Or run a utility container:  docker run -it --rm --link postgres-db:db busybox sh # in `busybox` ping db   ","version":"Next","tagName":"h2"},{"title":"Using the default network​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/docs/docker/docker-containers-communication#using-the-default-network","content":" If you are running your container without specifying attached network, it will use the docker default bridge network.  However The default bridge network allows container-to-container communication by IP address only. To use hostname or alias name in connecting rather than IP address, see the following methods.  So before connecting, we need get the container IP address by docker inspect.  Start a postgres db container:  docker run --rm --name postgres-db --detach -e POSTGRES_PASSWORD=mysecretpassword postgres   Get the IP address of the postgres db container:  docker inspect mynginx | grep IPAddress &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,   Run a postgres client container to connect the db container:  docker run -it --rm postgres psql -h &quot;172.17.0.2&quot; -U postgres psql (14.3) Type &quot;help&quot; for help. postgres=# SELECT 1; ?column? ---------- 1 (1 row)   ","version":"Next","tagName":"h2"},{"title":"Using the private network​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/docs/docker/docker-containers-communication#using-the-private-network","content":" Creating private bridge network will give you more privacy that it only allows only containers belonging to it can talk to each other.  Moreover, you can use hostname or alias name to connect without regard of IP address changing due to re-start.  Create a private bridge network:  docker network create postgres-net   Start a postgres db container:  docker run --rm --net postgres-net --name postgres-db --detach -e POSTGRES_PASSWORD=mysecretpassword postgres   Run a postgres client container to connect the db container:  docker run -it --rm --net postgres-net postgres psql -h postgres-db -U postgres psql (14.3) Type &quot;help&quot; for help. postgres=# SELECT 1; ?column? ---------- 1 (1 row)   ","version":"Next","tagName":"h2"},{"title":"Use Case in Docker Compose​","type":1,"pageTitle":"Communication Between Docker Containers","url":"/docs/docker/docker-containers-communication#use-case-in-docker-compose","content":" Actually, docker compose will create its private bridge network, and when it start containers, containers will be attached to that network in default.    docker-compose-postgres.yml # Use below credentials to access in `adminer` web to access `db`, # server: db (db1, db2 are also available!) # user: postgres # password: example version: '3.1' services: db: image: postgres restart: always environment: # POSTGRES_USER: postgres # `postgres` in default. POSTGRES_PASSWORD: example networks: default: aliases: - db1 - db2 adminer: image: adminer restart: always ports: - 8080:8080  ","version":"Next","tagName":"h2"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/frequently-asked-questions","content":"","keywords":"","version":"Next"},{"title":"Links/Graphs/BackLinks don't work. How do I enable them?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/frequently-asked-questions#linksgraphsbacklinks-dont-work-how-do-i-enable-them","content":" Ensure that you have all the [[recommended-extensions]] installed in Visual Studio CodeReload Visual Studio Code by running Cmd + Shift + P (Ctrl + Shift + P for Windows), type &quot;reload&quot; and run the Developer: Reload Window command to for the updated extensions take effectCheck the formatting rules for links on [[foam-file-format]] and [[wikilinks]]  ","version":"Next","tagName":"h2"},{"title":"I don't want Foam enabled for all my workspaces​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/frequently-asked-questions#i-dont-want-foam-enabled-for-all-my-workspaces","content":" Any extension you install in Visual Studio Code is enabled by default. Given the philosophy of Foam, it works out of the box without doing any configuration upfront. In case you want to disable Foam for a specific workspace, or disable Foam by default and enable it for specific workspaces, it is advised to follow the best practices as documented by Visual Studio Code  ","version":"Next","tagName":"h2"},{"title":"I want to publish the graph view to GitHub pages or Vercel​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/frequently-asked-questions#i-want-to-publish-the-graph-view-to-github-pages-or-vercel","content":" If you want a different front-end look to your published foam and a way to see your graph view, we'd recommend checking out these templates:  foam-gatsby by Mathieu Dutourfoam-gatsby-kb by hikerpig ","version":"Next","tagName":"h2"},{"title":"Set Up Samba Server in Docker","type":0,"sectionRef":"#","url":"/docs/docker/docker-setup-samba-server","content":"","keywords":"Set Up Samba Server in Docker","version":"Next"},{"title":"Start Samba Server in Docker​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/docs/docker/docker-setup-samba-server#start-samba-server-in-docker","content":" Here, we use Samba server image from dperson/samba. Although there is an alternative from ghcr.io/servercontainers/samba or crazymax/samba  In OSX, it's critical to use volume mount and avoid using bind mount as we mentioned above.  note In OSX, due to the docker desktop itself is running in VM, it will cause some error like Operation not supported when binding a local file folder via bind mount even you set 777 mask on the folder. So it's recommended to use volume mount to bind to /mnt in Samba server in OSX. Furthermore, the Samba server will log such message: error reading meta xattr: Not supported.  In Linux, it's okay to use either volume mount or bind mount.  docker run -it --rm \\ --name samba \\ -p 139:139 -p 445:445 \\ -v mnt:/mnt:z \\ dperson/samba \\ -p -s &quot;Mount;/mnt;yes;no;yes&quot; -u &quot;bob;bobspasswd&quot; -g &quot;log level = 5&quot;   note -s &quot;&lt;Mount;/mnt&gt;;yes;no;yes&quot; means [browsable:yes;readonly:no;guest:yes]&quot;, which will allow the guest to read and the user to read/write!  Get the samba server IP address:  $ docker inspect \\ -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \\ samba 172.17.0.2   ","version":"Next","tagName":"h2"},{"title":"Start Samba Client in Docker​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/docs/docker/docker-setup-samba-server#start-samba-client-in-docker","content":" Use dperson/samba or busybox image,  docker run -it --rm --privileged dperson/samba bash   or  docker run -it --rm --privileged busybox sh   Inside the container:  mkdir /smb_share # mount -t cifs //[server-ip]/[share-path] /[mount-point] mount -t cifs //172.17.0.2/Mount /smb_share -o rw,username=bob,password=bobspasswd # write file echo &quot;xxxx&quot; &gt; /smb_share/f.txt   ","version":"Next","tagName":"h2"},{"title":"Start Samba Client which Create Volume in Docker​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/docs/docker/docker-setup-samba-server#start-samba-client-which-create-volume-in-docker","content":" Create a CIFS/Samba Volume  docker volume create \\ --driver local \\ --opt type=cifs \\ --opt device=//172.17.0.2/Mount \\ --opt o=username=bob,password=bobspasswd \\ --name samba-volume   $ docker inspect samba-volume [ { &quot;CreatedAt&quot;: &quot;2023-08-13T16:24:03Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/samba-volume/_data&quot;, &quot;Name&quot;: &quot;samba-volume&quot;, &quot;Options&quot;: { &quot;device&quot;: &quot;//172.17.0.2/Mount&quot;, &quot;o&quot;: &quot;addr=username=bob,password=bobspasswd&quot;, &quot;type&quot;: &quot;cifs&quot; }, &quot;Scope&quot;: &quot;local&quot; } ]   Start a container using the created volume samba-volume.  docker run -it --rm \\ -v samba-volume:/mnt \\ busybox \\ sh   ","version":"Next","tagName":"h2"},{"title":"Use Case in Docker Compose​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/docs/docker/docker-setup-samba-server#use-case-in-docker-compose","content":" samba/docker-compose.yml loading... View on GitHub  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Set Up Samba Server in Docker","url":"/docs/docker/docker-setup-samba-server#troubleshooting","content":" When you mount an Samba Share in Linux, you may encounter error like failed: Invalid argument,  bash-5.1# mount -t cifs //172.17.0.2/Mount /mnt/smb_share -o iocharset=utf8,rw,vers=1.0 mount: mounting //172.17.0.2/Mount on /mnt/smb_share failed: Invalid argument   You can use dmesg to debug,  bash-5.1# dmesg [317258.750535] CIFS: Attempting to mount \\\\172.17.0.2\\Mount [317258.752956] CIFS: VFS: No username specified [317336.240984] cifs: Unknown parameter 'passwd' [317344.451345] CIFS: Attempting to mount \\\\172.17.0.2\\Mount  ","version":"Next","tagName":"h2"},{"title":"Install FFmpeg on Nvidia CUDA Container","type":0,"sectionRef":"#","url":"/docs/how-to/ffmpeg-on-cuda-container","content":"","keywords":"FFmpeg on CUDA Container","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Install FFmpeg on Nvidia CUDA Container","url":"/docs/how-to/ffmpeg-on-cuda-container#prerequisites","content":" Make sure Nvidia GPU Driver is installed in your host machine! As it will be mounted into the container.  Use ldconfig to check if the required Nvidia GPU driver libraries are available inside the container. Such as,  ldconfig -p | grep libcuda   note When running in the nvidia/cuda Docker container, what Nvidia libraries(from the host machine) should be mounted inside the container are specified by the NVIDIA_DRIVER_CAPABILITIES env variable, see driver-capabilities. Here for FFmpeg to employ GPU, it should be included at least as NVIDIA_DRIVER_CAPABILITIES=video,utility.  ","version":"Next","tagName":"h2"},{"title":"Step by Step​","type":1,"pageTitle":"Install FFmpeg on Nvidia CUDA Container","url":"/docs/how-to/ffmpeg-on-cuda-container#step-by-step","content":" docker run --rm --runtime=nvidia \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \\ nvidia/cuda nvidia-smi   docker run --rm --runtime=nvidia \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \\ nvidia/cuda bash   ","version":"Next","tagName":"h2"},{"title":"Complete Dockerfile​","type":1,"pageTitle":"Install FFmpeg on Nvidia CUDA Container","url":"/docs/how-to/ffmpeg-on-cuda-container#complete-dockerfile","content":" nvidia-cuda-ffmpeg/Dockerfile loading... View on GitHub  ","version":"Next","tagName":"h2"},{"title":"Known issues​","type":1,"pageTitle":"Install FFmpeg on Nvidia CUDA Container","url":"/docs/how-to/ffmpeg-on-cuda-container#known-issues","content":" Nvidia Docker encoding stops after long running time with such error message: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected.  [Issue]: NVidia Docker transcoding randomly stops working after 5 minutes to 4 hours later. · Issue #9287 · jellyfin/jellyfin · GitHub  Possible solution:  Edit /etc/defautls/grub,  GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash systemd.unified_cgroup_hierarchy=0&quot;   Then run update-grub and reboot. ","version":"Next","tagName":"h2"},{"title":"How to intercept HTTPs traffic from Android Emulator","type":0,"sectionRef":"#","url":"/docs/how-to/how-to-intercept-https-traffic-from-android-emulator","content":"","keywords":"intercept HTTPs traffic Android Emulator","version":"Next"},{"title":"Resources​","type":1,"pageTitle":"How to intercept HTTPs traffic from Android Emulator","url":"/docs/how-to/how-to-intercept-https-traffic-from-android-emulator#resources","content":" https://medium.com/hackernoon/intercept-https-traffic-on-a-android-emulator-46023f17f6b3  https://httptoolkit.com/docs/guides/android/  https://proxyman.io/posts/2020-09-19-Intercept-https-traffic-on-android-emulator  https://kpj.github.io/misc/InterceptingHTTPTraffic.html  https://www.reddit.com/r/androiddev/comments/17nfwyn/easiest_way_to_inspect_network_traffic_coming/  https://docs.telerik.com/fiddler-everywhere/capture-traffic/capture-from-android  https://www.linkedin.com/pulse/intercept-sslhttps-traffic-perform-penetration-testing-mayank-grover/  https://www.reddit.com/r/androiddev/comments/14x8eed/way_or_viewing_network_requests/  https://beguier.eu/nicolas/articles/android-mitm-intercept-trafic.html ","version":"Next","tagName":"h2"},{"title":"Generate self-signed SSL/TLS certificate for local IP address or local domain","type":0,"sectionRef":"#","url":"/docs/how-to/how-to-create-self-signed-certificate","content":"","keywords":"Self Signed Certificate","version":"Next"},{"title":"1. Create a local CA​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#1-create-a-local-ca","content":" Generate a file RootCA.key and a file RootCA.crt of our local root CA:  openssl req -x509 -nodes -new -sha256 -days 1024 -newkey rsa:2048 -keyout RootCA.key -out RootCA.crt -subj &quot;/C=US/CN=Example-Root-CA&quot;   You can change Example-Root-CA to others or add more fields to CA.  [Optional] Create a CA with a configuration file,  openssl req -x509 -nodes -new -keyout RootCA.key -out RootCA.crt -config &lt;(cat &lt;&lt;EOF [ req ] default_bits = 2048 default_md = sha256 default_days = 3650 prompt = no distinguished_name = req_distinguished_name [ req_distinguished_name ] C = US ST = California L = San Francisco O = Example Corp OU = IT Department CN = www.example.com emailAddress = admin@example.com EOF )   note process substitution does not work in bash scripts!  ","version":"Next","tagName":"h2"},{"title":"2. Create a signed certificate for the local server​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#2-create-a-signed-certificate-for-the-local-server","content":" Next, we should apply the local CA to sign a certificate for our local server using the server's Certificate Signing Request (CSR) , which will be accessed through the localhost or 127.0.0.1 from our local machine.  ","version":"Next","tagName":"h2"},{"title":"2.1. Generate the server's CSR​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#21-generate-the-servers-csr","content":" When generating the CSR file with OpenSSL, we can either specify certain details directly in the command line or use a configuration file. While you can provide some information via command-line arguments, complex configurations like specifying [v3_req] and [alt_names] in belowing are typically done through a configuration file.  subjectAltName in [v3_req] will let you specify more than domain/IP addresses as Subject Alternative Names (SANs). And now modern browser require the server certificate to include SANs. Usage of the Common Name (CN) alone is not considered secure enough, and omitting SANs may result a certificate validation error!  Here is an example for using pure arguments to specify -subj:  openssl req -new -newkey rsa:2048 -nodes -keyout privkey.pem -out csr.pem -subj &quot;/C=US/ST=California/L=San Francisco/O=Example Corp/OU=IT Department/CN=www.example.com/emailAddress=admin@example.com&quot;   Here is an example to use the configuration file as -config.  [Optional] Customize DNS by editing /etc/hosts (Other machines also have to do this if they would like to visit the server),  /etc/hosts 127.0.0.1 localhost 127.0.0.1 fake1.local 127.0.0.1 fake2.local   Create a configure file for CSR including typical sections as:  localhost.conf [req] default_bits = 2048 distinguished_name = req_distinguished_name req_extensions = v3_req prompt = no [req_distinguished_name] countryName = XX stateOrProvinceName = N/A localityName = N/A organizationName = Self-signed certificate commonName = 127.0.0.1: Self-signed certificate [v3_req] subjectAltName = @alt_names [alt_names] IP.1 = 127.0.0.1 DNS.1 = localhost DNS.2 = fake1.local DNS.3 = fake2.local   Generates the CSR  To generate a CSR using the configuration file with OpenSSL, you can use the following command:  openssl req -new -nodes -keyout localhost.key -out localhost.csr -config localhost.conf   [Optional] mix -subj and -config, to be short like:  SAN_LIST=&quot;[SAN]\\nsubjectAltName=DNS:localhost, DNS:*.localhost, IP:127.0.0.1&quot; openssl req -new -nodes -newkey rsa:2048 -keyout localhost.key -out localhost.csr -subj &quot;/C=US/ST=YourState/L=YourCity/O=Example CORP/CN=localhost.local&quot; -reqexts SAN -config &lt;(echo -e &quot;$SAN_LIST&quot;)   Here, localhost.conf is the configuration file and the two outputs are:  localhost.key is the private key file for the local server to communicate with the clients securely.localhost.csr is the CSR file that the local server will use in the next step to sign its certificate from the local CA.  Verify the CSR file localhost.csr:  openssl req -text -noout -verify -in localhost.csr   ","version":"Next","tagName":"h3"},{"title":"2.2. Sign the CSR with the local CA​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#22-sign-the-csr-with-the-local-ca","content":" Now, it's time for local CA to sign a certificate for our local server by using the server's CSR localhost.csr file, thereby issuing a signed certificate.  Generates localhost.crt by using CSR localhost.csr,  openssl x509 -req -sha256 -days 1024 -in localhost.csr -CA RootCA.crt -CAkey RootCA.key -CAcreateserial -extensions v3_req -extfile localhost.conf -out localhost.crt   info -extensions v3_req -extfile localhost.conf: openssl x509 will contain subjectAltName extension in the certificate.  warning If X509 extensions(subjectAltName) are missing from the certificate, the browser will still report security issues, such as its security certificate does not specify Subject Alternative Names.  Or after OpenSSL v3, you can copy the extensions specified in the CSR to the certificate by openssl x509 as this:  openssl x509 -req -sha256 -days 1024 -in localhost.csr -CA RootCA.crt -CAkey RootCA.key -CAcreateserial -copy_extensions copyall -out localhost.crt   View the generated localhost.crt:  openssl x509 -text -noout -in localhost.crt   Verify the generatedlocalhost.crt:  openssl verify -verbose -CAfile RootCA.crt localhost.crt   note If the CA and Subject are the same one, the step of creating the local CA can be skipped. openssl req -x509 -nodes -days 730 -newkey rsa:2048 -keyout localhost.key -out localhost.crt -config localhost.conf   ","version":"Next","tagName":"h3"},{"title":"3. Use the signed certificate in the local server​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#3-use-the-signed-certificate-in-the-local-server","content":" Run up a node https server to use the signed certificate for the local sever.  npx http-server -p 8082 --ssl --cert localhost.crt --key localhost.key   Then visit:  https://127.0.0.1:8082/https://localhost:8082/  The browser will give you security warning as the local root CA is not trusted in default.  ","version":"Next","tagName":"h2"},{"title":"4. Install the local CA​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#4-install-the-local-ca","content":" TO trust the root local CA, we must install the local CA certificate RootCA.crt into each system trust store or each browser.  Windows system trust storeUbuntu system trust storemacOS system trust store  Then visit again, the browser will show green!  ","version":"Next","tagName":"h2"},{"title":"[Optional] Sign the CSR with openssl ca​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#optional-sign-the-csr-with-openssl-ca","content":" For complicated configuration for CA to sign a certificate, you can use openssl ca and the configuration file is like:  cd /path/to/your/ca/ mkdir -p newcerts touch index.txt echo 1000 &gt; serial   cat &lt;&lt;EOF &gt; /path/to/your/ca/openssl.cnf [ ca ] default_ca = CA_default [ CA_default ] dir = /path/to/your/ca database = $dir/index.txt new_certs_dir = $dir/newcerts certificate = $dir/RootCA.crt serial = $dir/serial private_key = $dir/RootCA.key default_days = 365 default_md = sha256 policy = policy_any x509_extensions = usr_cert copy_extensions = copy [ policy_any ] countryName = supplied stateOrProvinceName = supplied organizationName = supplied organizationalUnitName = optional commonName = supplied emailAddress = optional [ usr_cert ] basicConstraints=CA:FALSE nsCertType = client, email nsComment = &quot;OpenSSL Generated Certificate&quot; subjectKeyIdentifier=hash authorityKeyIdentifier=keyid,issuer EOF   openssl ca -config /path/to/your/ca/openssl.cnf -in localhost.csr -out localhost.crt -batch   ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Chrome red security warning​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#chrome-red-security-warning","content":" Go to Developer Tools.Click Security tab.Check Security overview issues.  ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Generate self-signed SSL/TLS certificate for local IP address or local domain","url":"/docs/how-to/how-to-create-self-signed-certificate#resources","content":" How to create an HTTPS certificate for localhost domains · GitHub  How to add X.509 extensions to certificate OpenSSL | GoLinuxCloud  GitHub - FiloSottile/mkcert: A simple zero-config tool to make locally trusted development certificates with any names you'd like. ","version":"Next","tagName":"h2"},{"title":"How to mount ISO image file","type":0,"sectionRef":"#","url":"/docs/how-to/how-to-mount-iso-file","content":"","keywords":"How to mount ISO file","version":"Next"},{"title":"Mount ISO file on Linux​","type":1,"pageTitle":"How to mount ISO image file","url":"/docs/how-to/how-to-mount-iso-file#mount-iso-file-on-linux","content":" ","version":"Next","tagName":"h2"},{"title":"Mount ISO file on macOS​","type":1,"pageTitle":"How to mount ISO image file","url":"/docs/how-to/how-to-mount-iso-file#mount-iso-file-on-macos","content":" Attaching as a block device  # the '-nomount' option avoids the 'mount failed' error ❯ hdiutil attach -nomount mantic-mini-iso-amd64.iso /dev/disk6 GUID_partition_scheme /dev/disk6s1 Microsoft Basic Data /dev/disk6s2 EFI /dev/disk6s3 Microsoft Basic Data   ❯ diskutil info /dev/disk6s2 Device Identifier: disk6s2 Device Node: /dev/disk6s2 Whole: No Part of Whole: disk6 Volume Name: ESP Mounted: No Partition Type: EFI File System Personality: MS-DOS FAT12 Type (Bundle): msdos Name (User Visible): MS-DOS (FAT12)   [Optional] Load CD9660  # Load the kext module ❯ sudo kmutil load -p /System/Library/Extensions/cd9660.kext   Mount the disk with cd9660 (aka ISO9660) file system  # create mount point ❯ mkdir -p /tmp/ubuntu-mantic-iso # mount the disk ❯ mount -t cd9660 /dev/disk6 /tmp/ubuntu-mantic-iso   View the iso files,  ❯ tree -h -L 3 /tmp/ubuntu-mantic-iso [2.0K] /tmp/ubuntu-mantic-iso ├── [2.0K] EFI │ └── [2.0K] boot │ ├── [938K] bootx64.efi │ ├── [2.2M] grubx64.efi │ └── [841K] mmx64.efi ├── [2.0K] boot │ └── [2.0K] grub │ ├── [2.0K] fonts │ ├── [ 169] grub.cfg │ ├── [ 38K] i386-pc │ └── [ 36K] x86_64-efi ├── [2.0K] boot.catalog └── [2.0K] casper ├── [ 56M] initrd └── [ 13M] vmlinuz 9 directories, 7 files   Umount the disk  ❯ umount /dev/disk6   Detach the disk  ❯ hdiutil detach /dev/disk6  ","version":"Next","tagName":"h2"},{"title":"QEMU Direct Linux Kernel Boot","type":0,"sectionRef":"#","url":"/docs/how-to/qemu-linux-kernel-boot","content":"","keywords":"QEMU Linux Kernel Boot","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"QEMU Direct Linux Kernel Boot","url":"/docs/how-to/qemu-linux-kernel-boot#prerequisites","content":" On Ubuntu,  sudo apt-get install git fakeroot build-essential ncurses-dev xz-utils libssl-dev bc flex libelf-dev bison   On macOS, you need create a Case Sensitive filesystem and use GNU GCC instead of Clang the following ways:  hdiutil create -size 20g -type SPARSE -fs &quot;Case-sensitive HFS+&quot; -volname brosx brosx.sparseimage hdiutil attach brosx.sparseimage   hdiutil detach /Volumes/brosx -force   brew install gpatch gcc flock attr libtool libart   ln -s /opt/homebrew/bin/gcc-13 /opt/homebrew/bin/gcc n -s /opt/homebrew/bin/gcc-13 /opt/homebrew/bin/cc ln -s /opt/homebrew/bin/g++-13 /opt/homebrew/bin/g++ ln -s /opt/homebrew/bin/g++-13 /opt/homebrew/bin/c++   rm /opt/homebrew/bin/gcc /opt/homebrew/bin/cc /opt/homebrew/bin/g++ /opt/homebrew/bin/c++   ","version":"Next","tagName":"h2"},{"title":"Build Linux kernel​","type":1,"pageTitle":"QEMU Direct Linux Kernel Boot","url":"/docs/how-to/qemu-linux-kernel-boot#build-linux-kernel","content":" wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.1.55.tar.xz   tar xvf linux-6.1.55.tar.xz cd linux-6.1.55   # Use the default `x86_64` configuration file form `/x86/configs/x86_64_defconfig` make ARCH=x86_64 x86_64_defconfig   # Tweak some options for GDB and initramfs make menuconfig   make -j8   Generate kernel file ./arch/x86/boot/bzImage.  note To extract vmlinux from bzImage, ./scripts/extract-vmlinux ./arch/x86_64/boot/bzImage &gt;./arch/x86_64/boot/vmlinux   ","version":"Next","tagName":"h2"},{"title":"Build root filesystem​","type":1,"pageTitle":"QEMU Direct Linux Kernel Boot","url":"/docs/how-to/qemu-linux-kernel-boot#build-root-filesystem","content":" git clone https://github.com/buildroot/buildroot.git cd buildroot   make menuconfig   Choose x86_64 as Target Architecture and ext4 root file system.  make -j8   Generate root filesystem disk ./output/images/rootfs.ext4.  ","version":"Next","tagName":"h2"},{"title":"Run QEMU​","type":1,"pageTitle":"QEMU Direct Linux Kernel Boot","url":"/docs/how-to/qemu-linux-kernel-boot#run-qemu","content":" Copy bzImage and rootfs.ext4 to any host machine with QEMU available.  rsync -l ./linux-6.1.55/arch/x86/boot/bzImage destination_directory/ rsync -l ./buildroot/output/images/rootfs.ext4 destination_directory/   kernel=&quot;$PWD/linux_qemu/x86_64/bzImage&quot; vmlinuz=&quot;$PWD/linux_qemu/x86_64/vmlinux&quot; initrd=&quot;$PWD/linux_qemu/x86_64/rootfs.ext4&quot; img=&quot;$PWD/linux_qemu/x86_64/rootfs.ext4&quot;   qemu-system-x86_64 \\ -nographic \\ -m 4G \\ -kernel $kernel \\ -append &quot;earlyprintk loglevel=8 root=/dev/zero console=ttyS0&quot;   qemu-system-x86_64 \\ -nographic \\ -m 4G \\ -kernel $kernel \\ -hda $img \\ -append &quot;earlyprintk loglevel=8 root=/dev/sda rootfstype=ext4 console=ttyS0&quot; \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22 \\ -device virtio-net-pci,netdev=mynet   Default password: root  ","version":"Next","tagName":"h2"},{"title":"Debug Linux kernel​","type":1,"pageTitle":"QEMU Direct Linux Kernel Boot","url":"/docs/how-to/qemu-linux-kernel-boot#debug-linux-kernel","content":" qemu-system-x86_64 \\ -s -S \\ -nographic \\ -m 4G \\ -kernel $kernel \\ -append &quot;earlyprintk loglevel=8 root=/dev/zero console=ttyS0 nokaslr&quot;   Options in details,  -s: allows port tcp::1234 for remote debug-S: stop CPU until continue from GDB what is connected to tcp 1234 port-append nokaslr: turn off KASLR  Or with root filesystem,  qemu-system-x86_64 \\ -nographic \\ -m 4G \\ -s -S \\ -kernel $kernel \\ -hda $img \\ -append &quot;earlyprintk loglevel=8 root=/dev/sda rootfstype=ext4 console=ttyS0 nokaslr&quot; \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22 \\ -device virtio-net-pci,netdev=mynet   Enter gdb,  $ gdb ./vmlinux   In gdb shell,  (gdb) target remote 10.6.64.243:1234 Remote debugging using 10.6.64.243:1234 warning: No executable has been specified and target does not support determining executable automatically. Try using the &quot;file&quot; command. 0x000000000000fff0 in ?? () (gdb) continue Continuing.   ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"QEMU Direct Linux Kernel Boot","url":"/docs/how-to/qemu-linux-kernel-boot#resources","content":" Daniel P. Berrangé » Blog Archive » make-tiny-image.py: creating tiny initrds for testing QEMU or Linux kernel/userspace behaviour  GitHub - dhruvvyas90/qemu-rpi-kernel: Qemu kernel for emulating Rpi on QEMUhttps://medicineyeh.wordpress.com/2016/03/29/buildup-your-arm-image-for-qemu/  Prepare the environment for developing Linux kernel with qemu. | by DaeSeok Youn | Medium   ","version":"Next","tagName":"h2"},{"title":"FFmpeg Command Samples","type":0,"sectionRef":"#","url":"/docs/how-to/ffmpeg-command-samples","content":"","keywords":"learn ffmpeg","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#overview","content":" Examples of Container and Codec lists in Chrome1:  Video Container Format:  MP4 [.mp4 file extension]OggWebMWAVHLS [.m3u8 file extension]  Video Codec Format:  VP8VP9H.264 [Chrome only]H.265 [Chrome only and also only with the underlying OS support]MPEG-4 [Chrome OS only, aka Xvid, DivX]  ","version":"Next","tagName":"h2"},{"title":"About FFmpeg Command​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#about-ffmpeg-command","content":" The common patter of ffmpeg looks like:  ffmpeg [input_options] -i input.mp4 [output_options] output.mp4   In short:  The [input_options] before -i input.mp4 are options used for decoding the videoThe [output_options] before output.mp4 are options used for encoding the video  ","version":"Next","tagName":"h2"},{"title":"FFmpeg Command Examples​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#ffmpeg-command-examples","content":" It's note worthing to look over FFmpeg Wiki 2  ","version":"Next","tagName":"h2"},{"title":"List all available container formats​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#list-all-available-container-formats","content":" ffmpeg -formats   ","version":"Next","tagName":"h3"},{"title":"List all available codec formats​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#list-all-available-codec-formats","content":" ffmpeg -codecs   ","version":"Next","tagName":"h3"},{"title":"List all available encoder or decoder​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#list-all-available-encoder-or-decoder","content":" ffmpeg -encoders ffmpeg -decoders   # Show available `presets` ffmpeg -h encoder=h264_nvenc   ","version":"Next","tagName":"h3"},{"title":"List all frames timestamp​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#list-all-frames-timestamp","content":" ffprobe -select_streams v -show_entries frame=pict_type,pts_time -of csv=p=0 -i input.mp4   ","version":"Next","tagName":"h3"},{"title":"List all keyframe(I-frame) timestamp​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#list-all-keyframei-frame-timestamp","content":" ffprobe -select_streams v -show_entries frame=pict_type,pts_time -of csv=p=0 -skip_frame nokey -i input.mp4   ","version":"Next","tagName":"h3"},{"title":"Read video information json output​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#read-video-information-json-output","content":" ffprobe -v quiet -show_streams -select_streams v:0 -print_format json video.mp4   ","version":"Next","tagName":"h3"},{"title":"Transcode video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#transcode-video","content":" There are three possible and reasonable methods for transcoding:  software decoding and software encodingsoftware decoding and hardware encodinghardware decoding and hardware encoding  you can convert either the container formats or the codecs formats, such as:  # To `mp4` container and `h.264` codecs(the lower crf, the higher quality) ffmpeg -i input.avi -c:v libx264 -preset fast -crf 23 output.mp4 # To `mp4` container and `mpeg4` codecs ffmpeg -i input.avi -c:v libxvid -preset fast output.mp4 # To be friendly for streaming, adding necessary metadata to begin playback faster! ffmpeg -i input.avi -c:v libx264 -preset fast -crf 23 -movflags +faststart output.mp4 # Remove audio ffmpeg -i input.avi -c:v libx264 -preset fast -crf 23 -an output.mp4 # Use NVIDIA GPU ffmpeg -i input.avi -c:v h264_nvenc -preset fast output.avi # keep quality ffmpeg -i input.avi -c:v h264_nvenc -preset fast -rc constqp -cq 19 output.avi   ","version":"Next","tagName":"h3"},{"title":"Set keyframe interval​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#set-keyframe-interval","content":" # mpeg4 ffmpeg -i input.avi -vcodec libxvid -preset fast -g 10 -keyint_min 10 -sc_threshold 0 output.avi # NVIDIA GPU # This sets the I-frame interval at 10 and ensures that no I-frames will be inserted in scene changes ffmpeg -i input.avi -vcodec h264_nvenc -preset fast -g 10 -keyint_min 10 -sc_threshold 0 output.avi   ","version":"Next","tagName":"h3"},{"title":"Clip video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#clip-video","content":" # Fast clip with stream copy and faster seeking(700x) ffmpeg -ss 00:00:10 -i video.mp4 -to 00:00:50 -c:v copy output.mp4 # Fast clip with stream copy and slower seeking(600x) ffmpeg -i video.mp4 -ss 00:00:10 -to 00:00:50 -c:v copy output.mp4 # Slow clip with re-encoding and faster seeking(1x) ffmpeg -ss 00:00:10 -i video.mp4 -to 00:00:50 -c:v libx264 output.mp4   Use filter(Slow)  ffmpeg -y -i input.mp4 -an -c:v libx264 -filter:v &quot;trim=start=10:end=30&quot; output.mp4 # remove the black video ffmpeg -y -i input.mp4 -an -c:v libx264 -filter:v &quot;trim=start=10:end=30,setpts=PTS-STARTPTS&quot; output.mp4   NOTE: Cutting video with stream copy will lead the start frame is not precise!  ","version":"Next","tagName":"h3"},{"title":"Slow down/Speed up video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#slow-downspeed-up-video","content":" # Slow down to 1/2x in fast way ffmpeg -y -itsscale 2 -i video.mp4 -c:v copy output.mp4 # Speed up to 2x in fast way ffmpeg -y -itsscale 0.5 -i video.mp4 -c:v copy output.mp4 # Speed up to 2x with re-encoding in slow way ffmpeg -y -itsscale 0.5 -i video.mp4 -c:v libx264 output.mp4 # Speed up to 2x with `setpts filter`(which requires re-encoding) in slow way ffmpeg -i video.mp4 -filter:v &quot;setpts=0.5*PTS&quot; output.mp4 # Change fps to slow down/speed up but keeping duration ffmpeg -i video.mp4 -filter:v &quot;fps=30&quot; output.mp4   ","version":"Next","tagName":"h3"},{"title":"Draw region of interest(ROI) on a video​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#draw-region-of-interestroi-on-a-video","content":" # Draw one drawbox ffmpeg -i input.mp4 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5&quot; output.mp4 ffmpeg -i input.mp4 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5,drawtext=text='Test Text':x=100:y=100:fontsize=24:fontcolor=yellow:box=1:boxcolor=yellow&quot; output.mp4 ffmpeg -y -ss 30 -noaccurate_seek -i input.mp4 -t 10 -c:v libx264 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5,drawtext=text='Test Text':x=100:y=(100-text_h):fontsize=24:fontcolor=black:box=1:boxcolor=red:boxborderw=2&quot; output.mp4 # Trim video and draw box ffmpeg -y -i input.mp4 -an -c:v libx264 -filter:v &quot;trim=start=10:end=30,drawbox=x=100:y=100:w=200:h=150:color=red@0.5:enable='between(t,10,15)',setpts=PTS-STARTPTS&quot; output.mp4 ffmpeg -y -i input.mp4 -i overlay_video.mp4 -filter_complex &quot;[0:v][1:v]overlay=0:0:enable='between(t,0,25)'&quot; output.mp4   # Draw different `drawbox` at different time on video from a file(using `timeline` feature) # See timeline: https://ffmpeg.org/ffmpeg-filters.html#Timeline-editing # See expression: https://ffmpeg.org/ffmpeg-utils.html#Expression-Evaluation ffmpeg -i input.mp4 -filter_complex_script timeline.txt output.mp4 # `timeline.txt` look like: [0:v]drawbox=x=100:y=100:w=200:h=150:color=red@0.5:enable='between(t,0,21)'[box1]; [box1]drawbox=x=300:y=200:w=150:h=100:color=green@0.5:t=:enable='between(t,21,40)'[box2]; [box2]drawbox=x=50:y=300:w=300:h=200:color=blue@0.5:t=:enable='between(t,41,60)' # or using `n`: sequential number of the input frame, starting from 0 [0:v]drawbox=x=100:y=100:w=200:h=150:color=red@0.5:n=0:600[box1]; [box1]drawbox=x=300:y=200:w=150:h=100:color=green@0.5:n=601:1200[box2]; [box2]drawbox=x=50:y=300:w=300:h=200:color=blue@0.5:n=1201:1800   # Draw different `drawbox` at different time on video ffmpeg -i input.mp4 -filter_complex &quot;[0:v]drawbox=x=100:y=100:w=200:h=150:color=red:t=8:enable='between(t,0,21)'[box1];[box1]drawbox=x=300:y=200:w=150:h=100:color=green:t=8:enable='between(t,21,40)'[box2];[box2]drawbox=x=50:y=300:w=300:h=200:color=blue:t=8:enable='between(t,41,60)'&quot; output.mp4   ","version":"Next","tagName":"h3"},{"title":"Pipe ffmpeg​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#pipe-ffmpeg","content":" The FFmpeg Pipe is very useful in IPC for communicating FFmpeg with another process. For instance, an application generates images to Pipe stdin which FFmpeg reads and encodes into a video.  A real-life scenario that FFmpeg read from Pipe: An application applied OpenCV to process images for object detection, and it will draw ROI but lacks ability to encode a video as efficiently as FFmpeg does. So it's somewhat ideal to pipe these images to FFmpeg that can encode the video by leveraging hardware acceleration(GPU) capability.  However, FFmpeg can also write to a Pipe.  # It works in Linux and Windows(`cmd`, does not work in `PS`) ffmpeg -ss 00:00:10 -i video.mp4 -to 00:00:20 -an -c:v copy -f h264 pipe: | ffmpeg -y -i pipe: -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red&quot; output.mp4 ffmpeg -i input.mp4 -c:v rawvideo -pix_fmt bgr24 -r 60 -f rawvideo pipe: | ffmpeg -y -f rawvideo -pix_fmt bgr24 -s 1920x1080 -r 60 -i pipe: -pix_fmt yuv420p -c:v h264_nvenc foo.mp4 ffmpeg -i input.mp4 -pix_fmt yuv420p -r 60 -f rawvideo pipe: | ffmpeg -y -f rawvideo -pix_fmt yuv420p -s 1920x1080 -r 60 -i pipe: -c:v h264_nvenc foo.mp4 ffmpeg -i input.mp4 -an -f h264 pipe: | ffmpeg -y -f h264 -i pipe: -c:v h264_nvenc foo.mp4   ","version":"Next","tagName":"h3"},{"title":"Use testsrc​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#use-testsrc","content":" ffmpeg -y -f lavfi -i testsrc=duration=10:size=1920x1080:rate=60 -c:v libx264 -pix_fmt yuv420p testsrc.mp4   ","version":"Next","tagName":"h3"},{"title":"Split and Concatenate​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#split-and-concatenate","content":" ffmpeg -y -i input.mp4 -ss 0 -to 10 -c:v copy part1.mp4 ffmpeg -y -i input.mp4 -ss 10 -to 15 -c:v copy part2.mp4 ffmpeg -y -i input.mp4 -ss 15 -c:v copy part3.mp4   ffmpeg -y -i part2.mp4 -filter:v &quot;drawbox=x=100:y=100:w=200:h=150:color=red@0.5&quot; part2-draw.mp4   Slow,  ffmpeg -y -i part1.mp4 -i part2-draw.mp4 -i part3.mp4 -filter_complex &quot;[0:v][1:v][2:v]concat=n=3:v=1:a=0[outv]&quot; -map &quot;[outv]&quot; output.mp4   Fast(Concat protocol),  ffmpeg -i part1.mp4 -c copy part1.ts ffmpeg -i part2-draw.mp4 -c copy part2-draw.ts ffmpeg -i part3.mp4 -c copy part3.ts ffmpeg -y -i &quot;concat:part1.ts|part2-draw.ts|part3.ts&quot; -c:v copy output.mp4   Fast(Concat demuxer),  ffmpeg -y -f concat -i concat.txt -c:v copy output.mp4 # concat.txt file 'part1.mp4' file 'part2-draw.mp4' file 'part3.mp4' # Or avoid creating the input file # bash ffmpeg -y -f concat -safe 0 -i &lt;(echo &quot;file '$PWD/part1.mp4'&quot;;echo &quot;file '$PWD/part2-draw.mp4'&quot;;echo &quot;file '$PWD/part3.mp4'&quot;;) -c:v copy output.mp4 # cmd ffmpeg -y -f concat -safe 0 -i &lt;(@echo &quot;file '$PWD/part1.mp4'&quot;;@echo &quot;file '$PWD/part2-draw.mp4'&quot;;@echo &quot;file '$PWD/part3.mp4'&quot;;) -c:v copy output.mp4   https://trac.ffmpeg.org/wiki/Concatenate  ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"FFmpeg Command Samples","url":"/docs/how-to/ffmpeg-command-samples#references","content":"   Footnotes​ Chrome Audio/Video Support ↩ FFmpeg Wiki ↩ ","version":"Next","tagName":"h2"},{"title":"MDX Features of Docusaurus","type":0,"sectionRef":"#","url":"/docs/docusaurus-mdx-features","content":"","keywords":"mdx features in docusaurus","version":"Next"},{"title":"React Component​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#react-component","content":" Code as:  docs/mdx-features.mdx export const Highlight = ({children, color}) =&gt; ( &lt;span style={{ backgroundColor: color, borderRadius: '2px', color: '#fff', padding: '0.2rem', }}&gt; {children} &lt;/span&gt; ); &lt;Highlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/Highlight&gt; and &lt;Highlight color=&quot;#1877F2&quot;&gt;Facebook blue&lt;/Highlight&gt; are my favorite colors.   Render as:    Docusaurus green and Facebook blue are my favorite colors.        Code as:  src/components/Highlight import React from 'react'; export default function SharedHighlight({children, color}) { return ( &lt;span style={{ backgroundColor: color, borderRadius: '2px', color: '#fff', padding: '0.2rem', }}&gt; {children} &lt;/span&gt; ); }   With:  docs/mdx-features.mdx import SharedHighlight from '@site/src/components/Highlight'; &lt;SharedHighlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/SharedHighlight&gt; I can write **Markdown** alongside my _JSX_!   Render as:    Docusaurus green  I can write Markdown alongside my JSX!  ","version":"Next","tagName":"h2"},{"title":"Tabs​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#tabs","content":" Code as:  docs/mdx-features.mdx import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; &lt;Tabs&gt; &lt;TabItem value=&quot;apple&quot; label=&quot;Apple&quot; default&gt; This is an apple 🍎 &lt;/TabItem&gt; &lt;TabItem value=&quot;orange&quot; label=&quot;Orange&quot;&gt; This is an orange 🍊 &lt;/TabItem&gt; &lt;TabItem value=&quot;banana&quot; label=&quot;Banana&quot;&gt; This is a banana 🍌 &lt;/TabItem&gt; &lt;/Tabs&gt;   Render as:    AppleOrangeBanana This is an apple 🍎  ","version":"Next","tagName":"h2"},{"title":"NOTES​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#notes","content":" docs/mdx-features.mdx &lt;!-- Prettier doesn't change this --&gt; :::note Hello world :::   note Hello world  &lt;!-- Prettier changes this --&gt; :::note Hello world :::   &lt;!-- to this --&gt; ::: note Hello world:::   docs/mdx-features.mdx :::note[Your Title] Some **content** with _Markdown_ `syntax`. :::   Your Title Some content with Markdown syntax.  docs/mdx-features.mdx :::tip[Use tabs in admonitions] &lt;Tabs&gt; &lt;TabItem value=&quot;apple&quot; label=&quot;Apple&quot;&gt;This is an apple 🍎&lt;/TabItem&gt; &lt;TabItem value=&quot;orange&quot; label=&quot;Orange&quot;&gt;This is an orange 🍊&lt;/TabItem&gt; &lt;TabItem value=&quot;banana&quot; label=&quot;Banana&quot;&gt;This is a banana 🍌&lt;/TabItem&gt; &lt;/Tabs&gt; :::   Use tabs in admonitions AppleOrangeBanana This is an apple 🍎  ","version":"Next","tagName":"h2"},{"title":"Math​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#math","content":" docs/mdx-features.mdx Let $f\\colon[a,b]\\to\\R$ be Riemann integrable. Let $F\\colon[a,b]\\to\\R$ be $F(x)=\\int_{a}^{x} f(t)\\,dt$. Then $F$ is continuous, and at all $x$ such that $f$ is continuous at $x$, $F$ is differentiable at $x$ with $F'(x)=f(x)$.   Let f ⁣:[a,b]→Rf\\colon[a,b]\\to\\Rf:[a,b]→R be Riemann integrable. Let F ⁣:[a,b]→RF\\colon[a,b]\\to\\RF:[a,b]→R beF(x)=∫axf(t) dtF(x)=\\int_{a}^{x} f(t)\\,dtF(x)=∫ax​f(t)dt. Then FFF is continuous, and at all xxx such thatfff is continuous at xxx, FFF is differentiable at xxx with F′(x)=f(x)F'(x)=f(x)F′(x)=f(x).  docs/mdx-features.mdx $$ I = \\int_0^{2\\pi} \\sin(x)\\,dx $$   I=∫02πsin⁡(x) dxI = \\int_0^{2\\pi} \\sin(x)\\,dxI=∫02π​sin(x)dx  ","version":"Next","tagName":"h2"},{"title":"Diagrams​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#diagrams","content":" Example Mermaid diagram ```mermaid graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; ```     ","version":"Next","tagName":"h2"},{"title":"Code Block​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#code-block","content":" ```jsx title=&quot;/src/components/HelloCodeTitle.js&quot; function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; } ```   /src/components/HelloCodeTitle.js function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; }   Syntax highlighting for Other languages by prism: sh, editorconfig, etc.  editorconfig:  ```editorconfig title=&quot;/etc/samba.conf&quot; [documents] path = /data/documents valid users = @simon guest ok = no writable = yes browsable = yes ```   sh:  ```sh $ ls /home ```   ```js function HighlightSomeText(highlight) { if (highlight) { // highlight-next-line return 'This text is highlighted!'; } return 'Nothing highlighted'; } function HighlightMoreText(highlight) { // highlight-start if (highlight) { return 'This range is highlighted!'; } // highlight-end return 'Nothing highlighted'; } ```   function HighlightSomeText(highlight) { if (highlight) { return 'This text is highlighted!'; } return 'Nothing highlighted'; } function HighlightMoreText(highlight) { if (highlight) { return 'This range is highlighted!'; } return 'Nothing highlighted'; }   ```jsx {1,4-6,11} import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ```   import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent;   ```jsx {1,4-6,11} showLineNumbers import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ```   import React from 'react'; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent;   JavaScriptPythonJava function helloWorld() { console.log('Hello, world!'); }   ","version":"Next","tagName":"h2"},{"title":"Importing Code​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#importing-code","content":" npm install --save raw-loader   docs/mdx-features.mdx import CodeBlock from '@theme/CodeBlock'; import MyComponentSource from '!!raw-loader!./myComponent'; &lt;CodeBlock language=&quot;jsx&quot;&gt;{MyComponentSource}&lt;/CodeBlock&gt;     /** * Copyright (c) Facebook, Inc. and its affiliates. * * This source code is licensed under the MIT license found in the * LICENSE file in the root directory of this source tree. */ import React, { useState } from 'react'; export default function MyComponent() { const [bool, setBool] = useState(false); return ( &lt;div&gt; &lt;p&gt;MyComponent rendered !&lt;/p&gt; &lt;p&gt;bool={bool ? 'true' : 'false'}&lt;/p&gt; &lt;p&gt; &lt;button onClick={() =&gt; setBool((b) =&gt; !b)}&gt;toggle bool&lt;/button&gt; &lt;/p&gt; &lt;/div&gt; ); }   ","version":"Next","tagName":"h2"},{"title":"Importing Markdown​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#importing-markdown","content":" docs/mdx-features.mdx import PartialExample from './_markdown-partial-example.mdx'; &lt;PartialExample name=&quot;Sebastien&quot; /&gt;     Hello Sebastien  This is text some content from _markdown-partial-example.mdx.  docs/mdx-features.mdx import PartialExample1 from './wiki/wiki-skia.md'; &lt;PartialExample1 /&gt;   It imports file from wiki-skia.md:    Wiki Skia  What the difference between SkImage/SkPicture/SkCanvas/SkSurface?  SkBitmap based SkCanvas very slow... How to improve draw speeds?  How to move SkImage from CPU to GPU?  How to control the SkImage GPU back cache size?  As far as I understand when I load SkImage from file or SkBitmap the SkImage lives in CPU side memory. Then the moment I draw this SkImage on a GPU backed canvas it will make a copy of the CPU data into a GPU backed texture. So now we technically have two copies available on the SkImage. Then each time I draw that SkImage it will do it quickly cause it's already in the GPU side.  ","version":"Next","tagName":"h2"},{"title":"Import Code Snippets from GitHub Repositories​","type":1,"pageTitle":"MDX Features of Docusaurus","url":"/docs/docusaurus-mdx-features#import-code-snippets-from-github-repositories","content":" A Docusaurus v2 plugin that supports referencing code examples from public GitHub repositories.  src/theme/ReferenceCodeBlock/index.tsx loading... View on GitHub  code-snippets/XKeyIn.cpp loading... View on GitHub ","version":"Next","tagName":"h2"},{"title":"Inspect Shared Library","type":0,"sectionRef":"#","url":"/docs/how-to/inspect-shared-library","content":"","keywords":"debug dynamic library shared library","version":"Next"},{"title":"Using ldd Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-ldd-command","content":" Available in Linux:  ldd /usr/bin/vim linux-vdso.so.1 (0x00007ffc75fb1000) libgtk-3.so.0 =&gt; /usr/lib/libgtk-3.so.0 (0x00007fa4dcb5e000) libgdk-3.so.0 =&gt; /usr/lib/libgdk-3.so.0 (0x00007fa4dca64000) libXau.so.6 =&gt; /usr/lib/libXau.so.6 (0x00007fa4db7a9000) .... liblzma.so.5 =&gt; /usr/lib/liblzma.so.5 (0x00007fa4db63f000) liblz4.so.1 =&gt; /usr/lib/liblz4.so.1 (0x00007fa4db61d000) libgcrypt.so.20 =&gt; /usr/lib/libgcrypt.so.20 (0x00007fa4db4ff000) libgpg-error.so.0 =&gt; /usr/lib/libgpg-error.so.0 (0x00007fa4db4d8000)   ","version":"Next","tagName":"h2"},{"title":"Using objdump Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-objdump-command","content":" Available in Linux:  objdump -p /usr/bin/vim | grep 'NEEDED' NEEDED libpython3.7m.so.1.0 NEEDED libcrypt.so.2 NEEDED libpthread.so.0 NEEDED libdl.so.2 NEEDED libutil.so.1 NEEDED libm.so.6 NEEDED libselinux.so.1 NEEDED libtinfo.so.6 NEEDED libacl.so.1 NEEDED libgpm.so.2 NEEDED libc.so.6   ","version":"Next","tagName":"h2"},{"title":"Using readelf Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-readelf-command","content":" Available in Linux:  readelf --dynamic /usr/bin/vim | grep NEEDED 0x0000000000000001 (NEEDED) Shared library: [libpython3.7m.so.1.0] 0x0000000000000001 (NEEDED) Shared library: [libcrypt.so.2] 0x0000000000000001 (NEEDED) Shared library: [libpthread.so.0] 0x0000000000000001 (NEEDED) Shared library: [libdl.so.2] 0x0000000000000001 (NEEDED) Shared library: [libutil.so.1] 0x0000000000000001 (NEEDED) Shared library: [libm.so.6] 0x0000000000000001 (NEEDED) Shared library: [libselinux.so.1] 0x0000000000000001 (NEEDED) Shared library: [libtinfo.so.6] 0x0000000000000001 (NEEDED) Shared library: [libacl.so.1] 0x0000000000000001 (NEEDED) Shared library: [libgpm.so.2] 0x0000000000000001 (NEEDED) Shared library: [libc.so.6]   ","version":"Next","tagName":"h2"},{"title":"Using otool Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-otool-command","content":" Available in OSX:  otool -L libOpenCvSharpExtern.dylib   ","version":"Next","tagName":"h2"},{"title":"Reading the /proc/<pid>/maps File​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#reading-the-procpidmaps-file","content":" Available in Linux:  cat /proc/179015/maps ... 7f2cb67c3000-7f2cb67c6000 r--p 00000000 08:13 3810274 /usr/lib/libnss_files-2.31.so 7f2cb67c6000-7f2cb67cd000 r-xp 00003000 08:13 3810274 /usr/lib/libnss_files-2.31.so .. 7f2cb6a89000-7f2cb6a8a000 r--p 00002000 08:13 3810903 /usr/lib/libutil-2.31.so 7f2cb6a8a000-7f2cb6a8b000 r--p 00002000 08:13 3810903 /usr/lib/libutil-2.31.so ... 7f2cb9802000-7f2cb9803000 rw-p 00000000 00:00 0 7ffe77658000-7ffe7767a000 rw-p 00000000 00:00 0 [stack] 7ffe776c8000-7ffe776cc000 r--p 00000000 00:00 0 [vvar] 7ffe776cc000-7ffe776ce000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall]   awk '$NF!~/\\.so/{next} {$0=$NF} !a[$0]++' /proc/179015/maps ... /usr/lib/libpython3.8.so.1.0 /usr/lib/libgpg-error.so.0.29.0 /usr/lib/libgcrypt.so.20.2.5 /usr/lib/liblz4.so.1.9.2 /usr/lib/liblzma.so.5.2.5 /usr/lib/libsystemd.so.0.28.0 /usr/lib/libogg.so.0.8.4 /usr/lib/libvorbis.so.0.4.8 /usr/lib/libblkid.so.1.1.0 /usr/lib/libXdmcp.so.6.0.0 /usr/lib/libXau.so.6.0.0 /usr/lib/libdatrie.so.1.3.5 ...   ","version":"Next","tagName":"h2"},{"title":"Using vmmap Command​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-vmmap-command","content":" ","version":"Next","tagName":"h2"},{"title":"Using ctypes in Python​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-ctypes-in-python","content":" import ctypes ctypes.cdll.LoadLibrary(&quot;libOpenCvSharpExtern.so&quot;) ctypes.CDLL(&quot;libOpenCvSharpExtern.so&quot;)   dlopen() DYLD_PRINT_LIBRARIES=1 dlopen_test.out /opt/vcpkg/installed/arm64-osx-dynamic/lib/libpng16.dylib   objdump -p /usr/local/lib/libOpenCvSharpExtern.so   ","version":"Next","tagName":"h2"},{"title":"Using nm​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-nm","content":" Show list of symbols:  ❯ nm -g /opt/vcpkg/installed/arm64-osx-dynamic/lib/libintl.8.dylib U _CFArrayGetCount U _CFArrayGetValueAtIndex U _CFGetTypeID U _CFLocaleCopyPreferredLanguages U _CFPreferencesCopyAppValue U _CFRelease U _CFStringGetCString U _CFStringGetTypeID U __DefaultRuneLocale U ___CFConstantStringClassReference   ","version":"Next","tagName":"h3"},{"title":"Using dumpbin​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-dumpbin","content":" Available in Windows  Show dependent dynamic libraries(DLL):  dumpbin /dependents your_dll_file.dll   ","version":"Next","tagName":"h3"},{"title":"Using Microsoft.PowerShell​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#using-microsoftpowershell","content":" (Get-Command &quot;C:\\Path\\To\\Thing.dll&quot;).FileVersionInfo (Get-Item &quot;C:\\Windows\\System32\\nvcuda.dll&quot;).VersionInfo   ","version":"Next","tagName":"h3"},{"title":"Useful Environment Variables​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#useful-environment-variables","content":" OSX:  DYLD_LIBRARY_PATHDYLD_PRINT_LIBRARIESDYLD_PRINT_STATISTICS  Linux:  LD_LIBRARY_PATHLD_DEBUG=libs  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Inspect Shared Library","url":"/docs/how-to/inspect-shared-library#references","content":" Additional MSVC Build Tools  How to Show All Shared Libraries Used by Executables in Linux ","version":"Next","tagName":"h2"},{"title":"Zero-Cost CI/CD with Git Hooks and Docker Compose","type":0,"sectionRef":"#","url":"/docs/how-to/zero-cost-ci-cd","content":"","keywords":"ci cd docker git hook","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#overview","content":" Here’s the basic idea:  You push your code from your dev machine to both: A central Git host (like GitHub/GitLab)Your production server (a bare Git repo) The production server runs a post-receive hook to: Checkout the latest codeRebuild and restart Docker containers  This method is great for solo developers or small teams who want simple, fast deployments without external dependencies.    ","version":"Next","tagName":"h2"},{"title":"Step-by-Step Setup​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#step-by-step-setup","content":" ","version":"Next","tagName":"h2"},{"title":"1. Prepare the Production Server​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#1-prepare-the-production-server","content":" Install Git and Docker:​  sudo apt update &amp;&amp; sudo apt install git docker docker-compose -y   Create a bare Git repository:​  mkdir -p ~/repos/myapp.git cd ~/repos/myapp.git git init --bare   Set up SSH access:​  From your dev machine, copy your SSH key to the production server:  ssh-copy-id user@yourserver   Test it:  ssh user@yourserver     ","version":"Next","tagName":"h3"},{"title":"2. Add the Production Server as a Remote on Your Dev Machine​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#2-add-the-production-server-as-a-remote-on-your-dev-machine","content":" In your project repo:  git remote add production ssh://user@yourserver/home/user/repos/myapp.git   Now you can push to the production server:  git push production main     ","version":"Next","tagName":"h3"},{"title":"3. Create a Post-Receive Hook on the Server​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#3-create-a-post-receive-hook-on-the-server","content":" On the production server:  nano ~/repos/myapp.git/hooks/post-receive   Paste the following:  #!/bin/bash APP_DIR=/var/www/myapp # Checkout code git --work-tree=$APP_DIR --git-dir=$(pwd) checkout -f # Deploy with Docker cd $APP_DIR || exit docker compose down docker compose build docker compose up -d   Make it executable:  chmod +x ~/repos/myapp.git/hooks/post-receive     ","version":"Next","tagName":"h3"},{"title":"What Exactly Happens During git push​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#what-exactly-happens-during-git-push","content":" You run: git push production main Your local Git client: Connects to the production server over SSH.Invokes git-receive-pack on the server’s bare Git repo. Git negotiates what data is needed: It sends new commits, trees, and blobs over SSH. The server’s Git receives the data and updates the bare repo. The post-receive hook is automatically triggered: It checks out the new code to the app directory.It runs docker compose to deploy the latest version. Only once the hook script finishes does the git push complete on your dev machine.  This means your git push blocks and provides real-time feedback on deployment success or failure.    ","version":"Next","tagName":"h2"},{"title":"How It Works​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#how-it-works","content":" When you run git push production main, Git connects to the server over SSH.Your dev Git sends commit data to the bare repo on the server.Git automatically runs the post-receive hook.The hook checks out the new code and runs Docker commands.  No daemon, no polling, no fancy tools. Just Git + SSH + Docker.    ","version":"Next","tagName":"h2"},{"title":"FAQ​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#faq","content":" Q: Does the push block until the deployment finishes?  Yes. The git push command will block until the post-receive hook finishes. That way, you get immediate feedback if the deployment fails.  Q: Does Git use SSH to send data?  Absolutely. All Git data (commits, trees, blobs) is transferred securely over SSH when using ssh:// remotes.  Q: Can I still use GitHub/GitLab?  Yes! You can push to both:  git push origin main # Push to GitHub git push production main # Deploy to server     ","version":"Next","tagName":"h2"},{"title":"Final Thoughts​","type":1,"pageTitle":"Zero-Cost CI/CD with Git Hooks and Docker Compose","url":"/docs/how-to/zero-cost-ci-cd#final-thoughts","content":" This setup gives you a super simple and secure way to deploy code with nothing more than Git and Docker. For many indie devs and internal tools, it’s all you really need.  Want logging? Add &gt;&gt; /var/log/deploy.log 2&gt;&amp;1 to the hook. Want async? Run the hook script in the background with &amp;.  Happy hacking! 🚀 ","version":"Next","tagName":"h2"},{"title":"Hello!","type":0,"sectionRef":"#","url":"/docs/introduction","content":"Hello!","keywords":"","version":"Next"},{"title":"Awesome Troubleshooting","type":0,"sectionRef":"#","url":"/docs/practice/awesome-troubleshooting","content":"Awesome Troubleshooting A curated collection of troubleshooting practices: How we spent two weeks hunting an NFS bug in the Linux kernel","keywords":"best-practice troubleshooting","version":"Next"},{"title":"C/C++ Build System","type":0,"sectionRef":"#","url":"/docs/practice/c-build-system","content":"","keywords":"practice avalonia","version":"Next"},{"title":"Build System​","type":1,"pageTitle":"C/C++ Build System","url":"/docs/practice/c-build-system#build-system","content":" Make  Ninja  MSBuild  ","version":"Next","tagName":"h2"},{"title":"Build System Generator​","type":1,"pageTitle":"C/C++ Build System","url":"/docs/practice/c-build-system#build-system-generator","content":" CMake  Meson  ","version":"Next","tagName":"h2"},{"title":"Package Management​","type":1,"pageTitle":"C/C++ Build System","url":"/docs/practice/c-build-system#package-management","content":" vcpkgconan ","version":"Next","tagName":"h2"},{"title":"Intel VROC RAID on Ubuntu","type":0,"sectionRef":"#","url":"/docs/how-to/raid-intel-vroc","content":"","keywords":"Setup Intel VROC RAID on Ubuntu","version":"Next"},{"title":"Background​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#background","content":" On the premise machine, there are 2 NVMe SSDs and 8 SATA hard drives(HDDs), and also it ships with a in-box hardware-assisted RAID controller(Intel VROC) on the Intel CPU, which is supposed to keep overall advantages over software RAID.  For me, I would like to use these 8 HDDs(sda, sdb, ..., sdh) to store data for long time, while retaining the balance between redundancy and performance. So here RAID 5(Stripping with Parity) comes into my mind.  To leverage the power of Intel VROC in Ubuntu(Linux), you also need the mdadm command line tool to manage intel VROC which support RAID 0, RAID 1, RAID 5 and RAID 10  note In my understanding, the intel VROC register in system with the common interface with mdadm, so the mdadm software can operate it. And running command will show the mdadm is using intel VROC, $ sudo mdadm --detail-platform Platform : Intel(R) Virtual RAID on CPU Version : 8.0.3.1002 RAID Levels : raid0 raid1 raid10 raid5 Chunk Sizes : 4k 8k 16k 32k 64k 128k 2TB volumes : supported 2TB disks : supported Max Disks : 8 Max Volumes : 2 per array, 8 per controller I/O Controller : /sys/devices/pci0000:00/0000:00:17.0 (SATA) Port7 : /dev/sdh (ZR909K07) Port6 : /dev/sdg (ZV70BN24) Port3 : /dev/sdd (ZV70BD3T) Port4 : /dev/sde (ZR909Q89) Port1 : /dev/sdb (ZRT0S2FM) Port5 : /dev/sdf (ZR9099MM) Port2 : /dev/sdc (ZR909AGN) Port0 : /dev/sda (ZV70BMH9) Platform : Intel(R) Virtual RAID on CPU Version : 8.0.3.1002 RAID Levels : raid0 raid1 raid10 Chunk Sizes : 4k 8k 16k 32k 64k 128k 2TB volumes : supported 2TB disks : supported Max Disks : 96 Max Volumes : 2 per array, 24 per controller 3rd party NVMe : supported I/O Controller : /sys/devices/pci0000:8d/0000:8d:00.5 (VMD) NVMe under VMD : /dev/nvme0n1 (633FC084FCVK) NVMe under VMD : /dev/nvme1n1 (633FC0DEFCVK) I/O Controller : /sys/devices/pci0000:6f/0000:6f:00.5 (VMD) I/O Controller : /sys/devices/pci0000:51/0000:51:00.5 (VMD)   info Install Ubuntu Server on RAID: Ubuntu Server Image has inbox mdadm utilities and VMD drivers(which enable intel VROC functionalities), so it is quite convenient to create the RAID 1 on 2 SSDs either in BIOS stage(for intel VROC only) or in storage layer step during OS installation stage(software RAID), then install Ubuntu Server OS on the RAID 1. After creating the RAID 1 via intel VROC in BIOS, Ubuntu Server installation can detect the RAID created by VROC in step when set up the storage layer. If you skip BIOS to create RAID during OS installation, remember to add -e isms when using mdadm to create RAID(you can enter the terminal, do ``) otherwise the RAID is software based and does not apply VROC. Install Ubuntu Desktop on RAID: Ubuntu Desk Image does not ship the mdadm tool, so it is nearly impossible to create RAID and install Ubuntu Desktop OS on the RAID(however this one Install Ubuntu 20.04 desktop with RAID 1 and LVM on machine with UEFI BIOS from stackoverflow seems to be successful)  ","version":"Next","tagName":"h2"},{"title":"Set up RAID 5 array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#set-up-raid-5-array","content":" Here, I use 8 disks: /dev/sda, /dev/sdb, ..., /dev/sdh to create RAID 5 array and mount it for use in practice.  ","version":"Next","tagName":"h2"},{"title":"Create RAID array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#create-raid-array","content":" When creating RAID array, Intel VROC is different with software RAID array creation as an additional container is needed to create firstly. Inside the container, some information is labelled into the drives for Intel VROC controller to recognize them.  Create RAID Container with Intel IMSM Metadata  the total number of drives is 8 and -e imsm.  sudo mdadm --create /dev/md/imsm0 /dev/sd[a-h] -n 8 -e imsm   Then, Create a RAID array in the /dev/md/imsm0 container using total 8 drives with RAID 5.  sudo mdadm --create /dev/md/md0 /dev/md/imsm0 -l 0 -n 2   ","version":"Next","tagName":"h3"},{"title":"Mount the RAID array for use​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#mount-the-raid-array-for-use","content":" After you create the RAID array in above step, all partitions and data will be erased from all individual disks.  The RAID array is treated as a logical drive now.  Create a ext4 filesystem on the RAID array  sudo mkfs.ext4 -F /dev/md/md0   Mount the RAID array  sudo mkdir -p /mnt/md0 sudo mount /dev/md/md0 /mnt/md0   ","version":"Next","tagName":"h3"},{"title":"Save RAID array configuration​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#save-raid-array-configuration","content":" To make sure that the RAID array is reassembled and mounted automatically after reboot, we will have to add some necessary information into /etc/mdadm/mdadm.conf and /etc/fstab.  Scan active array and append into /etc/mdadm/mdadm.conf file with following:  sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf   Update initramfs, so the array will be available at early boot:  sudo update-initramfs -u   Add mount options to /etc/fstab, you can use UUID=xxxx instead of the /dev/md0.  echo '/dev/md0 /mnt/md0 ext4 defaults,nofail,discard 0 0' | sudo tee -a /etc/fstab   ","version":"Next","tagName":"h3"},{"title":"Remove RAID Array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#remove-raid-array","content":" ","version":"Next","tagName":"h2"},{"title":"[Optional] Umount the array from filesystem​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#optional-umount-the-array-from-filesystem","content":" Umount the array from filesystem if mounted,  sudo umount /dev/md/md0   ","version":"Next","tagName":"h3"},{"title":"Stop RAID container and array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#stop-raid-container-and-array","content":" # Stop RAID container sudo mdadm --stop /dev/md/imsm0 # Stop RAID array sudo mdadm --stop /dev/md/md0 # Stop all arrays and containers sudo mdadm --stop --scan   ","version":"Next","tagName":"h3"},{"title":"Removes the RAID metadata​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#removes-the-raid-metadata","content":" Removes the RAID metadata on each drive and resets the drive to normal  sudo mdadm --zero-superblock /dev/sda sudo mdadm --zero-superblock /dev/sd[a-h]   ","version":"Next","tagName":"h3"},{"title":"[Optional] Remove RAID configuration​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#optional-remove-raid-configuration","content":" Remove mount information to the array if exist. Edit the /etc/fstab:  /etc/fstab sudo nano /etc/fstab   Also, remove the array definition if exist, from the /etc/mdadm/mdadm.conf file:  /etc/mdadm/mdadm.conf sudo nano /etc/mdadm/mdadm.conf   ","version":"Next","tagName":"h3"},{"title":"Manage RAID Array with mdadm​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#manage-raid-array-with-mdadm","content":" ","version":"Next","tagName":"h2"},{"title":"Find all RAID arrays​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#find-all-raid-arrays","content":" $ cat /proc/mdstat Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] md126 : active raid1 nvme0n1[1] nvme1n1[0] 3800741888 blocks super external:/md127/0 [2/2] [UU] md127 : inactive nvme0n1[1](S) nvme1n1[0](S) 10402 blocks super external:imsm unused devices: &lt;none&gt;   ","version":"Next","tagName":"h3"},{"title":"Query information on a RAID array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#query-information-on-a-raid-array","content":" sudo mdadm --detail /dev/md0 sudo mdadm --query /dev/md0   ","version":"Next","tagName":"h3"},{"title":"Query information on a physical disk drive​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#query-information-on-a-physical-disk-drive","content":" sudo mdadm --query /dev/sda sudo mdadm --examine /dev/sda   ","version":"Next","tagName":"h3"},{"title":"Stop a RAID array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#stop-a-raid-array","content":" sudo mdadm --stop /dev/md0 # Stop all arrays sudo mdadm --stop --scan   ","version":"Next","tagName":"h3"},{"title":"Starting a RAID Array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#starting-a-raid-array","content":" # This works if the array is defined in the configuration `/etc/mdadm/mdadm.conf` file. sudo mdadm --assemble --scan sudo mdadm --assemble /dev/md0 # If the array is not persisted in `/etc/mdadm/mdadm.conf` file but keeping RAID metadata sudo mdadm --assemble /dev/md0 /dev/sda /dev/sdb   ","version":"Next","tagName":"h3"},{"title":"Adding spare devices to a RAID Array​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#adding-spare-devices-to-a-raid-array","content":" sudo mdadm /dev/md0 --add /dev/sde   $ lsblk -f NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS loop0 squashfs 4.0 0 100% /snap/core20/1974 loop1 squashfs 4.0 0 100% /snap/lxd/24322 loop2 squashfs 4.0 0 100% /snap/snapd/19457 sda isw_raid_member 1.3.00 sdb isw_raid_member 1.3.00 sdc isw_raid_member 1.3.00 sdd isw_raid_member 1.3.00 sde isw_raid_member 1.3.00 sdf isw_raid_member 1.3.00 sdg isw_raid_member 1.3.00 sdh isw_raid_member 1.3.00 nvme0n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127 nvme1n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127   sudo fdisk -l /dev/sda   ","version":"Next","tagName":"h3"},{"title":"Delete partition and data in disk​","type":1,"pageTitle":"Intel VROC RAID on Ubuntu","url":"/docs/how-to/raid-intel-vroc#delete-partition-and-data-in-disk","content":" sudo dd if=/dev/zero of=/dev/sda bs=512 count=1  ","version":"Next","tagName":"h3"},{"title":"Code Snippet Management","type":0,"sectionRef":"#","url":"/docs/practice/code-snippet-management","content":"Code Snippet Management","keywords":"code snippet manager","version":"Next"},{"title":"QEMU Emulate Raspberry Pi 3 and 4","type":0,"sectionRef":"#","url":"/docs/how-to/qemu-raspberry-pi","content":"","keywords":"QEMU Raspberry Pi 3/4","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#prerequisites","content":" Docker be required in macOScan be skipped in Linuxcan use wsl as an alternative in Windows QEMU homebrew install in macOS Raspberry Pi image: 2024-05-03-raspios-bullseye-arm64-lite.img  Since I am in mac M1, and the raspberry pi image which contains a fat filesystem as boot and a ext4 filesystem as OS, we need write some configuration into it. So I will use a Docker Ubuntu container to do the operation on the the filesystem. There some other tools to do the like of these operations:  ext4fuse is free and easy to install via homebrew, but it has limit as read-only access.ExtFS from Paragon supports read-write access while you need pay for it.virtual machine Docker in OSX make use of virtual machine while it is quick and flexible to use.  ","version":"Next","tagName":"h2"},{"title":"Raspberry Pi image​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#raspberry-pi-image","content":" cd ~ wget https://downloads.raspberrypi.org/raspios_arm64/images/raspios_arm64-2024-05-03/2024-05-03-raspios-bullseye-arm64-lite.img.xz xz -d 2024-05-03-raspios-bullseye-arm64-lite.img.xz   ","version":"Next","tagName":"h2"},{"title":"Docker Ubuntu container​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#docker-ubuntu-container","content":" Mount the folder including 2024-05-03-raspios-bullseye-arm64-lite.img  docker run -it -d --privileged -v $PWD:/qemu --name ubuntu ubuntu docekr exec -it ubuntu bash   ","version":"Next","tagName":"h2"},{"title":"Extracting Kernel and device tree​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#extracting-kernel-and-device-tree","content":" Operations all in Ubuntu container.  root@f36a3251391d:/qemu# fdisk -l 2024-05-03-raspios-bullseye-arm64-lite.img Disk 2024-05-03-raspios-bullseye-arm64-lite.img: 1.96 GiB, 2101346304 bytes, 4104192 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x544c6228 Device Boot Start End Sectors Size Id Type 2024-05-03-raspios-bullseye-arm64-lite.img1 8192 532479 524288 256M c W95 FAT32 (LBA) 2024-05-03-raspios-bullseye-arm64-lite.img2 532480 4104191 3571712 1.7G 83 Linux   The first partition is boot filesystem.The second partition is real root filesystem.  All the data we need is in the first partition, to do the operation is mounting it.  The offset of the first partition: 8192 * 512 = 4194304,  root@f36a3251391d:/qemu# mount -o loop,offset=4194304 2024-05-03-raspios-bullseye-arm64-lite.img /mnt/rpi-boot/   root@f36a3251391d:/qemu# ls -ls /mnt/rpi-boot/ total 30244 20 -rwxr-xr-x 1 root root 18693 Apr 5 11:32 COPYING.linux 2 -rwxr-xr-x 1 root root 1594 Apr 5 11:32 LICENCE.broadcom 30 -rwxr-xr-x 1 root root 30390 Apr 5 11:32 bcm2710-rpi-2-b.dtb 32 -rwxr-xr-x 1 root root 32753 Apr 5 11:32 bcm2710-rpi-3-b-plus.dtb 32 -rwxr-xr-x 1 root root 32142 Apr 5 11:32 bcm2710-rpi-3-b.dtb 30 -rwxr-xr-x 1 root root 30285 Apr 5 11:32 bcm2710-rpi-cm3.dtb 32 -rwxr-xr-x 1 root root 31318 Apr 5 11:32 bcm2710-rpi-zero-2-w.dtb 32 -rwxr-xr-x 1 root root 31318 Apr 5 11:32 bcm2710-rpi-zero-2.dtb 52 -rwxr-xr-x 1 root root 52593 Apr 5 11:32 bcm2711-rpi-4-b.dtb 52 -rwxr-xr-x 1 root root 52682 Apr 5 11:32 bcm2711-rpi-400.dtb 38 -rwxr-xr-x 1 root root 38182 Apr 5 11:32 bcm2711-rpi-cm4-io.dtb 52 -rwxr-xr-x 1 root root 53202 Apr 5 11:32 bcm2711-rpi-cm4.dtb 50 -rwxr-xr-x 1 root root 50504 Apr 5 11:32 bcm2711-rpi-cm4s.dtb 52 -rwxr-xr-x 1 root root 52476 Apr 5 11:32 bootcode.bin 2 -rwxr-xr-x 1 root root 154 May 3 03:11 cmdline.txt 4 -rwxr-xr-x 1 root root 2109 May 3 02:53 config.txt ... 2 -rwxr-xr-x 1 root root 145 May 3 03:11 issue.txt 8028 -rwxr-xr-x 1 root root 8219600 Apr 5 11:32 kernel8.img ...   To run QEMU we will need the kernel and device tree, so let’s copy them out:  root@f36a3251391d:/qemu# cp /mnt/rpi-boot/kernel8.img . root@f36a3251391d:/qemu# cp /mnt/rpi-boot/bcm2710-rpi-3-b.dtb .   ","version":"Next","tagName":"h2"},{"title":"Setting up default user​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#setting-up-default-user","content":" Operations all in docker container.  Now in order to set up user and enable ssh in default, we need write files into /userconf and /ssh under the boot filesystem mounted as /mnt/rpi-boot/.  Set up a default user:pi and password:raspberry.  Hash password raspberry using openssl,  root@f36a3251391d:/qemu# openssl passwd Password: Verifying - Password: $1$d...AvcL$wqfUqTIauUP1TVJ/uU1td0   root@f36a3251391d:/qemu# echo 'pi:$1$d...AvcL$wqfUqTIauUP1TVJ/uU1td0' | tee /mnt/rpi-boot/userconf   Enable ssh,  root@f36a3251391d:/qemu# touch /mnt/rpi-boot/ssh   root@f36a3251391d:/qemu# umount /mnt/rpi-boot   ","version":"Next","tagName":"h2"},{"title":"Running QEMU​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#running-qemu","content":" ","version":"Next","tagName":"h2"},{"title":"Emulate Raspberry Pi 3​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#emulate-raspberry-pi-3","content":" Now switch back to the host macOS to run QEMU,  Resize the image to the next power of 2 size,  The original size,  ❯ stat -f%z 2024-05-03-raspios-bullseye-arm64-lite.img 2101346304   To resize to 4GB,  qemu-img resize ./2024-05-03-raspios-bullseye-arm64-lite.img 4G   qemu-system-aarch64 \\ -machine raspi3b \\ -cpu cortex-a72 \\ -nographic \\ -m 1G \\ -smp 4 \\ -dtb bcm2710-rpi-3-b.dtb \\ -kernel kernel8.img \\ -append &quot;rw earlyprintk loglevel=8 console=ttyAMA0,115200 dwc_otg.lpm_enable=0 root=/dev/mmcblk0p2 rootdelay=1&quot; \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 \\ -device usb-net,netdev=net0 \\ -sd 2024-05-03-raspios-bullseye-arm64-lite.img   Options in detail:  -machine raspi3b: use raspberry pi 3 machine.-append: console=ttyAMA0: output the VM std to QEMU console.root=/dev/mmcblk0p2: mount real root filesystem to /dev/mmcblk0p2(the second partition of mmcblk0) as we -sd xx will be mounted to /dev/mmcblk0. -netdev user,id=net0,hostfwd=tcp::2222-:22: network mapping host port 2222 to the VM 22-device usb-net,netdev=net0: expose netdev=net0 as usb-net in the raspberry pi 3 machine.-sd 2024-05-03-raspios-bullseye-arm64-lite.img: sd drive is available in the raspberry pi 3 machine.  ","version":"Next","tagName":"h3"},{"title":"Emulate Raspberry Pi 4 with virt​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#emulate-raspberry-pi-4-with-virt","content":" We will use generic virtual machine virt to act as raspi4, since there is no raspi4 machine defined in QEMU official machines. However you can still use raspi3 to act as raspi4 as they are same!  Hardware Acceleration can be enable in virt machine by using -accel hvf option in my mac M1 host as it's arm-based.  So virt will bring high performance and increase efficiency!  After tuning options and searching from many resources, the operational setting for QEMU to emulate is,  Use ubuntu-22.04.3-preinstalled-server-arm64+raspi.img, of which the default user is ubuntu and password is ubuntu.  kernel=&quot;$PWD/ubuntu-22.04.3-preinstalled-server-arm64+raspi-boot/vmlinuz&quot; initrd=&quot;$PWD/ubuntu-22.04.3-preinstalled-server-arm64+raspi-boot/initrd.img&quot; img=&quot;$PWD/ubuntu-22.04.3-preinstalled-server-arm64+raspi.img&quot;   For SCSI hard disk​  This storage device file will be named /dev/sdX,  qemu-system-aarch64 \\ -machine virt \\ -accel hvf \\ -cpu host \\ -smp 4 \\ -m 4G \\ -nographic \\ -kernel $kernel \\ -initrd $initrd \\ -append &quot;earlyprintk loglevel=8 root=/dev/sda2 rootfstype=ext4 rw console=ttyAMA0&quot; \\ -drive file=$img,format=raw,if=none,id=drive0 \\ -device virtio-scsi-pci,id=scsi \\ -device scsi-hd,drive=drive0,bus=scsi.0 \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22 \\ -device virtio-net-pci,netdev=mynet   Options in detail:  -accel hvf: hardware acceleration in mac M1. Don't use in x86_64 host.-cpu host: change to -cpu cortex-a72 when no hardware acceleration available such as in x86_64 host.-append root=/dev/sda2: the second partition of the ubuntu-22.04.3-preinstalled-server-arm64+raspi.img disk image hold the real root filesystem. -initrd $initrd the boot loader works using configuration like vmlinuz initrd=initrd.img root=/dev/sda2.  For virtual disk storage device​  This storage device file will be named /dev/vdX,  qemu-system-aarch64 \\ -machine virt \\ -accel hvf \\ -cpu host \\ -smp 4 \\ -m 4G \\ -nographic \\ -kernel $kernel \\ -initrd $initrd \\ -append &quot;earlyprintk loglevel=8 root=/dev/vda2 rootfstype=ext4 rw console=ttyAMA0&quot; \\ -drive file=$img,format=raw,if=none,id=drive0 \\ -device virtio-blk-pci,drive=drive0 \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22 \\ -device virtio-net-pci,netdev=mynet   For NVMe storage device​  This storage device file will be named /dev/nvmeX,  qemu-system-aarch64 \\ -machine virt \\ -accel hvf \\ -cpu host \\ -smp 4 \\ -m 4G \\ -nographic \\ -kernel $kernel \\ -append &quot;earlyprintk loglevel=8 root=/dev/nvme0n1p2 rootfstype=ext4 rw console=ttyAMA0&quot; \\ -drive file=$img,format=raw,if=none,id=drive0 \\ -device nvme,drive=drive0,serial=deadbeaf1 \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22 \\ -device virtio-net-pci,netdev=mynet   Options in detail:  no -initrd $initrd the boot loader works using configuration like vmlinuz root=/dev/nvme0n1p2.we directly mount the real filesystem /dev/nvme0n1p2, skipping to mount the initial RAM disk.I test other type storage device must binding -initrd $initrd while there is no need for NVME. In my assumption, those storage devices need to be configured in the initramfs.  For usb storage​  This storage device file will be named /dev/sdX,  qemu-system-aarch64 \\ -machine virt \\ -cpu cortex-a57 \\ -smp 4 \\ -m 4G \\ -no-reboot \\ -nographic \\ -kernel $kernel \\ -initrd $initrd \\ -append &quot;earlyprintk loglevel=8 root=/dev/sda2 rootfstype=ext4 console=ttyAMA0 raid=noautodetect&quot; \\ -device usb-ehci \\ -device usb-storage,drive=disk0 \\ -drive file=$img,format=raw,if=none,id=disk0 \\ -device virtio-net-pci,netdev=mynet \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22   Options in detail:  -device usb-ehci: usb bus -&gt; PCI bus-device usb-storage: usb storage device -&gt; usb bus  ","version":"Next","tagName":"h3"},{"title":"Test Raspberry Pi VM​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#test-raspberry-pi-vm","content":" Log into the Raspberry Pi via ssh from the macOS host,  ❯ ssh -p 2222 pi@localhost The authenticity of host '[localhost]:2222 ([127.0.0.1]:2222)' can't be established. ED25519 key fingerprint is SHA256:6igL6iaigBCszv8m6nyNl+tsB2siV/tL+TRQANC6nBw. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '[localhost]:2222' (ED25519) to the list of known hosts. pi@localhost's password: Linux raspberrypi 6.1.21-v8+ #1642 SMP PREEMPT Mon Apr 3 17:24:16 BST 2023 aarch64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Fri Sep 22 16:30:58 2023 SSH is enabled and the default password for the 'pi' user has not been changed. This is a security risk - please login as the 'pi' user and type 'passwd' to set a new password. pi@raspberrypi:~ $   ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"QEMU Emulate Raspberry Pi 3 and 4","url":"/docs/how-to/qemu-raspberry-pi#resources","content":" Emulating a Raspberry Pi in QEMU | InterruptEmulating a Raspberry Pi in QEMU  How to emulate block devices with QEMU  Emulation of block devices — Das U-Boot unknown version documentation ","version":"Next","tagName":"h2"},{"title":"RAID on Ubuntu","type":0,"sectionRef":"#","url":"/docs/how-to/raid-on-ubuntu","content":"","keywords":"Setup Intel VROC RAID on Ubuntu","version":"Next"},{"title":"Background​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#background","content":" Recently, there is a chance for me to install Ubuntu(server) OS on a Dell Precision xxx workstation which is includes 2 NVMe SSDs and 8 SATA hard drives(HDDs) and has in-box hardware-assisted RAID controller(Intel VROC). In the past, I just play cloud virtual machines or personal host with single disk.  Configuring storage is a critical part of setting up a reliable workstation. So firstly, how to organize these following disks to their roles?  2 SSDs hold the system to load quickly8 HDDs store data persistently  In order to access these physical disks easily and reduce damages from data loss, I need to combine multiple disks to act as one, while keep data redundant and backup. After step-by-step research, there are some enterprise solutions present for me. These drive layer or file system layer approaches designed for specific purposes have their own advantages over others while they maybe achieve some same features.  Here are some benefits and shortcomings of them alongside common use cases:  RAID(Redundant Array of Independent Disks) Abstraction level: drive layer Concept: RAID uses multiple drives to act as one(logical drive). Benefits: improve data redundancy and data read/write performance LVM(Logical Volume Management) Abstraction level: file system layer Concept: Manage a logical volume over multiple drives, each drive is a Physical Volume(PV). Benefits: combine multiple disks into one logical volume, extend the volume with new disk added, increase/decrease mounted folder in file system ZFS(Z File System)  There are three types of raid, as Wiki saying:  hardware RAIDsoftware RAID mdadm in Linux hardware-assisted software RAID, firmware RAID, fake RAID Intel VROC (Virtual RAID on CPU)  This document will introduce how to set up software RAID(RAID0, RAID1, RAID5, RAID 10) on already-installed Ubuntu.  To create a RAID to hold the Ubuntu OS when installing Ubuntu, see SoftwareRAID or How to install Ubuntu with software RAID-1  In addition, there are different challenges you will face when installing Ubuntu Server and Ubuntu Desktop.  Install Ubuntu Server on RAID: Ubuntu Server Image has inbox mdadm utilities, so it is quite convenient to create the software RAID on multiple disks then install Ubuntu Server OS on the RAID in storage layer step during OS installation stage.    Install Ubuntu Desktop on RAID: Ubuntu Desk Image does not ship the mdadm tool, so it is nearly impossible to create RAID and install Ubuntu Desktop OS on the RAID(however this one Install Ubuntu 20.04 desktop with RAID 1 and LVM on machine with UEFI BIOS from stackoverflow seems to be successful)  ","version":"Next","tagName":"h2"},{"title":"Set up RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#set-up-raid-array","content":" To create a RAID array ready to use in practice, there are always common steps:  Create a RAID array(RAID 0, RAID 1, RAID 5 or RAID 10)Mount the RAID arraySave the RAID array configuration for system boot  ","version":"Next","tagName":"h2"},{"title":"Create RAID array with mdadm​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#create-raid-array-with-mdadm","content":" Create RAID 0 array using devices: /dev/sda and /dev/sdb  sudo mdadm --create --verbose /dev/md0 -l 0 -n 2 /dev/sda /dev/sdb   ","version":"Next","tagName":"h3"},{"title":"Mount RAID array for use​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#mount-raid-array-for-use","content":" Create a ext4 filesystem on the array  sudo mkfs.ext4 -F /dev/md0   Mount the array  sudo mkdir -p /mnt/md0 sudo mount /dev/md0 /mnt/md0   ","version":"Next","tagName":"h3"},{"title":"Save RAID array configuration​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#save-raid-array-configuration","content":" Persist the RAID array configuration to make the system reassemble and mount the RAID array automatically after reboot.  Append the line to /etc/mdadm/mdadm.conf:  sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf   Make RAID array available in early boot stage:  sudo update-initramfs -u   Persist the mount point, edit /etc/fstab:  /etc/fstab /dev/md0 /mnt/md0 ext4 defaults,nofail,discard 0 0   or persist the mount point by using UUID, get UUID of the disk drive,  $ blkid /dev/md124 /dev/md124: UUID=&quot;b7fa44f2-0f05-47a1-b4ef-e9ad306898de&quot; BLOCK_SIZE=&quot;4096&quot; TYPE=&quot;ext4&quot;   then edit in /etc/fstab,  /etc/fstab UUID=b7fa44f2-0f05-47a1-b4ef-e9ad306898de /volume ext4 defaults,nofail,discard 0 0   finally apply the new mount,  sudo mount -a   ","version":"Next","tagName":"h3"},{"title":"Delete RAID Array with mdadm​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#delete-raid-array-with-mdadm","content":" Make sure to remove what are using the RAID array,  [Optional] Umount the array from filesystem if mounted,  sudo umount /dev/md0   Stop RAID array,  sudo mdadm --stop /dev/md0 # Stop all arrays sudo mdadm --stop --scan   Removes the RAID metadata and resets them to normal on the Drives,  sudo mdadm --zero-superblock /dev/sda sudo mdadm --zero-superblock /dev/sd[a-h]   [Optional] Remove any persistent references to the array if exist. Edit the /etc/fstab:  /etc/fstab sudo nano /etc/fstab   [Optional] Also, remove the array definition if exist, from the /etc/mdadm/mdadm.conf file:  /etc/mdadm/mdadm.conf sudo nano /etc/mdadm/mdadm.conf   ","version":"Next","tagName":"h2"},{"title":"Manage RAID Array with mdadm​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#manage-raid-array-with-mdadm","content":" ","version":"Next","tagName":"h2"},{"title":"Find the RAID arrays​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#find-the-raid-arrays","content":" $ cat /proc/mdstat Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] md126 : active raid1 nvme0n1[1] nvme1n1[0] 3800741888 blocks super external:/md127/0 [2/2] [UU] md127 : inactive nvme0n1[1](S) nvme1n1[0](S) 10402 blocks super external:imsm unused devices: &lt;none&gt;   ","version":"Next","tagName":"h3"},{"title":"Query information on RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#query-information-on-raid-array","content":" sudo mdadm --detail /dev/md0 sudo mdadm --query /dev/md0   ","version":"Next","tagName":"h3"},{"title":"Query information on individual physical devices​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#query-information-on-individual-physical-devices","content":" sudo mdadm --query /dev/sda sudo mdadm --examine /dev/sda   ","version":"Next","tagName":"h3"},{"title":"Stop RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#stop-raid-array","content":" sudo mdadm --stop /dev/md0 # Stop all arrays sudo mdadm --stop --scan   ","version":"Next","tagName":"h3"},{"title":"Start an RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#start-an-raid-array","content":" # This works if the array is defined in the configuration `/etc/mdadm/mdadm.conf` file. sudo mdadm --assemble --scan sudo mdadm --assemble /dev/md0 # If the array is not persisted in `/etc/mdadm/mdadm.conf` file but keeping RAID metadata sudo mdadm --assemble /dev/md0 /dev/sda /dev/sdb   ","version":"Next","tagName":"h3"},{"title":"Add a spare device to an RAID array​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#add-a-spare-device-to-an-raid-array","content":" sudo mdadm /dev/md0 --add /dev/sde   ","version":"Next","tagName":"h3"},{"title":"Check block devices​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#check-block-devices","content":" $ lsblk -f NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS loop0 squashfs 4.0 0 100% /snap/core20/1974 loop1 squashfs 4.0 0 100% /snap/lxd/24322 loop2 squashfs 4.0 0 100% /snap/snapd/19457 sda isw_raid_member 1.3.00 sdb isw_raid_member 1.3.00 sdc isw_raid_member 1.3.00 sdd isw_raid_member 1.3.00 sde isw_raid_member 1.3.00 sdf isw_raid_member 1.3.00 sdg isw_raid_member 1.3.00 sdh isw_raid_member 1.3.00 nvme0n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127 nvme1n1 isw_raid_member 1.3.00 ├─md126 │ ├─md126p1 vfat FAT32 292B-DB66 1G 1% /boot/efi │ └─md126p2 ext4 1.0 0f58386c-334d-4877-8051-b855bae37fb0 3.3T 0% / └─md127   ","version":"Next","tagName":"h3"},{"title":"List UUID of devices​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#list-uuid-of-devices","content":" $ sudo blkid /dev/sdf: TYPE=&quot;isw_raid_member&quot; /dev/nvme0n1: TYPE=&quot;isw_raid_member&quot; /dev/sdd: TYPE=&quot;isw_raid_member&quot; /dev/sdb: TYPE=&quot;isw_raid_member&quot; /dev/sdg: TYPE=&quot;isw_raid_member&quot; /dev/sde: TYPE=&quot;isw_raid_member&quot; /dev/sdc: TYPE=&quot;isw_raid_member&quot; /dev/md126p2: UUID=&quot;ff1f3640-e590-486b-8570-c34dfd7bd1de&quot; BLOCK_SIZE=&quot;4096&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;07473e4a-9324-435d-9238-cf358cd9a6a9&quot; /dev/md126p1: UUID=&quot;A636-3441&quot; BLOCK_SIZE=&quot;512&quot; TYPE=&quot;vfat&quot; PARTUUID=&quot;7eb27871-d9ad-4132-af06-7110948faf06&quot; /dev/nvme1n1: TYPE=&quot;isw_raid_member&quot; /dev/sda: TYPE=&quot;isw_raid_member&quot; /dev/md124: UUID=&quot;b7fa44f2-0f05-47a1-b4ef-e9ad306898de&quot; BLOCK_SIZE=&quot;4096&quot; TYPE=&quot;ext4&quot; /dev/sdh: TYPE=&quot;isw_raid_member&quot; /dev/loop1: TYPE=&quot;squashfs&quot; /dev/loop4: TYPE=&quot;squashfs&quot; /dev/loop2: TYPE=&quot;squashfs&quot; /dev/loop0: TYPE=&quot;squashfs&quot; /dev/loop3: TYPE=&quot;squashfs&quot;   ","version":"Next","tagName":"h3"},{"title":"Partition a disk​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#partition-a-disk","content":" sudo fdisk -l /dev/sda   ","version":"Next","tagName":"h3"},{"title":"Create filesystem on disk​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#create-filesystem-on-disk","content":" sudo mkfs.ext4 -F /dev/sda   ","version":"Next","tagName":"h3"},{"title":"Delete partition and data in disk​","type":1,"pageTitle":"RAID on Ubuntu","url":"/docs/how-to/raid-on-ubuntu#delete-partition-and-data-in-disk","content":" sudo dd if=/dev/zero of=/dev/sda bs=512 count=1  ","version":"Next","tagName":"h3"},{"title":"Dotfiles Guide","type":0,"sectionRef":"#","url":"/docs/practice/dotfiles-guide","content":"Dotfiles Guide Tutorials - dotfiles.github.io Getting started with dotfilesFrontend Ramblings RSS feedThe content of this website on GitHubMy Mastodon profileMy Twitter profileShare this article on TwitterShare this article on Hacker News","keywords":"dotfiles","version":"Next"},{"title":"FastAPI Best Practices","type":0,"sectionRef":"#","url":"/docs/practice/fastapi-best-practices","content":"","keywords":"limit only one access to endpoint at a time lock access to endpoint at a time","version":"Next"},{"title":"Limit Only One Access to Endpoint at a Time​","type":1,"pageTitle":"FastAPI Best Practices","url":"/docs/practice/fastapi-best-practices#limit-only-one-access-to-endpoint-at-a-time","content":" Limit only one access to an endpoint at a time with asyncio.Lock in asyncio in FastAPI.  fastapi_request_lock.py loading... View on GitHub  NOTE: The asyncio.Lock only take effect in the asyncio loop level, if using unicorn to run server in multiple processes, it can not lock the request!  No limitation.  fastapi_request_nolock.py loading... View on GitHub  Limit only one access to an endpoint at a time with thread.Lock  Limit only one access to an endpoint at a time with process.Lock  ","version":"Next","tagName":"h2"},{"title":"Attach A Background Service Into the Application​","type":1,"pageTitle":"FastAPI Best Practices","url":"/docs/practice/fastapi-best-practices#attach-a-background-service-into-the-application","content":" Run a background service behind the FastAPI server:  share the same asyncio main loop with the serverthe service start when the server starts and stop when the server stopsit should be light-weight and non-CPU heavy workload  Coroutines and Tasks — Python 3.11.4 documentationEvent Loop — Python 3.11.4 documentation  fastapi_background_service.py loading... View on GitHub ","version":"Next","tagName":"h2"},{"title":"Your markdown including PlantUML code block","type":0,"sectionRef":"#","url":"/docs/practice/markdown-plantuml","content":"Your markdown including PlantUML code block @startuml :User: --&gt; (Use) &quot;Main Admin&quot; as Admin &quot;Use the application&quot; as (Use) Admin --&gt; (Admin the application) @enduml ","keywords":"","version":"Next"},{"title":"OpenCV tips","type":0,"sectionRef":"#","url":"/docs/practice/opencv-tips","content":"OpenCV tips Q: Whether the image/frame from VideoCapture is in BGR or YUV pixels format? A: VideoCapture will convert the image automatically to BGR colorspace. you can disable this conversion (and receive YUV) by setting the CAP_PROP_CONVERT_RGB property to false.","keywords":"","version":"Next"},{"title":"Python Benchmark","type":0,"sectionRef":"#","url":"/docs/practice/python-benchmark","content":"","keywords":"performance measure","version":"Next"},{"title":"Measuring Lock Performance in Python on Linux​","type":1,"pageTitle":"Python Benchmark","url":"/docs/practice/python-benchmark#measuring-lock-performance-in-python-on-linux","content":" Here’s a quick look at the cost of acquiring and releasing an uncontended lock using Python’s threading.Lock on a Linux system:  $ ./python -m timeit \\ -s &quot;from threading import Lock; l=Lock(); a=l.acquire; r=l.release&quot; \\ &quot;a(); r()&quot; 10000000 loops, best of 3: 0.127 usec per loop   Now, let’s compare that with the cost of calling a dummy Python function:  $ ./python -m timeit -s &quot;def a(): pass&quot; &quot;a(); a()&quot; 1000000 loops, best of 3: 0.221 usec per loop   And a trivial C function call (returning the False singleton via bool()):  $ ./python -m timeit -s &quot;a=bool&quot; &quot;a(); a()&quot; 10000000 loops, best of 3: 0.164 usec per loop   Interestingly, using a Lock as a context manager is actually slower, not faster, despite what you might expect:  $ ./python -m timeit -s &quot;from threading import Lock; l=Lock()&quot; \\ &quot;with l: pass&quot; 1000000 loops, best of 3: 0.242 usec per loop   So at least on Linux, there doesn't seem to be much low-hanging fruit left when it comes to optimizing lock performance.  Bonus: As of recent Python versions, RLock is now just as fast as Lock in uncontended cases:  $ ./python -m timeit \\ -s &quot;from threading import RLock; l=RLock(); a=l.acquire; r=l.release&quot; \\ &quot;a(); r()&quot; 10000000 loops, best of 3: 0.114 usec per loop  ","version":"Next","tagName":"h2"},{"title":"Python C Library","type":0,"sectionRef":"#","url":"/docs/practice/python-c-library","content":"","keywords":"Python C","version":"Next"},{"title":"Python int object​","type":1,"pageTitle":"Python C Library","url":"/docs/practice/python-c-library#python-int-object","content":" Python uses a variable-size integer representation,  Overhead size: 24 bytes, including Python header objectData size: 4 or 8 bytes, storing smaller int using 4 bytes and bigger int using 8 bytes.  &gt;&gt;&gt; sys.getsizeof(0x560f7ab1e1c0) 32 &gt;&gt;&gt; sys.getsizeof(0xc0) 28   ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Python C Library","url":"/docs/practice/python-c-library#resources","content":"","version":"Next","tagName":"h2"},{"title":"Python Module","type":0,"sectionRef":"#","url":"/docs/practice/python-module","content":"","keywords":"","version":"Next"},{"title":"Python Module Search Path​","type":1,"pageTitle":"Python Module","url":"/docs/practice/python-module#python-module-search-path","content":" The Module Search Path  Introduction to Python module search path ","version":"Next","tagName":"h2"},{"title":"Git Best Practices","type":0,"sectionRef":"#","url":"/docs/practice/git-best-practices","content":"","keywords":"Git Submodules","version":"Next"},{"title":"Git SSH key​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#git-ssh-key","content":" How to Authenticate Your Git to GitHub with SSH Keys  ","version":"Next","tagName":"h2"},{"title":"Git credential​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#git-credential","content":" Store username/password instead of ssh for multiple remotes  To enable git credentials  # local git config credential.helper store # global git config --global credential.helper store   Each credential is stored in ~/.git-credentials file on its own line as a URL like:  https://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@github.com   Configure credentials,  # Global git config --global credential.https://github.com.username &lt;your_username&gt; # Or git config --local user.name &lt;your_username&gt; git config --local user.email &lt;your_useremail&gt; # Then git pull or git push to let it cache your username/password after it prompt you to input password in the first time   Alternatively, we can directly edit our global Git config file ~/.gitconfig,  [credential &quot;https://github.com&quot;] username = &lt;username&gt;   Git - Config Username &amp; Password - Store Credentials - ShellHacks  Configuring Git Credentials  ","version":"Next","tagName":"h2"},{"title":"Git submodule​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#git-submodule","content":" Pull the repo and its all submodules in the first time.  git clone http://10.6.64.66:30000/mtr/mtr.git cd mtr git submodule update --init --recursive --progress   Or just one command to clone with all the submodules.  git clone --recursive http://10.6.64.66:30000/mtr/mtr.git   Pull the repo and its all submodules later  git submodule update --recursive --progress   Enter each sub repository to pull its own latest of main per repository, when the parent repo does point to the latest branch of its submodules!  Sometimes, it is very annoying to keep the parent repository up to date on the latest reference of its every sub repository! This approach give you the flexibility while being like a shortcut.  git submodule foreach git checkout main   git submodule foreach git pull   git submodule foreach git pull origin main   git submodule update --recursive --remote   ","version":"Next","tagName":"h2"},{"title":"Discard local commits​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#discard-local-commits","content":" Assume your local repo has 10 commits ahead of the origin/main, and you want to move back to the origin/main.  git reset --hard origin/main   ","version":"Next","tagName":"h2"},{"title":"Create a subdirectory inside Git and use it as Git submodule​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#create-a-subdirectory-inside-git-and-use-it-as-git-submodule","content":" First, create a subdirectory ./include/private and initialize it as a new Git repository inside Git repo, then push it to remote.  Once done, we'll have subdirectory ./include/private which has been gitted.  Check current parent Git submodule, and our ./include/private sit outside.  $ git submodule -96788d8ac53a815778a8cfd19addb3590a8be5ea code-snippets/assembly -990e3db80c61c64ba9097adb7e729a6568c272ec code-snippets/c -dd2097c0884363948877dea2b3f68efb90e6d204 code-snippets/cpp -a4c753b89c423cd02718bece5a8a6302bd2385c7 code-snippets/docker-compose -e23cfb82c6fefae4fdc6a144228bebb580bf7c13 code-snippets/python   Option 1, Using git submodule add command in parent directory(note: works even ./include/private folder exists):  git submodule add https://github.com/liviaerxin/private.git include/private   Finally, commit,  git commit -m &quot;add submodule private&quot;   Option 2, Editing .gitmodules file in parent directory by adding:  .gitmodules [submodule &quot;include/private&quot;] path = include/private url = https://github.com/liviaerxin/private.git   Indexing the submodule,  git add include/private   Verifying that the submodule has been included,  $ git submodule -96788d8ac53a815778a8cfd19addb3590a8be5ea code-snippets/assembly -990e3db80c61c64ba9097adb7e729a6568c272ec code-snippets/c -dd2097c0884363948877dea2b3f68efb90e6d204 code-snippets/cpp -a4c753b89c423cd02718bece5a8a6302bd2385c7 code-snippets/docker-compose -e23cfb82c6fefae4fdc6a144228bebb580bf7c13 code-snippets/python -e9e1b5f114e5da1896dae6c08ca01bc83b844b4d include/private   For managing the submodule,  Sync local private file: .git/config by running git submodule init, which will let the submodule update when running git submodule update later.  Finally, commit,  git commit -m &quot;add submodule private&quot;   In conclusion, git submodule add equals:  Run git clone {submodule} include/{submodule}.Add submodule configuration such as path = include/{submodule} and url=https://github.com/xxx/{submodule}.git into .gitmodules file.Add the sub folder include/{submodule}: git add include/{submodule}.Sync .git/config file: git submodule init.  Sometimes the warning to the changes of submodules will be annoying, especially if you update submodules frequently.  To ignore all changes to the submodules:  .gitmodules [submodule &quot;include/private&quot;] path = include/private url = https://github.com/liviaerxin/private.git ignore = all   Once you have ignored changes in a submodule, you will no longer see them in the output of the git status command. You will also not be able to commit or push the changes to the submodule.  To view the changes in the submodule, you can use the git submodule summary command. This command will show you a summary of the changes in the submodule, even if they are ignored.  Once you have unignored changes in a submodule, you will be able to see them in the output of the git status command and you will be able to commit and push the changes to the submodule.  Here are some additional things to keep in mind when ignoring changes in a submodule:  Ignoring changes in a submodule does not prevent you from updating the submodule. You can still use the git submodule update command to update the submodule to the latest version.Ignoring changes in a submodule does not prevent you from cloning the submodule. You can still use the git submodule clone command to clone the submodule into another repository.Ignoring changes in a submodule does not prevent you from deleting the submodule. You can still use the git submodule deinit command to delete the submodule from your repository.  Overall, ignoring changes in a submodule can be a useful way to keep your repository clean and organized. However, it is important to understand the implications of ignoring changes before you do so.  ","version":"Next","tagName":"h2"},{"title":"Carve out a subdirectory from Git and use it as Git submodule​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#carve-out-a-subdirectory-from-git-and-use-it-as-git-submodule","content":" ","version":"Next","tagName":"h2"},{"title":"Git subtree​","type":1,"pageTitle":"Git Best Practices","url":"/docs/practice/git-best-practices#git-subtree","content":" When you should consider using a Git subtree instead of Git submodule?  If you need to manage multiple projects within a single repository as:  I have a main project for writing blogsHowever, I also manage several projects to maintain my daily coding snippets such as C, Python, ...etc.  I want to store these sub-projects in the specified sub-folders of my main repository.I will update these sub-projects frequently.I will pull and push changes of these sub-projects in local sub working directories.I don't want to the main repository to warn me the new commits from sub-projects every time I update the sub-projects.  Drawbacks:  To update and commit changes on these sub-projects you need to remember some &quot;information&quot;, because the metadata of these sub-projects will be not stored in a file like .gitmodules in main repo.  About Git subtree merges  Git Subtree: Alternative to Git Submodule | Atlassian Git Tutorial ","version":"Next","tagName":"h2"},{"title":"Python Package Management","type":0,"sectionRef":"#","url":"/docs/practice/python-package-management","content":"","keywords":"docs docusaurus","version":"Next"},{"title":"Todo List​","type":1,"pageTitle":"Python Package Management","url":"/docs/practice/python-package-management#todo-list","content":" First tabstopA second tabstopA third tabstop  Note Created: 2024-06-26    Try out the above example by running the Foam: Create New Note From Template command and selecting the your-first-template template. Notice what happens when your new note is created!  To remove this template, simply delete the .foam/templates/your-first-template.md file.  Enjoy! ","version":"Next","tagName":"h2"},{"title":"Python Celery","type":0,"sectionRef":"#","url":"/docs/practice/python-celery-workflow","content":"","keywords":"Python Celery","version":"Next"},{"title":"Construct a workflow​","type":1,"pageTitle":"Python Celery","url":"/docs/practice/python-celery-workflow#construct-a-workflow","content":" ","version":"Next","tagName":"h2"},{"title":"Avoid running synchronous subtasks within a task​","type":1,"pageTitle":"Python Celery","url":"/docs/practice/python-celery-workflow#avoid-running-synchronous-subtasks-within-a-task","content":" ","version":"Next","tagName":"h2"},{"title":"Asynchronous tasks with a task​","type":1,"pageTitle":"Python Celery","url":"/docs/practice/python-celery-workflow#asynchronous-tasks-with-a-task","content":" @app.task(bind=True) def update_page_info(self, url): # fetch_page -&gt; parse_page -&gt; store_page chain = fetch_page.s(url) | parse_page.s() | store_page_info.s(url) # chain() self.replace(chain) @app.task() def fetch_page(url): return myhttplib.get(url) @app.task() def parse_page(page): return myparser.parse_document(page) @app.task(ignore_result=True) def store_page_info(info, url): PageInfo.objects.create(url=url, info=info)   ","version":"Next","tagName":"h2"},{"title":"Monitor the workflow​","type":1,"pageTitle":"Python Celery","url":"/docs/practice/python-celery-workflow#monitor-the-workflow","content":" ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Python Celery","url":"/docs/practice/python-celery-workflow#resources","content":" Designing Dynamic Workflows with Celery and Python | by Marin Aglić | Data Engineer Things  The Curious Case of Celery Work-flows  Celery ETA Tasks Demystified. At Instawork, we use Celery to queue… | by Oleg Pesok | Instawork Engineering  Canvas: Designing Work-flows — Celery 5.3.6 documentation ","version":"Next","tagName":"h2"},{"title":"REST API Filtering, Sorting and Pagination","type":0,"sectionRef":"#","url":"/docs/practice/rest-api-filtering-sorting-pagination","content":"REST API Filtering, Sorting and Pagination api-guidelines/Guidelines.md at vNext · microsoft/api-guidelines · GitHub REST API Design: Filtering, Sorting, and Pagination | Moesif Blog How we write our query filter engine on our REST API (part 1) | by Adam Ben Aharon | Melio’s R&amp;D blog | Medium","keywords":"rest api filtering  sorting and pagination","version":"Next"},{"title":"Resumable Upload","type":0,"sectionRef":"#","url":"/docs/practice/resumable-upload","content":"","keywords":"file upload resumable","version":"Next"},{"title":"A Basic Resumable Upload​","type":1,"pageTitle":"Resumable Upload","url":"/docs/practice/resumable-upload#a-basic-resumable-upload","content":" fastapi_resumable_upload.py loading... View on GitHub  ","version":"Next","tagName":"h2"},{"title":"TUS Resumable Upload​","type":1,"pageTitle":"Resumable Upload","url":"/docs/practice/resumable-upload#tus-resumable-upload","content":" FastAPI implementing tus v1.0.0 server in Python  fastapi_tusd.py loading... View on GitHub  Implementations | tus.io  Resumable file upload  GitHub - tus/tus-js-client: A pure JavaScript client for the tus resumable upload protocol  GitHub - tus/tusd: Reference server implementation in Go of tus: the open protocol for resumable file uploads  IO, StreamIO, FileIO  high-level used by asyncio.io in socket/tcp/http  Streams — Python 3.11.4 documentation  starlette.Request.stream = http Request Body  low-level: io — Core tools for working with streams — Python 3.11.4 documentation ","version":"Next","tagName":"h2"},{"title":"RPC vs MQ","type":0,"sectionRef":"#","url":"/docs/practice/rpc_vs_mq","content":"","keywords":"","version":"Next"},{"title":"RPC​","type":1,"pageTitle":"RPC vs MQ","url":"/docs/practice/rpc_vs_mq#rpc","content":" ","version":"Next","tagName":"h2"},{"title":"IPC​","type":1,"pageTitle":"RPC vs MQ","url":"/docs/practice/rpc_vs_mq#ipc","content":" IPC: (local)Inter-Process Communication  Using gRPC for (local) inter-process communication  IPC Benchmark  ","version":"Next","tagName":"h2"},{"title":"MQ​","type":1,"pageTitle":"RPC vs MQ","url":"/docs/practice/rpc_vs_mq#mq","content":"","version":"Next","tagName":"h2"},{"title":"Serialization","type":0,"sectionRef":"#","url":"/docs/practice/serialization","content":"","keywords":"MessagePack msgpack json serialization Protocol Buffers Protobuf","version":"Next"},{"title":"Json​","type":1,"pageTitle":"Serialization","url":"/docs/practice/serialization#json","content":" ","version":"Next","tagName":"h2"},{"title":"MessagePack​","type":1,"pageTitle":"Serialization","url":"/docs/practice/serialization#messagepack","content":" msgpack GitHub  ","version":"Next","tagName":"h2"},{"title":"Protocol Buffers​","type":1,"pageTitle":"Serialization","url":"/docs/practice/serialization#protocol-buffers","content":" Protocol Buffers  ","version":"Next","tagName":"h2"},{"title":"Supported Features​","type":1,"pageTitle":"Serialization","url":"/docs/practice/serialization#supported-features","content":" Protocol\tDiscriminator Property &amp; PolymorphismJson\t✔️ MessagePack\t✔️ Protobuf\t✖️ ","version":"Next","tagName":"h2"},{"title":"Celery","type":0,"sectionRef":"#","url":"/docs/practice/python-celery","content":"","keywords":"celery","version":"Next"},{"title":"Celery worker​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#celery-worker","content":" Celery worker Mechanism:  To start a Celery worker will start a main process that will spawn child processes or threads(based on the --pool option): the main process will handle receiving task/sending task result the and these child processes/threads(a.k.a execution pool) execute the actual tasks.  To increase the number of child processes/threads(via --concurrency option) will increase the number of tasks the Celery worker can process in parallel. More processes are usually better.  However, in reality, there are some situations in following modes:  Run N workers with M child processes each.Run 1 worker with N*M child processes.Run N workers with only 1 main process each.Run N workers with M child threads each.Run 1 worker with N*M child threads.  Whether to use processes or threads depends on what your tasks will actually do and whether they are GPU bound or IO bound.  ","version":"Next","tagName":"h2"},{"title":"Worker procedure​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#worker-procedure","content":" from celery import Celery app = Celery(...) @app.task() def add(x, y): return x + y @app.task() def mul(x, y): return x * y   The @app.task decoration will use Task class in default if you don't specify explicitly.  When a worker start by celery -A tasks worker,  Worker will spawn child Processes, the number of child Processes is based on CPU cores in default.Each child Process will initialize a Task instance for every decorated function. Here add() has its own Task instance and mul() also has its own Task instance respectively.  When a client call add.delay(1, 2),  Worker receive a Task in Queue.Worker assign the Task to a child Process, which will determine to use which Task instance to execute. A Task instance is initialized in each decorated function and registered with a task name using function name in default(such as add, mul). Here is the Task instance with name add() should be picked up to run the task.When be decorated in add(), the Task instance run() method will be add() original function body. The child Process will use the Task instance's __call__() method to run task, and __call__() will invoke the run() within itself.  ","version":"Next","tagName":"h3"},{"title":"Option --pool=prefork​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#option---poolprefork","content":" It spawns multiple processes.  When start a Celery worker via celery -A tasks worker --loglevel INFO --concurrency 3 --pool=prefork, what will happen underneath?  Celery start a main process.The main process will then spawn 3 child processes. The default concurrency is based on the number of CPU available on the machine. The default pool is prefork which uses multiprocessing library from Python.These child processes will execute the tasks assigned from the main process.  ","version":"Next","tagName":"h3"},{"title":"Option --pool=eventlet or --pool=gevent​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#option---pooleventlet-or---poolgevent","content":" It creates multiple threads.  When start a Celery worker via celery -A tasks worker --loglevel INFO --concurrency 3 --pool=eventlet  ","version":"Next","tagName":"h3"},{"title":"Option --pool=solo​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#option---poolsolo","content":" It will not create any child process or thread to run task. The tasks will be executed in main process, which causes the main process to be blocked.  It seems as: Run 1 worker with 1 process, however --concurrency will not take any effect when --pool=solo!  When coming to a microservices environment, this option becomes useful and practical especially running CPU intensive tasks. The container manager such as Docker can increase the task processing capabilities through managing the number of worker containers instead of managing the number of pool processes per worker.  When start a Celery worker via celery -A tasks worker --loglevel INFO --pool=solo  ","version":"Next","tagName":"h3"},{"title":"Celery Task​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#celery-task","content":" What's the lifecycle of a Celery task from the time it's created to the it's done?  Here we analyze a simple task with all Celery configuration in default and use Redis as broker and backend  @app.task(acks_late=True) def wait(secs: float) -&gt; str: print(f&quot;wait() - Start, secs[{secs}]s&quot;) time.sleep(secs) print(f&quot;wait() - Done, secs[{secs}]s&quot;) return f&quot;wait() - Done, secs[{secs}]s&quot;   When a client call wait.delay(60), this task is added to a default queue named celery in Redis.Celery worker polls the queue and pulls the task, then it removes the task from the queue and moves it a special queue named unacked in Redis.The worker holds on to the task(prefetch), until it has abilities to process the task.Once after The worker successfully processes the task, it acks now (acks_late=True) that it removes the task from the unacked queue in Redis. If acks_late=False, the worker acks before processing the task.  Let's get more concrete understanding in practices.  First, let's enter a redis-cli interactive mode with the newly launched application,  127.0.0.1:6379&gt; KEYS * 1) &quot;_kombu.binding.email_service&quot; 2) &quot;_kombu.binding.ml_service&quot; 3) &quot;_kombu.binding.celery.pidbox&quot; 4) &quot;_kombu.binding.celeryev&quot; 5) &quot;_kombu.binding.celery&quot;   At the beginning, you can see that the celery key and the unacked key do not exist in Redis.  Then, let's call wait.delay(60) multiple times at the same time,  127.0.0.1:6379&gt; KEYS * 1) &quot;unacked_index&quot; 2) &quot;_kombu.binding.email_service&quot; 3) &quot;_kombu.binding.celery.pidbox&quot; 4) &quot;celery-task-meta-3d6b2028-6ee6-4e2c-85f1-cbeba644aca5&quot; 5) &quot;celery&quot; 6) &quot;_kombu.binding.celeryev&quot; 7) &quot;_kombu.binding.celery&quot; 8) &quot;_kombu.binding.ml_service&quot; 9) &quot;celery-task-meta-e5a1b7db-f1ad-4d3e-b2b9-3b7de8f8c87e&quot; 10) &quot;unacked&quot; 127.0.0.1:6379&gt; TYPE unacked hash 127.0.0.1:6379&gt; TYPE celery list   After we create tasks, the celery key of list type and the unacked key of hash type are both created in Redis.  127.0.0.1:6379&gt; LRANGE celery 0 -1 1) &quot;{\\&quot;body\\&quot;: \\&quot;W1s2MC4wXSwge30sIHsiY2FsbGJhY2tzIjogbnVsbCwgImVycmJhY2tzIjogbnVsbCwgImNoYWluIjogbnVsbCwgImNob3JkIjogbnVsbH1d\\&quot;, \\&quot;content-encoding\\&quot;: \\&quot;utf-8\\&quot;, \\&quot;content-type\\&quot;: \\&quot;application/json\\&quot;, \\&quot;headers\\&quot;: {\\&quot;lang\\&quot;: \\&quot;py\\&quot;, \\&quot;task\\&quot;: \\&quot;app.celery_app.tasks.wait\\&quot;, \\&quot;id\\&quot;: \\&quot;da959152-1f45-4846-99e4-5205d30c1be7\\&quot;, \\&quot;shadow\\&quot;: null, \\&quot;eta\\&quot;: null, \\&quot;expires\\&quot;: null, \\&quot;group\\&quot;: null, \\&quot;group_index\\&quot;: null, \\&quot;retries\\&quot;: 0, \\&quot;timelimit\\&quot;: [null, null], \\&quot;root_id\\&quot;: \\&quot;da959152-1f45-4846-99e4-5205d30c1be7\\&quot;, \\&quot;parent_id\\&quot;: null, \\&quot;argsrepr\\&quot;: \\&quot;(60.0,)\\&quot;, \\&quot;kwargsrepr\\&quot;: \\&quot;{}\\&quot;, \\&quot;origin\\&quot;: \\&quot;gen11@a840cdd15b13\\&quot;, \\&quot;ignore_result\\&quot;: false}, \\&quot;properties\\&quot;: {\\&quot;correlation_id\\&quot;: \\&quot;da959152-1f45-4846-99e4-5205d30c1be7\\&quot;, \\&quot;reply_to\\&quot;: \\&quot;4b0f2f2d-aee2-3349-81ab-e95a1f0e9f02\\&quot;, \\&quot;delivery_mode\\&quot;: 2, \\&quot;delivery_info\\&quot;: {\\&quot;exchange\\&quot;: \\&quot;\\&quot;, \\&quot;routing_key\\&quot;: \\&quot;celery\\&quot;}, \\&quot;priority\\&quot;: 0, \\&quot;body_encoding\\&quot;: \\&quot;base64\\&quot;, \\&quot;delivery_tag\\&quot;: \\&quot;d657c66d-4e4b-483d-9fbe-fe4b5b9541e7\\&quot;}}&quot; 2) &quot;{\\&quot;body\\&quot;: \\&quot;W1s2MC4wXSwge30sIHsiY2FsbGJhY2tzIjogbnVsbCwgImVycmJhY2tzIjogbnVsbCwgImNoYWluIjogbnVsbCwgImNob3JkIjogbnVsbH1d\\&quot;, \\&quot;content-encoding\\&quot;: \\&quot;utf-8\\&quot;, \\&quot;content-type\\&quot;: \\&quot;application/json\\&quot;, \\&quot;headers\\&quot;: {\\&quot;lang\\&quot;: \\&quot;py\\&quot;, \\&quot;task\\&quot;: \\&quot;app.celery_app.tasks.wait\\&quot;, \\&quot;id\\&quot;: \\&quot;1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8\\&quot;, \\&quot;shadow\\&quot;: null, \\&quot;eta\\&quot;: null, \\&quot;expires\\&quot;: null, \\&quot;group\\&quot;: null, \\&quot;group_index\\&quot;: null, \\&quot;retries\\&quot;: 0, \\&quot;timelimit\\&quot;: [null, null], \\&quot;root_id\\&quot;: \\&quot;1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8\\&quot;, \\&quot;parent_id\\&quot;: null, \\&quot;argsrepr\\&quot;: \\&quot;(60.0,)\\&quot;, \\&quot;kwargsrepr\\&quot;: \\&quot;{}\\&quot;, \\&quot;origin\\&quot;: \\&quot;gen11@a840cdd15b13\\&quot;, \\&quot;ignore_result\\&quot;: false}, \\&quot;properties\\&quot;: {\\&quot;correlation_id\\&quot;: \\&quot;1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8\\&quot;, \\&quot;reply_to\\&quot;: \\&quot;4b0f2f2d-aee2-3349-81ab-e95a1f0e9f02\\&quot;, \\&quot;delivery_mode\\&quot;: 2, \\&quot;delivery_info\\&quot;: {\\&quot;exchange\\&quot;: \\&quot;\\&quot;, \\&quot;routing_key\\&quot;: \\&quot;celery\\&quot;}, \\&quot;priority\\&quot;: 0, \\&quot;body_encoding\\&quot;: \\&quot;base64\\&quot;, \\&quot;delivery_tag\\&quot;: \\&quot;927d1ac0-3709-4e23-8c0f-037713c55217\\&quot;}}&quot;   127.0.0.1:6379&gt; HGETALL unacked 1) &quot;927d1ac0-3709-4e23-8c0f-037713c55217&quot; 2) &quot;[{\\&quot;body\\&quot;: \\&quot;W1s2MC4wXSwge30sIHsiY2FsbGJhY2tzIjogbnVsbCwgImVycmJhY2tzIjogbnVsbCwgImNoYWluIjogbnVsbCwgImNob3JkIjogbnVsbH1d\\&quot;, \\&quot;content-encoding\\&quot;: \\&quot;utf-8\\&quot;, \\&quot;content-type\\&quot;: \\&quot;application/json\\&quot;, \\&quot;headers\\&quot;: {\\&quot;lang\\&quot;: \\&quot;py\\&quot;, \\&quot;task\\&quot;: \\&quot;app.celery_app.tasks.wait\\&quot;, \\&quot;id\\&quot;: \\&quot;1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8\\&quot;, \\&quot;shadow\\&quot;: null, \\&quot;eta\\&quot;: null, \\&quot;expires\\&quot;: null, \\&quot;group\\&quot;: null, \\&quot;group_index\\&quot;: null, \\&quot;retries\\&quot;: 0, \\&quot;timelimit\\&quot;: [null, null], \\&quot;root_id\\&quot;: \\&quot;1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8\\&quot;, \\&quot;parent_id\\&quot;: null, \\&quot;argsrepr\\&quot;: \\&quot;(60.0,)\\&quot;, \\&quot;kwargsrepr\\&quot;: \\&quot;{}\\&quot;, \\&quot;origin\\&quot;: \\&quot;gen11@a840cdd15b13\\&quot;, \\&quot;ignore_result\\&quot;: false}, \\&quot;properties\\&quot;: {\\&quot;correlation_id\\&quot;: \\&quot;1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8\\&quot;, \\&quot;reply_to\\&quot;: \\&quot;4b0f2f2d-aee2-3349-81ab-e95a1f0e9f02\\&quot;, \\&quot;delivery_mode\\&quot;: 2, \\&quot;delivery_info\\&quot;: {\\&quot;exchange\\&quot;: \\&quot;\\&quot;, \\&quot;routing_key\\&quot;: \\&quot;celery\\&quot;}, \\&quot;priority\\&quot;: 0, \\&quot;body_encoding\\&quot;: \\&quot;base64\\&quot;, \\&quot;delivery_tag\\&quot;: \\&quot;927d1ac0-3709-4e23-8c0f-037713c55217\\&quot;}}, \\&quot;\\&quot;, \\&quot;celery\\&quot;]&quot;   Wait for all these tasks to be done  127.0.0.1:6379&gt; KEYS * 1) &quot;_kombu.binding.email_service&quot; 2) &quot;celery-task-meta-da959152-1f45-4846-99e4-5205d30c1be7&quot; 3) &quot;celery-task-meta-815587f5-782d-454a-8498-b4ebbb91abd8&quot; 4) &quot;_kombu.binding.celery.pidbox&quot; 5) &quot;celery-task-meta-3d6b2028-6ee6-4e2c-85f1-cbeba644aca5&quot; 6) &quot;_kombu.binding.celeryev&quot; 7) &quot;_kombu.binding.celery&quot; 8) &quot;_kombu.binding.ml_service&quot; 9) &quot;celery-task-meta-1ddc3c5e-fa33-4d12-aa3f-c3d13581a4c8&quot; 10) &quot;celery-task-meta-e5a1b7db-f1ad-4d3e-b2b9-3b7de8f8c87e&quot;   After all tasks are done successfully, both keys: celery and unacked are removed from Redis.  The result of a task is stored in celery-task-meta-{{uuid}} key.  127.0.0.1:6379&gt; TYPE celery-task-meta-da959152-1f45-4846-99e4-5205d30c1be7 string 127.0.0.1:6379&gt; GET celery-task-meta-da959152-1f45-4846-99e4-5205d30c1be7 &quot;{\\&quot;status\\&quot;: \\&quot;SUCCESS\\&quot;, \\&quot;result\\&quot;: \\&quot;wait() - Done, secs[60.0]s\\&quot;, \\&quot;traceback\\&quot;: null, \\&quot;children\\&quot;: [], \\&quot;date_done\\&quot;: \\&quot;2024-11-07T07:54:16.954872\\&quot;, \\&quot;task_id\\&quot;: \\&quot;da959152-1f45-4846-99e4-5205d30c1be7\\&quot;}&quot;   ","version":"Next","tagName":"h2"},{"title":"Serve machine learning model​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#serve-machine-learning-model","content":" Properly running a machine learning model in task is different with running other jobs as we need avoiding loading ML model every time we run tasks. So it is stateful that we should keep something in worker.  ","version":"Next","tagName":"h2"},{"title":"Different workers for different tasks​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#different-workers-for-different-tasks","content":" Assuming a such situation: There is a worker x to only handle email tasks and a worker y to only handle machine learning related tasks.  These are configurations for project x:  #Celery routing. app.conf.task_routes = { 'celery_app.email_tasks.*': { 'queue': 'email_service', }, } #Run celery. celery -A celery_app.email_tasks:app worker -l info -E -Q email_service   These are configurations for project y:  #Celery routing. app.conf.task_routes = { 'celery_app.ml_tasks.*': { 'queue': 'ml_service', }, } #Run celery. celery -A celery_app.ml_tasks:app worker -l info -E -Q ml_service   Details in explanation:  Different workers handle their own queues for separate tasks.  Look at https://github.com/liviaerxin/fastapi-celery-ml for see a complete Celery project.  ","version":"Next","tagName":"h2"},{"title":"Code Analysis​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#code-analysis","content":" from celery import signature sig = add.s(2, 2) sig.freeze()   ","version":"Next","tagName":"h2"},{"title":"Known issues​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#known-issues","content":" Result state is always PENDING in windows  FIX: use --pool=solo instead of --pool=prefork in default. multiprocessing may cause this problem as its some defect in windows!  Long running jobs redelivering after broker visibility timeout with celery and redis · Issue #5935 · celery/celery · GitHubLong tasks are executed multiple times · Issue #3430 · celery/celery · GitHub  No Worker Heartbeat With Solo Pool · Issue #3768 · celery/celery · GitHub  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Celery","url":"/docs/practice/python-celery#resources","content":" Celery - Distributed Task Queue — Celery 5.3.4 documentation  Celery Execution Pools: What is it all about?  Celery Execution Pool: The worker and the pool - separation of concerns  Serving ML Models in Production with FastAPI and Celery | by Jonathan Readshaw | Towards Data Science  GitHub - liviaerxin/FastAPISpamDetection: Code for my Medium article: &quot;How you can quickly deploy your ML models with FastAPI&quot;  Celery ETA Tasks Demystified. At Instawork, we use Celery to queue… | by Oleg Pesok | Instawork Engineering ","version":"Next","tagName":"h2"},{"title":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","type":0,"sectionRef":"#","url":"/docs/system-design/building-chatgpt-style-conversation","content":"","keywords":"system-design data-structure chatgpt message-tree","version":"Next"},{"title":"The Problem: Conversations Aren't Flat​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#the-problem-conversations-arent-flat","content":" In a normal messaging app, conversations are usually linear. But in an AI chat interface like ChatGPT, users can:  Regenerate a previous responseFork a new conversation from any earlier messageTraverse back and forth in a conversation tree  This means the data structure must support branching, not just a flat message list.    ","version":"Next","tagName":"h2"},{"title":"The Solution: Message Trees​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#the-solution-message-trees","content":" Instead of storing messages in a simple list, we structure them as a tree:  Every message has a unique idEach message (except the root) has a parent_idA conversation is a path from the root message to a leaf message  root_msg └── msg_1 └── msg_2a └── msg_3 └── msg_2b &lt;- regenerated fork   To get the path for msg3: • msg3 → msg_2a → msg1 → root_msg  This allows us to support regenerations and alternate responses while preserving context.    ","version":"Next","tagName":"h2"},{"title":"Designing the Table in PostgreSQL​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#designing-the-table-in-postgresql","content":" CREATE TABLE messages ( id UUID PRIMARY KEY, parent_id UUID REFERENCES messages(id), role TEXT CHECK (role IN ('user', 'assistant')), content TEXT, created_at TIMESTAMP DEFAULT now() );   Each message is either from the user or the assistant, and optionally links to its parent message.    ","version":"Next","tagName":"h2"},{"title":"Reconstructing the Message Path with a Recursive Query​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#reconstructing-the-message-path-with-a-recursive-query","content":" To feed the correct context to the model, we need to reconstruct the full path from the root to a given message. PostgreSQL makes this easy with a recursive CTE:  WITH RECURSIVE path AS ( SELECT id, parent_id, role, content, created_at FROM messages WHERE id = 'leaf_message_id' -- starting point UNION ALL SELECT m.id, m.parent_id, m.role, m.content, m.created_at FROM messages m INNER JOIN path p ON m.id = p.parent_id ) SELECT * FROM path ORDER BY created_at;   This walks up the tree from the leaf to the root, and we then sort by timestamp to restore the original message order.    ","version":"Next","tagName":"h2"},{"title":"Handling Branches and Regenerations​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#handling-branches-and-regenerations","content":" Every time a user regenerates or forks a response:  A new message is created with the same parent_id as the message being regeneratedThis creates a new branch in the tree  To show branches in the UI (like ChatGPT does with “Regenerate” or “Continue from here”), simply:  SELECT * FROM messages WHERE parent_id = 'some_message_id';   You’ll get all the children of that message, representing possible next steps or alternate replies.    ","version":"Next","tagName":"h2"},{"title":"Why Not Store Child References?​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#why-not-store-child-references","content":" You might wonder if we should store child_id or even entire paths for fast lookup. In practice:  Storing child_id isn't necessary, as we can always query it and the parent may have more than one child.Caching full paths (denormalized) is an optimization, not a requirement.  This keeps the data normalized and clean, while PostgreSQL handles recursion efficiently.    ","version":"Next","tagName":"h2"},{"title":"Performance Tips​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#performance-tips","content":" Index parent_id for fast joinsUse UUID or short BIGINT ids depending on your needsOptionally cache reconstructed paths in Redis for real-time rendering    ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Building ChatGPT-Style Conversations: Message Structure and Recursive Retrieval in PostgreSQL","url":"/docs/system-design/building-chatgpt-style-conversation#conclusion","content":" Designing a ChatGPT-style conversation system requires more than just storing messages. You need a tree-based structure, recursive retrieval, and efficient querying. PostgreSQL, with its support for recursive CTEs, is a rock-solid choice for managing this elegantly.  Whether you’re building a chatbot platform, customer support tool, or AI assistant, these techniques can give you the flexibility to handle complex conversational flows just like ChatGPT.  Happy building! 🫠 ","version":"Next","tagName":"h2"},{"title":"CIL(Common Intermediate Language)","type":0,"sectionRef":"#","url":"/docs/wiki/common-intermediate-language","content":"","keywords":"","version":"Next"},{"title":"What is CIl/IL​","type":1,"pageTitle":"CIL(Common Intermediate Language)","url":"/docs/wiki/common-intermediate-language#what-is-cilil","content":" For C# or Java, the program is not directly compiled to machine code, but intermediate language code. For C#, the intermediate code is called Common Intermediate Language(CIL, or IL). So whether the *.dll or *.exe compiled from C#, is composed of IL code and its corresponding meta data. At runtime, the JIT(Just-In-Compiler) compile the IL code to the native machine code.  ","version":"Next","tagName":"h2"},{"title":"What is JIT​","type":1,"pageTitle":"CIL(Common Intermediate Language)","url":"/docs/wiki/common-intermediate-language#what-is-jit","content":" ./ilasm ~/Documents/peggy-foam-wiki/docs/IL/test/test.il -dll ./ildasm ~/Documents/peggy-foam-wiki/docs/IL/test/test.dll -t dotnet myapp.dll  ","version":"Next","tagName":"h2"},{"title":"Designing Comment Threads for TikTok-Style Applications","type":0,"sectionRef":"#","url":"/docs/system-design/designing-tiktok-comment-threads","content":"","keywords":"system-design data-structure tiktok comment-threads","version":"Next"},{"title":"Comment Thread Structure​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#comment-thread-structure","content":" Each video can have:  Many top-level commentsOptional replies, each pointing to a parent_comment_id  ","version":"Next","tagName":"h2"},{"title":"Data Model (Relational Database)​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#data-model-relational-database","content":" CREATE TABLE comments ( id BIGINT PRIMARY KEY, video_id BIGINT NOT NULL, user_id BIGINT NOT NULL, parent_comment_id BIGINT NULL, content TEXT NOT NULL, replies_n BIGINT NOT NULL DEFAULT 0, created_at TIMESTAMP NOT NULL DEFAULT NOW(), );     ","version":"Next","tagName":"h3"},{"title":"Reconstructing Comment Threads​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#reconstructing-comment-threads","content":" To render a full comment thread:  ","version":"Next","tagName":"h2"},{"title":"1. Fetch Top-Level Comments​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#1-fetch-top-level-comments","content":" SELECT * FROM comments WHERE video_id = :video_id AND parent_comment_id IS NULL ORDER BY replies_n DESC LIMIT 50;   ","version":"Next","tagName":"h3"},{"title":"2. Fetch Replies for Those Comments​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#2-fetch-replies-for-those-comments","content":" SELECT * FROM comments WHERE parent_comment_id IN (:top_level_comment_ids);   ","version":"Next","tagName":"h3"},{"title":"3. Group Replies by Parent (In Memory)​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#3-group-replies-by-parent-in-memory","content":" from collections import defaultdict replies_by_parent = defaultdict(list) for reply in replies: replies_by_parent[reply[&quot;parent_comment_id&quot;]].append(reply) thread = [] for comment in top_comments: comment[&quot;replies&quot;] = replies_by_parent.get(comment[&quot;id&quot;], []) thread.append(comment)   This reconstruction approach is efficient: O(N + M) time, where N = number of top-level comments, M = total replies.  ","version":"Next","tagName":"h3"},{"title":"Scaling Considerations​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#scaling-considerations","content":" To handle millions of comments:  Paginate top-level comments (and load replies lazily)Use indexes on video_id, parent_comment_idConsider denormalizing data for hot content  ","version":"Next","tagName":"h2"},{"title":"🔥 Bonus: Redis Caching for Hot Threads​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#-bonus-redis-caching-for-hot-threads","content":" To offload frequent reads and keep response times sub-ms, cache the full reconstructed thread in Redis.  ","version":"Next","tagName":"h2"},{"title":"Example Structure​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#example-structure","content":" { &quot;video_id&quot;: 123, &quot;comments&quot;: [ { &quot;id&quot;: 1, &quot;content&quot;: &quot;Nice vid!&quot;, &quot;replies&quot;: [ { &quot;id&quot;: 4, &quot;content&quot;: &quot;Agree!&quot; }, { &quot;id&quot;: 5, &quot;content&quot;: &quot;Same!&quot; } ] } ] }   ","version":"Next","tagName":"h3"},{"title":"Redis Key​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#redis-key","content":" comment_thread:video:123   ","version":"Next","tagName":"h3"},{"title":"Recompute and Cache​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#recompute-and-cache","content":" def recompute_comment_thread(video_id): top_comments = db.query(...) replies = db.query(...) ... redis.set(f&quot;comment_thread:video:{video_id}&quot;, json.dumps(thread), ex=600)   You can trigger recompute on new comment events or via background jobs.  ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Designing Comment Threads for TikTok-Style Applications","url":"/docs/system-design/designing-tiktok-comment-threads#conclusion","content":" Designing comment threads involves balancing simplicity, flexibility, and performance. Whether you're serving 100 users or 100 million, the pattern of parent-child comment structuring and smart reconstruction gives you a strong foundation.  And when performance matters most? Add a Redis layer to make it fly. ","version":"Next","tagName":"h2"},{"title":".NET Finalizer","type":0,"sectionRef":"#","url":"/docs/wiki/dotnet-finalizer","content":"","keywords":"","version":"Next"},{"title":"Tips​","type":1,"pageTitle":".NET Finalizer","url":"/docs/wiki/dotnet-finalizer#tips","content":" finalizers should not be accessing managed objects. ","version":"Next","tagName":"h2"},{"title":"Graphical User Interface(GUI)","type":0,"sectionRef":"#","url":"/docs/wiki/graphical-user-interface","content":"","keywords":"","version":"Next"},{"title":"Android graphics​","type":1,"pageTitle":"Graphical User Interface(GUI)","url":"/docs/wiki/graphical-user-interface#android-graphics","content":" two core pieces:  SurfaceFlingerSkia  Graphics | Android Open Source ProjectAndroid Graphics Internals - Stack Overflow  ","version":"Next","tagName":"h2"},{"title":"WayLand​","type":1,"pageTitle":"Graphical User Interface(GUI)","url":"/docs/wiki/graphical-user-interface#wayland","content":" What is Wayland? · Writing Wayland clients  The Hello Wayland Tutorial | FLOSS &amp; Cia  How to use Wayland with C to make a Linux app | by Sergey Bugaev | Medium ","version":"Next","tagName":"h2"},{"title":"LevelDB","type":0,"sectionRef":"#","url":"/docs/wiki/leveldb","content":"LevelDB Principle and use of leveldb - Birost SSTable and Log Structured Storage: LevelDB - igvita.com LevelDB Benchmarks","keywords":"","version":"Next"},{"title":"IoC","type":0,"sectionRef":"#","url":"/docs/wiki/ioc","content":"IoC Introduction IoC, DIP, DI and IoC Container Lazily resolving services to fix circular dependencies in .NET Core - Thomas Levesque's .NET Blog Dealing With Circular Dependency Injection References - .NET Core Tutorials","keywords":"","version":"Next"},{"title":"WiFi AutoSwitch Windows","type":0,"sectionRef":"#","url":"/docs/wiki/wifi-autoswitch-windows","content":"WiFi AutoSwitch Windows If autoSwitch is turned on, it allows Windows to continue looking for other auto-connected wireless networks while connected to the current wireless network. If a higher priority auto-connected wireless network than the currently connected wireless network comes in range, Windows will automatically connect to it instead. It also needs to work along with priority setting. For example: There're 3 networks of profile name: TP-Link-1, TP-Link-2 and TP-Link-3. PC(windows) will try to connect TP-Link-3 if it's in range when it's already connected to TP-Link-1 or TP-Link-2. Setup autoswitch: netsh wlan set profileparameter name=&quot;TP-Link-1&quot; autoswitch=Yes netsh wlan set profileparameter name=&quot;TP-Link-2&quot; autoswitch=Yes netsh wlan set profileparameter name=&quot;TP-Link-3&quot; autoswitch=No Setup priority: netsh wlan set profileorder name=&quot;TP-Link-1&quot; interface=&quot;Wi-Fi&quot; priority=3 netsh wlan set profileorder name=&quot;TP-Link-2&quot; interface=&quot;Wi-Fi&quot; priority=2 netsh wlan set profileorder name=&quot;TP-Link-3&quot; interface=&quot;Wi-Fi&quot; priority=1 other tools: List profile name: netsh wlan show profiles List connected WiFi: netsh wlan show interfaces Enable Auto Switch for Wireless Network Connection in Windows 10 Change WiFi network priority in Windows 10","keywords":"autoswitch wifi windows","version":"Next"},{"title":"Python Unicode String","type":0,"sectionRef":"#","url":"/docs/practice/python-unicode-string","content":"","keywords":"Python Unicode memory layout","version":"Next"},{"title":"FAQ​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#faq","content":" ","version":"Next","tagName":"h2"},{"title":"Why Python doesn't use UTF-8 encoding variable-length bytes in memory directly, why it will convert them to UCS-2 or UCS-4 data?​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#why-python-doesnt-use-utf-8-encoding-variable-length-bytes-in-memory-directly-why-it-will-convert-them-to-ucs-2-or-ucs-4-data","content":" Keep each character in a string in the same width in the data memory.  What's the purpose?  Indexing into strings in Python is operated in a constant time, as it's based on the fixed-length encodings.  ","version":"Next","tagName":"h3"},{"title":"How do other programming languages access character in a string by index?​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#how-do-other-programming-languages-access-character-in-a-string-by-index","content":" Go Iterating yields Unicode code pointIndexing yields a byte Rust Iteration yields Unicode code point(method.chars()) or byte(method.bytes)Indexing not supported Python Iterating yields Unicode code pointIndexing yields Unicode code point  ","version":"Next","tagName":"h3"},{"title":"How a string in Python is printed on the screen?​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#how-a-string-in-python-is-printed-on-the-screen","content":" You may wonder why they have the same result on the terminal from Python output, assuming that the terminal is using the UTF-8 encoding.  &gt;&gt;&gt; &quot;\\xe9&quot; 'é' &gt;&gt;&gt; &quot;\\u00e9&quot; 'é' &gt;&gt;&gt; 'é' 'é'   When Python prints string, here the print() is used in default in Python IDE.  create PyObject. When the string is \\xe9 or \\u00e9, the Python interpret they as the code point of which 0xe9 is 233 in decimal. And convert it in PyUnicodeObject of which stores data use UCS-1 as 0xe9 is one byte.print the PyUnicodeObject. As the system locale uses UTF-8 Encoding, Python will convert UCS-1 data to utf-8 encoding bytes, which is b'\\xc3\\xa9'. &gt;&gt;&gt; 'é'.encode() b'\\xc3\\xa9' send bytes to the terminal emulator. b'\\xc3\\xa9' is sent to the terminal which uses the UTF-8 encoding. b'\\xc3\\xa9' in utf-8 is decode as code point \\u00e9 which represent the character é.draw character. The terminal emulator draw the glyph é on the screen as you see.  If the terminal using latin-1, &quot;\\xe9&quot; will be show as Ã© in the terminal.  ","version":"Next","tagName":"h3"},{"title":"How to write raw bytes to the terminal from Python?​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#how-to-write-raw-bytes-to-the-terminal-from-python","content":" As you see above, Python print strings:  firstly strings are encoded to utf-8 bytes.secondly these bytes are sent to the terminal.  Sometimes, you may want to write raw bytes in the terminal directly to see how the terminal represents these bytes on the screen.  ➜ ~ python3 -c 'import sys; sys.stdout.buffer.write(b&quot;\\xc3\\xa9&quot;)' é   UTF-8 terminal represent bytes b&quot;\\xc3\\xa9&quot; to character é.  ➜ ~ python3 -c 'import sys; sys.stdout.buffer.write(b&quot;\\xe9&quot;)' �   UTF-8 terminal can not represent bytes b&quot;\\xe9&quot; to any known character, as b&quot;\\xe9&quot; is not a valid utf-8 encoding bytes.  python - Python3 print raw byte - Stack Overflow  ","version":"Next","tagName":"h3"},{"title":"An outline of the PyUnicodeObject​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#an-outline-of-the-pyunicodeobject","content":" References are mainly from:  ctypes — A foreign function library for Python — Python 3.12.1 documentationunicodeobject.hunicodeobject.c  A brief PyUnicodeObject structure defined from unicodeobject.h:  cpython/Include/cpython/unicodeobject.h typedef struct { Py_ssize_t ob_refcnt; PyTypeObject *ob_type; } PyObject /* --- Unicode Type ------------------------------------------------------- */ typedef struct { /* There are 4 forms of Unicode strings: - compact ascii: * structure = PyASCIIObject * test: PyUnicode_IS_COMPACT_ASCII(op) * kind = PyUnicode_1BYTE_KIND * compact = 1 * ascii = 1 * ready = 1 * (length is the length of the utf8 and wstr strings) * (data starts just after the structure) * (since ASCII is decoded from UTF-8, the utf8 string are the data) - compact: * structure = PyCompactUnicodeObject * test: PyUnicode_IS_COMPACT(op) &amp;&amp; !PyUnicode_IS_ASCII(op) * kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND or PyUnicode_4BYTE_KIND * compact = 1 * ready = 1 * ascii = 0 * utf8 is not shared with data * utf8_length = 0 if utf8 is NULL * wstr is shared with data and wstr_length=length if kind=PyUnicode_2BYTE_KIND and sizeof(wchar_t)=2 or if kind=PyUnicode_4BYTE_KIND and sizeof(wchar_t)=4 * wstr_length = 0 if wstr is NULL * (data starts just after the structure) - legacy string, not ready: * structure = PyUnicodeObject * test: kind == PyUnicode_WCHAR_KIND * length = 0 (use wstr_length) * hash = -1 * kind = PyUnicode_WCHAR_KIND * compact = 0 * ascii = 0 * ready = 0 * interned = SSTATE_NOT_INTERNED * wstr is not NULL * data.any is NULL * utf8 is NULL * utf8_length = 0 - legacy string, ready: * structure = PyUnicodeObject structure * test: !PyUnicode_IS_COMPACT(op) &amp;&amp; kind != PyUnicode_WCHAR_KIND * kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND or PyUnicode_4BYTE_KIND * compact = 0 * ready = 1 * data.any is not NULL * utf8 is shared and utf8_length = length with data.any if ascii = 1 * utf8_length = 0 if utf8 is NULL * wstr is shared with data.any and wstr_length = length if kind=PyUnicode_2BYTE_KIND and sizeof(wchar_t)=2 or if kind=PyUnicode_4BYTE_KIND and sizeof(wchar_4)=4 * wstr_length = 0 if wstr is NULL Compact strings use only one memory block (structure + characters), whereas legacy strings use one block for the structure and one block for characters. Legacy strings are created by PyUnicode_FromUnicode() and PyUnicode_FromStringAndSize(NULL, size) functions. They become ready when PyUnicode_READY() is called. See also _PyUnicode_CheckConsistency(). */ PyObject_HEAD Py_ssize_t length; /* Number of code points in the string */ Py_hash_t hash; /* Hash value; -1 if not set */ struct { /* SSTATE_NOT_INTERNED (0) SSTATE_INTERNED_MORTAL (1) SSTATE_INTERNED_IMMORTAL (2) If interned != SSTATE_NOT_INTERNED, the two references from the dictionary to this object are *not* counted in ob_refcnt. */ unsigned int interned:2; /* Character size: - PyUnicode_WCHAR_KIND (0): * character type = wchar_t (16 or 32 bits, depending on the platform) - PyUnicode_1BYTE_KIND (1): * character type = Py_UCS1 (8 bits, unsigned) * all characters are in the range U+0000-U+00FF (latin1) * if ascii is set, all characters are in the range U+0000-U+007F (ASCII), otherwise at least one character is in the range U+0080-U+00FF - PyUnicode_2BYTE_KIND (2): * character type = Py_UCS2 (16 bits, unsigned) * all characters are in the range U+0000-U+FFFF (BMP) * at least one character is in the range U+0100-U+FFFF - PyUnicode_4BYTE_KIND (4): * character type = Py_UCS4 (32 bits, unsigned) * all characters are in the range U+0000-U+10FFFF * at least one character is in the range U+10000-U+10FFFF */ unsigned int kind:3; /* Compact is with respect to the allocation scheme. Compact unicode objects only require one memory block while non-compact objects use one block for the PyUnicodeObject struct and another for its data buffer. */ unsigned int compact:1; /* The string only contains characters in the range U+0000-U+007F (ASCII) and the kind is PyUnicode_1BYTE_KIND. If ascii is set and compact is set, use the PyASCIIObject structure. */ unsigned int ascii:1; /* The ready flag indicates whether the object layout is initialized completely. This means that this is either a compact object, or the data pointer is filled out. The bit is redundant, and helps to minimize the test in PyUnicode_IS_READY(). */ unsigned int ready:1; /* Padding to ensure that PyUnicode_DATA() is always aligned to 4 bytes (see issue #19537 on m68k). */ unsigned int :24; } state; wchar_t *wstr; /* wchar_t representation (null-terminated) */ } PyASCIIObject; /* Non-ASCII strings allocated through PyUnicode_New use the PyCompactUnicodeObject structure. state.compact is set, and the data immediately follow the structure. */ typedef struct { PyASCIIObject _base; Py_ssize_t utf8_length; /* Number of bytes in utf8, excluding the * terminating \\0. */ char *utf8; /* UTF-8 representation (null-terminated) */ Py_ssize_t wstr_length; /* Number of code points in wstr, possible * surrogates count as two code points. */ } PyCompactUnicodeObject; /* Strings allocated through PyUnicode_FromUnicode(NULL, len) use the PyUnicodeObject structure. The actual string data is initially in the wstr block, and copied into the data block using _PyUnicode_Ready. */ typedef struct { PyCompactUnicodeObject _base; union { void *any; Py_UCS1 *latin1; Py_UCS2 *ucs2; Py_UCS4 *ucs4; } data; /* Canonical, smallest-form Unicode buffer */ } PyUnicodeObject;   As it's known that each Unicode character in string is represented by a Unicode code point. In PyUnicodeObject, these code points are the encoding saved in the data, so PyUnicodeObject does not use the UTF-8 encoding in the data.  ","version":"Next","tagName":"h2"},{"title":"How a Unicode string object is created?​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#how-a-unicode-string-object-is-created","content":" Invokes several internal C functions in such a sequence generally,  PyObject *PyUnicode_FromStringAndSize(const char *str, Py_ssize_t size) PyObject *PyUnicode_DecodeUTF8Stateful(const char *str, Py_ssize_t size, const char *errors, Py_ssize_t *consumed) static PyObject *unicode_decode_utf8(const char *s, Py_ssize_t size, _Py_error_handler error_handler, const char *errors, Py_ssize_t *consumed) PyObject *PyUnicode_New(Py_ssize_t size, Py_UCS4 maxchar) static Py_ssize_t ascii_decode(const char *start, const char *end, Py_UCS1 *dest) ch = ucs2lib_utf8_decode(&amp;s, end, writer.data, &amp;writer.pos); // ucs2lib.h #define STRINGLIB(F) ucs2lib_##F STRINGLIB(utf8_decode)(const char **inptr, const char *end, STRINGLIB_CHAR *dest, Py_ssize_t *outpos)   ","version":"Next","tagName":"h2"},{"title":"Inspect Unicode string object in Python 3​","type":1,"pageTitle":"Python Unicode String","url":"/docs/practice/python-unicode-string#inspect-unicode-string-object-in-python-3","content":" Let's examine the internal data struct of a string object in modern Python 3.  note You keep the character being referred otherwise the GC may release that memory, Define the layout mapping unicodeobject, import ctypes # It's recommended to go to see [python 3.10 unicodeobject.h](https://github.com/python/cpython/blob/3.10/Include/cpython/unicodeobject.h#L85-L244) class PyASCIIObject(ctypes.Structure): # internal fields of the string object _fields_ = [ (&quot;ob_refcnt&quot;, ctypes.c_long), (&quot;ob_type&quot;, ctypes.c_void_p), (&quot;length&quot;, ctypes.c_ssize_t), (&quot;hash&quot;, ctypes.c_ssize_t), (&quot;interned&quot;, ctypes.c_uint, 2), (&quot;kind&quot;, ctypes.c_uint, 3), (&quot;compact&quot;, ctypes.c_uint, 1), (&quot;ascii&quot;, ctypes.c_uint, 1), (&quot;ready&quot;, ctypes.c_uint, 1), (&quot;_padding&quot;, ctypes.c_uint, 24), (&quot;wstr&quot;, ctypes.POINTER(ctypes.c_wchar)) ] def __repr__(self) -&gt; str: return f&quot;ob_refcnt[{self.ob_refcnt}], length[{self.length}], interned[{self.interned}], kind[{self.kind}], compact[{self.compact}], ascii[{self.ascii}], ready[{self.ready}]&quot; class PyCompactUnicodeObject(PyASCIIObject): # internal fields of the string object _fields_ = [ (&quot;utf8_length&quot;, ctypes.c_ssize_t), (&quot;utf8&quot;, ctypes.POINTER(ctypes.c_char)), (&quot;wstr_length&quot;, ctypes.c_ssize_t), ] def __repr__(self) -&gt; str: return super().__repr__() + f&quot; utf8_length[{self.utf8_length}], utf8[{self.utf8}], wstr_length[{self.wstr_length}]&quot; class PyUnicodeObject(PyCompactUnicodeObject): class _Data(ctypes.Union): _fields_ = [ (&quot;any&quot;, ctypes.c_void_p), (&quot;latin1&quot;, ctypes.POINTER(ctypes.c_uint8)), (&quot;ucs2&quot;, ctypes.POINTER(ctypes.c_uint16)), (&quot;ucs4&quot;, ctypes.POINTER(ctypes.c_uint32)), ] _fields_ = [ (&quot;data&quot;, _Data), ] Type: compact ascii. Key fields: kind[1], compact[1], ascii[1], ready[1] &gt;&gt;&gt; string_obj = &quot;Hello, ctypes!&quot; &gt;&gt;&gt; addr = id(string_obj) &gt;&gt;&gt; ascii_obj = PyASCIIObject.from_address(addr) &gt;&gt;&gt; print(ascii_obj) ob_refcnt[1], length[14], interned[0], kind[1], compact[1], ascii[1], ready[1] &gt;&gt;&gt; &gt;&gt;&gt; # compact ascii: data starts just after the structure &gt;&gt;&gt; data_addr = addr + ctypes.sizeof(PyASCIIObject) &gt;&gt;&gt; data = ctypes.cast(data_addr, ctypes.c_char_p) &gt;&gt;&gt; print(f&quot;data: {data.value}&quot;) data: b'Hello, ctypes!' Type: compact UCS-2. Key fields: kind[1], compact[1], ascii[1], ready[1] &gt;&gt;&gt; string_obj = &quot;你好!&quot; &gt;&gt;&gt; addr = id(string_obj) &gt;&gt;&gt; ascii_obj = PyASCIIObject.from_address(addr) &gt;&gt;&gt; print(ascii_obj) ob_refcnt[1], length[3], interned[0], kind[2], compact[1], ascii[0], ready[1] &gt;&gt;&gt; &gt;&gt;&gt; compact_obj = PyCompactUnicodeObject.from_address(addr) &gt;&gt;&gt; print(compact_obj) ob_refcnt[1], length[3], interned[0], kind[2], compact[1], ascii[0], ready[1] utf8_length[0], utf8[&lt;ctypes.LP_c_char object at 0x7f0c29297ac 0&gt;], wstr_length[0] &gt;&gt;&gt; &gt;&gt;&gt; # compact: data starts just after the structure &gt;&gt;&gt; data_addr = addr + ctypes.sizeof(PyCompactUnicodeObject) &gt;&gt;&gt; data = ctypes.cast(data_addr, ctypes.POINTER(ctypes.c_uint16)) &gt;&gt;&gt; print(f&quot;data: {data[0]}, {data[0]:#06x}, {chr(data[0])}&quot;) data: 20320, 0x4f60, 你 &gt;&gt;&gt; print(f&quot;data: {data[1]}, {data[1]:#06x}, {chr(data[1])}&quot;) data: 22909, 0x597d, 好 &gt;&gt;&gt; print(f&quot;data: {data[2]}, {data[2]:#06x}, {chr(data[2])}&quot;) data: 33, 0x0021, ! &gt;&gt;&gt; print(f&quot;data: {data[3]}, {data[3]:#06x}, {chr(data[3])}&quot;) data: 0, 0x0000, Type: compact UCS-4. Key fields: kind[4], compact[1], ascii[1], ready[1] &gt;&gt;&gt; string_obj = &quot;你好🤨&quot; &gt;&gt;&gt; addr = id(string_obj) &gt;&gt;&gt; ascii_obj = PyASCIIObject.from_address(addr) &gt;&gt;&gt; print(ascii_obj) ob_refcnt[1], length[3], interned[0], kind[4], compact[1], ascii[0], ready[1] &gt;&gt;&gt; &gt;&gt;&gt; compact_obj = PyCompactUnicodeObject.from_address(addr) &gt;&gt;&gt; print(compact_obj) ob_refcnt[1], length[3], interned[0], kind[4], compact[1], ascii[0], ready[1] utf8_length[0], utf8[&lt;ctypes.LP_c_char object at 0x7f0c292b1ac 0&gt;], wstr_length[3] &gt;&gt;&gt; &gt;&gt;&gt; # compact: data starts just after the structure &gt;&gt;&gt; data_addr = addr + ctypes.sizeof(PyCompactUnicodeObject) &gt;&gt;&gt; data = ctypes.cast(data_addr, ctypes.POINTER(ctypes.c_uint32)) &gt;&gt;&gt; print(f&quot;data: {data[0]}, {data[0]:#010x}, {chr(data[0])}&quot;) data: 20320, 0x00004f60, 你 &gt;&gt;&gt; print(f&quot;data: {data[1]}, {data[1]:#010x}, {chr(data[1])}&quot;) data: 22909, 0x0000597d, 好 &gt;&gt;&gt; print(f&quot;data: {data[2]}, {data[2]:#010x}, {chr(data[2])}&quot;) data: 129320, 0x0001f928, 🤨 &gt;&gt;&gt; print(f&quot;data: {data[3]}, {data[3]:#010x}, {chr(data[3])}&quot;) data: 0, 0x00000000, Type: legacy string. Key fields: kind[2], compact[0], ascii[0] I can't produce it in Python3.10, maybe you can try python2.7. All codes are at object layout Resources​ How Python saves memory when storing strings | Artem Golubin Python behind the scenes #9: how Python strings work https://nedbatchelder.com/text/unipain.html https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/ ","version":"Next","tagName":"h2"},{"title":"Learn ASGI","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-asgi","content":"Learn ASGI Nowadays as web server framework is getting easy to use and work with. In Python areas, FastAPI obtains nearly 60k stars and becomes the most popular web framework for Pythoners. Looking at the advantage of FastAPI, it simplifies everything from parsing http requests, middleware processing, authentication, database manipulation and more. Let's dive into the behind-the-scenes technique stacks of FastAPI. Before research, there are some common questions around the web server development: How to process messages on HTTP protocol on TCP protocol? What're the favorite library used to do that?What are the differences between WSGI and ASGI?Data model used for database and users stacks from low-level to high-level Uvicorn: ASGI web server implementation/interface scopereceivesend h11 to process HTTP messageswebsocket to process websocket messages Starlette: ASGI frameworkabstract Request class for receive in Uvicornabstract Response class for send in Uvicornprovide middleware FastAPI: Fast to codeOpenAPI docsPydantic native model APIRoute APIRouter `Application` &lt;-- `APIRouter` &lt;-- `APIRoute` ","keywords":"Learn ASGI","version":"Next"},{"title":"Wiki Avalonia","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-avalonia","content":"","keywords":"practice avalonia","version":"Next"},{"title":"SkiaSharp​","type":1,"pageTitle":"Wiki Avalonia","url":"/docs/wiki/wiki-avalonia#skiasharp","content":" SkiaSharp/GRContextTest.cs at main · mono/SkiaSharp · GitHub ","version":"Next","tagName":"h2"},{"title":"Wiki Assembly","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-assembly","content":"","keywords":"Wiki Assembly","version":"Next"},{"title":"Assembler​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#assembler","content":" GNU assembler (GAS) x86-64 GNU assembler AT&amp;T syntax aarch64 GNU assembler aarch64/arm64 syntax Clang Assembler external Assembler GNU Assembler LLVM’s integrated assemblerhttps://clang.llvm.org/docs/Toolchain.html#assemble Netwide assembler (NASM) Intel syntaxx86-64 macOSlinuxwindows MASM Intel syntax  A example NASM assembly file print.asm to print message  print.asm ; print.asm ; nasm -f elf64 print.asm &amp;&amp; ld print.o &amp;&amp; ./a.out ; echo $? ; objdump -d a.out section .data message db, &quot;Welcome, to, Segmentation, Faults!, &quot; section .text global _start _printMessage: mov rax, 4 ; System call number for sys_write mov rbx, 1 ; File descriptor 1 is stdout mov rcx, message ; Pointer to the message string mov rdx, 32 ; Length of the message int 0x80 ; Call kernel ret ; Return from the function _exit: mov rax, 1 ; System call number for sys_exit mov rbx, 0 ; Exit code 0 int 0x80 ; Call kernel _start: call _printMessage ; Call the print message function mov rax, 1 ; System call number for sys_exit mov rbx, 1 ; Exit code 0 int 0x80 ; Call kernel   sum.asm ; sum.asm ; nasm -f elf64 sum.asm &amp;&amp; ld sum.o &amp;&amp; ./a.out ; echo $? ; objdump -d a.out section .text global _start ; Function to calculate the sum of two integers sum: mov rax, rdi ; Move the first argument (a) to rax add rax, rsi ; Add the second argument (b) to rax ret ; Return with the result in rax _start: ; Example usage of the sum function mov rdi, 5 ; First argument: a = 5 mov rsi, 7 ; Second argument: b = 7 call sum ; Call the sum function ; The result is now in rax ; It can be used or printed, depending on the context mov rdi, rax ; Exit code 0 ; Exit the program mov rax, 60 ; System call number for sys_exit syscall ; Make the system call   ","version":"Next","tagName":"h2"},{"title":"Memory Layout​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#memory-layout","content":" The structure of an assembly file generally consists of serval section:  .text section: .text section is generally read-only. It is typically used for storing executable code, and it is not intended to be modified during program execution..text section contains the machine code instructions that the processor will execute..text section contains global constant data. .data section: .data section is writable. It is used for storing initialized data that can be modified during the execution of the program..data section contains global variable data. .bss section: It's mostly the same with .data section except it's used for storing uninitialized data .rodata section: It is used for read-only data, such as constant strings.  Here's a simple example illustrating the use of these sections:  .section .text .global _start _start: // Code goes here .section .data my_data: .word 42 // Initialized data .section .bss my_uninitialized_data: .skip 4 // Uninitialized data, occupies 4 bytes .section .rodata my_string: .asciz &quot;Hello, World!&quot; // Read-only data   A compiled program's memory layout consists of these segments. A running program's memory layout consists of these segments, and also heap and stack memory.  ","version":"Next","tagName":"h2"},{"title":"Memory Layout of a Running Program​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#memory-layout-of-a-running-program","content":" A running program typically consists of serval segments or sections, each serving a specific purpose but common sections include:  Stack: Stores local variables and function call information.Memory is automatically allocated and de-allocated as functions are called and return.Register(sp in arm64, stack pointer) is used to manage and point to the stack memory.Size is limited(may lead to stack overflow). set via ulimit -s in linux. Heap: Dynamic memory managed by programmer at runtime.Memory is allocated and deallocated explicitly using functions like malloc/free in C, and new/delete in C++, brk system call in assembly etc.Store dynamic data that can be shared across functions. Data lifecycle is not bound to functions.Size is much larger than the stack, Data(.data, .bss): Stores global variables/constants, separated into initialized and uninitialized Text(.text): Stores the code being executed  CS 225 | Stack and Heap Memory  ","version":"Next","tagName":"h2"},{"title":"Label​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#label","content":" Label  ","version":"Next","tagName":"h2"},{"title":"Instruction​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#instruction","content":" Assembly instructions are human readable representation of the machine code as CPU can only understand the machine code.  Instruction: Opcode + Oprand  ","version":"Next","tagName":"h2"},{"title":"Opcode​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#opcode","content":" Intel x86 Assembler Instruction Set Opcode Table  ","version":"Next","tagName":"h3"},{"title":"Oprand​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#oprand","content":" Data area  Register Operand​  Register Operand  mov rdi, rsi   Immediate Operand​  Immediate Operand  mov rdi, 0x21 mov rdi, 5 mov edi, 0x21314151   In aarch64, the immediate value is subject to:  Arithmetic instructions (add{s}, sub{s}, cmp, cmn) take a 12-bit unsigned immediate with an optional 12-bit left shift.Move instructions (movz, movn, movk) take a 16-bit immediate optionally shifted to any 16-bit-aligned position within the register.Address calculations (adr, adrp) take a 21-bit signed immediate, although there's no actual syntax to specify it directly - to do so you'd have to resort to assembler expression trickery to generate an appropriate &quot;label&quot;.Logical instructions (and{s}, orr, eor, tst) take a &quot;bitmask immediate&quot;.  Memory Operand​  mov rdi, [sdi]   ","version":"Next","tagName":"h3"},{"title":"Instruction Encoding​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#instruction-encoding","content":" Assembler will encode the human-readable instruction into machine code.  In aarch64, the encoding instruction is fixed-size(4 bytes) machine code.In x86_64, the encoding instruction is non-fixed-size(up to 16 bytes) machine code.  Encoding Real x86 Instructions  Let's have a glimpse on the impact of the fixed/non-fixed encoding.  In order to load 32-bit integer, x86_64 need only one instruction while more instructions are needed for aarch64 to do that.  Load a 32-bit integer 0x1a2b3c4d in x86_64,  mov rid, 0x1a2b3c4d   Load a 32-bit integer 32-bit 0x1a2b3c4d in aarch64(the immediate value in mov must be in the range of 16-bit, so it needs two instructions),  movz x1, 0x3c4d movk x1, 0x1a2b, lsl 16   ","version":"Next","tagName":"h3"},{"title":"NASM x86_64 cheat sheet​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#nasm-x86_64-cheat-sheet","content":" NASM x86_64 cheat sheet  ","version":"Next","tagName":"h2"},{"title":"GAS aarch64 cheat sheet​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#gas-aarch64-cheat-sheet","content":" GAS aarch64 cheat sheet  ","version":"Next","tagName":"h2"},{"title":"Assembly's Role in Compiler​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#assemblys-role-in-compiler","content":" In the compiling process, a compiler such as GCC will translate C code into Assembly code for different CPU architectures, then use its corresponding Assembler to translate the Assembly code to the machine code which is CPU dependent.  The Assembly plays intermediate role in the compiler, while higher language like C sits upfront and machine code runs at the bottom.  I have another writing to introduce my understanding of the compiler from the practice more than the theoretical point of view, and how to write a compiler.  wiki-compiler.mdx  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Wiki Assembly","url":"/docs/wiki/wiki-assembly#resources","content":" Notes on x86-64 Assembly and Machine Code · GitHub  nasmtasutorial  x64_cheatsheet.pdf  mit x86-64 architecture guide  Assembly 1: Basics – CS 61 2018  CS107 Guide to x86-64  Guide to x86 Assembly  Assembly Language &amp; Computer Architecture Lecture (CS301)  The Hub of Heliopolis - Getting Started with x86-64 Assembly on Linux  Guide to x86 Assembly  Let's Write Some X86-64  pcasm-book.pdf  Sample 64-bit nasm programs ","version":"Next","tagName":"h2"},{"title":"Wiki Coral","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-coral","content":"","keywords":"wiki coral mendel","version":"Next"},{"title":"Resources​","type":1,"pageTitle":"Wiki Coral","url":"/docs/wiki/wiki-coral#resources","content":" Get started with the Dev Board | CoralCoralClose  ","version":"Next","tagName":"h2"},{"title":"Background​","type":1,"pageTitle":"Wiki Coral","url":"/docs/wiki/wiki-coral#background","content":" The official documents Get started with the Dev Board contains comprehensive how-to contents and rich examples. Here are just some experiences from myself. You can always go back to the official website to review and get the details.  The recommended method to access the Coral board is using Mendel Development Tool (mdt), which is required to be installed on your host machine alongside the Python. Common steps to enter the shell terminal from mdt are in following:  mdt tool generate a pair of SSH keys, save the private key on the host and push the public key to the Coral using http via 41337 port.Coral board has a running a mdt-keymaster server that is listening 41337 port, and put the public key into ~/.ssh/authorized_keys.mdt shell now can login to the shell terminal of Coral board like ssh mendel@192.168.100.2 when connecting over USB-C(OTG) or ssh mendel@indigo-quill.local over the same network where your host PC is.  info Coral board is set up by disabling password login in OpenSSH in default, so it must be provided with SSH key otherwise you change the setting to be like PasswordAuthentication yes.  note You can check the key master by, mendel@indigo-quill:~$ lsof -i:41337 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME mdt-keyma 7846 mendel 5u IPv4 20302 0t0 TCP 192.168.100.2:41337 (LISTEN) mdt-keyma 7847 mendel 6u IPv4 20619 0t0 TCP 192.168.101.2:41337 (LISTEN)   Although mdt maybe facilitate the access to the Coral board, some magics and additional steps are kept from sight.  To do not use mdt, we need access the dev board through serial console instead of mdt keymaster server, to make configuration.  There are general ways to access a just-setup Coral in brief steps:  Connect to Coral board's serial console by the instructions Connect to the Dev Board's serial consoleLog into the Dev board by username: mendel and password: mendel in default.Enable SSH Password Authentication. Edit /etc/ssh/sshd_config to change PasswordAuthentication no to PasswordAuthentication yes, and sudo service ssh restart to restart the ssh service.Log into the shell using username: mendel and password: mendel.If you want to keep the secure shell, generate private SSH key stored in host and public SSH key saved into Coral. ","version":"Next","tagName":"h2"},{"title":"Wiki Compiler","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-compiler","content":"","keywords":"Writing Compiler","version":"Next"},{"title":"Parser​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#parser","content":" BNF syntax is used to for computer to understand the expression, and is a critical concept to be followed to sequently parse the tokens to AST tree. Certainly, only one loop on the tokens is enough, which makes it very efficient.  BNF syntax for arithmetic operations grammar, including +, -, *, \\, and u-(unary -) .  expression : term | expression `+` expression | expression `-` expression term : factor | term `*` term | term `/` term factor : NUMBER | `(` expression `)` | `u-`factor | `u+`factor   BNF syntax for arithmetic operations and variable assignment.  expression : term | expression `+` expression | expression `-` expression term : factor | term `*` term | term `/` term | term `%` term factor : NUMBER | ID | `(` expression `)` | `u-`factor | `u+`factor | assignment assignment : ID `=` expression   In addition, BNF can also be applied to define regular expressions:  expression : term | term `|` term term : factor | term term factor : atom | atom `*` atom : CHAR | `(` expression `)` CHAR : any valid character except meta characters (e.g., &quot;*&quot;, &quot;|&quot;, &quot;(&quot;)   ","version":"Next","tagName":"h2"},{"title":"Difference between Compiler and Interpreter​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#difference-between-compiler-and-interpreter","content":" an interpreter also does lexer and parser jobs as a compiler does in step 1 and 2, but instead of generating low-level code, the interpreter generates the results directly.  ","version":"Next","tagName":"h2"},{"title":"Bootstrap a compiler​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#bootstrap-a-compiler","content":" A new programming language and a compiler written also in the new language is supposed to develop from an existing language. The progress is called bootstrapping, which can be summarized as,  C1 + L1 -&gt; C20 C20 + L2u -&gt; C21 C21 + L2 -&gt; C22 C22 + L2 -&gt; C23 C23 + L2 -&gt; C24   L1 : an existing languageC1 : an existing compiler for language L1C20: a compiler written in language L1 for language L2uC21: a compiler written in language L2u for language L2L2u: is subset of language L2  Bootstrapping stage:  Write a bootstrap compiler C20 to understand language L2u(a subset of language L2) in using existed language L1 and its corresponding compiler C1.Use the compiler C20 and language L2u to write the compiler C21 to understand language L2.Now C21 is a fully self-hosted compiler, as well as its descendants C22, C23, and C24.  ","version":"Next","tagName":"h2"},{"title":"Where did the existing compiler C1 come from?​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#where-did-the-existing-compiler-c1-come-from","content":" There is no need to use a compiler C1 + L1 if you write the bootstrap compiler C20 in machine code. This solves the chicken-and-egg problem totally for programming languages.  Bootstrapping initial compiler C20: A small and simple compiler is created manually in machine code or written in assembly language.[Option*] Translate the assembly language into machine code manually if it's not written in machine code.The initial compiler is just capable enough to understand a subset of the target language C it is supposed to compile. Use the initial compiler C20 to compile the compiler C21 written in language C while the C21 is also supposed to compile language C.Now compiler C21 a fully self-compilation.  Strange Loops: Ken Thompson and the Self-referencing C Compiler | ScienceBlogs  Bootstrapping (compilers) - Wikipedia  Compilers: Principles, Techniques, and Tools - Wikipedia  ","version":"Next","tagName":"h3"},{"title":"Implementations​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#implementations","content":" Interpreter:  GitHub - rswier/c4: C in four functionsGitHub - lotabout/write-a-C-interpreter: Write a simple interpreter of C. Inspired by c4 and largely based on it.  Self-hosted Compiler:  GitHub - DoctorWkt/acwj: A Compiler Writing JourneyGitHub - certik/bcompile: Bootstrapping a simple compiler from nothing  The basic knowledge of lexer and parser is critical and necessary for developing a programming language,  flex/lexyacc/parser  ","version":"Next","tagName":"h3"},{"title":"Compiler for a subset of C language bootstrapping from Python​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#compiler-for-a-subset-of-c-language-bootstrapping-from-python","content":" Recently, I am becoming interested in building a lexer, parser and code generator to try to create a mini language and deep insight of how GCC or Clang/LLVM do their jobs.  For educational purposes, learning in practice is my favorite approach to grasp an overview.  Let's do it!  Prerequisites:  Python for writing the bootstrap compiler  I use ply, a pure Python implementation of the lex and yacc tools to facilitate me to write the bootstrap compiler for the subset of C language.  ","version":"Next","tagName":"h3"},{"title":"Compiler for a subset of C language bootstrapping from C​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#compiler-for-a-subset-of-c-language-bootstrapping-from-c","content":" Prerequisites:  An existing GCC for writing the bootstrap compiler  Here are some popular tutorials from GitHub - DoctorWkt/acwj: A Compiler Writing Journey.  You can also refer GitHub - lotabout/write-a-C-interpreter although I prefer classifying it as interpreter not a complete compiler.  ","version":"Next","tagName":"h3"},{"title":"Compiler bootstrapping from assembly​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#compiler-bootstrapping-from-assembly","content":" ","version":"Next","tagName":"h3"},{"title":"Compiler bootstrapping from HEX​","type":1,"pageTitle":"Wiki Compiler","url":"/docs/wiki/wiki-compiler#compiler-bootstrapping-from-hex","content":" GitHub - certik/bcompile: Bootstrapping a simple compiler from nothing ","version":"Next","tagName":"h3"},{"title":"Wiki Cryptography","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-cryptography","content":"","keywords":"Wiki Cryptography","version":"Next"},{"title":"Asymmetric cryptography​","type":1,"pageTitle":"Wiki Cryptography","url":"/docs/wiki/wiki-cryptography#asymmetric-cryptography","content":" Also known as public-key cryptography  ","version":"Next","tagName":"h2"},{"title":"Symmetric cryptography​","type":1,"pageTitle":"Wiki Cryptography","url":"/docs/wiki/wiki-cryptography#symmetric-cryptography","content":" ","version":"Next","tagName":"h2"},{"title":"AES-128​","type":1,"pageTitle":"Wiki Cryptography","url":"/docs/wiki/wiki-cryptography#aes-128","content":" ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Wiki Cryptography","url":"/docs/wiki/wiki-cryptography#resources","content":"","version":"Next","tagName":"h2"},{"title":"Learn CMake","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-cmake","content":"","keywords":"cmake project structure","version":"Next"},{"title":"CMake Project Structure​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#cmake-project-structure","content":" A typical CMake project can be regarded to has three Tree:  Source Tree:  project_root ├── CMakeLists.txt ├── simple_example.cpp ├── simple_lib.cpp └── simple_lib.hpp   Build Tree:  project_root ├── CMakeLists.txt ├── simple_example.cpp ├── simple_lib.cpp ├── simple_lib.hpp └── build └── CMakeCache.txt   Install Tree:  This tree is located in the CMAKE_INSTALL_PREFIX, of which default value is platform-dependent. By default, it is set to /usr/local on Unix-like systems (Linux, macOS) and C:/Program Files/&lt;Project Name&gt; on Windows..  To change it, you can pass -DCMAKE_INSTALL_PREFIX argument during CMake configuration step, like this:  cmake -B build -S . -DCMAKE_INSTALL_PREFIX=/my/custom/installation/path   Alternatively, you can change it by passing --prefix(it can be relative path) argument during CMake install step, like this:  cmake --install build --prefix &quot;/my/custom/installation/path&quot;   It's recommended to use a default install layout as GNUInstallDirs.  A install tree will look like as below if you'd like all things to be installed inside the project via cmake --install build --prefix &quot;./install.  project_root ├── CMakeLists.txt ├── simple_example.cpp ├── simple_lib.cpp ├── simple_lib.hpp ├── build │ └── CMakeCache.txt └── install ├── bin │ └── executables ├── sbin │ └── sysadmin executables ├── lib │ ├── compiled libraries (*.so (unix) or *.dll (windows)) │ └── library archive files (*.lib (windows)) ├── libexec │ └── executables not directly invoked by user ├── include │ └── header files └── doc └── documentation   ","version":"Next","tagName":"h2"},{"title":"How CMake Works​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#how-cmake-works","content":" A typical workflow of CMake includes Configure, Build and Install steps, combined with the above mentioned Trees concepts.  Configure step will generate a sort of configuration files, the most important ones among them are CMakeCache.txt, cmake_install.cmake and Makefile if using Make as building system. With these generated configuration files, the later steps Build and Install will run according to them.Build step will generate the build binary directory.Install step will generate the install binary directory.  ","version":"Next","tagName":"h2"},{"title":"How to make your package be found by others by find_package()​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#how-to-make-your-package-be-found-by-others-by-find_package","content":" package configuration files: find_package  Title  ","version":"Next","tagName":"h2"},{"title":"RPATH in CMake​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#rpath-in-cmake","content":" 1  ","version":"Next","tagName":"h2"},{"title":"CMake Variables​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#cmake-variables","content":" There are some useful and important CMake variables that will be introduced here:  CMAKE_PREFIX_PATH  CMAKE_IGNORE_PATH  ","version":"Next","tagName":"h2"},{"title":"clang FAQ​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#clang-faq","content":" ","version":"Next","tagName":"h2"},{"title":"Find out clang include search path​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#find-out-clang-include-search-path","content":" ❯ clang -x c -v -E /dev/null ... #include &quot;...&quot; search starts here: #include &lt;...&gt; search starts here: /opt/homebrew/Cellar/llvm/17.0.1/lib/clang/17/include /Library/Developer/CommandLineTools/SDKs/MacOSX14.sdk/usr/include /Library/Developer/CommandLineTools/SDKs/MacOSX14.sdk/System/Library/Frameworks (framework directory) End of search list. # 1 &quot;/dev/null&quot; # 1 &quot;&lt;built-in&gt;&quot; 1 # 1 &quot;&lt;built-in&gt;&quot; 3 # 420 &quot;&lt;built-in&gt;&quot; 3 # 1 &quot;&lt;command line&gt;&quot; 1 # 1 &quot;&lt;built-in&gt;&quot; 2 # 1 &quot;/dev/null&quot; 2   ","version":"Next","tagName":"h3"},{"title":"Add include search path to clang​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#add-include-search-path-to-clang","content":" Use environment variables C_INCLUDE_PATH for c and CPLUS_INCLUDE_PATH for c++.  clang:  ❯ C_INCLUDE_PATH=/opt/homebrew/include clang -x c -v -E /dev/null ... #include &quot;...&quot; search starts here: #include &lt;...&gt; search starts here: /usr/local/include /opt/homebrew/include /Library/Developer/CommandLineTools/usr/lib/clang/15.0.0/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include /Library/Developer/CommandLineTools/usr/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks (framework directory)   clang++:  ❯ CPLUS_INCLUDE_PATH=/opt/homebrew/include clang -x c++ -v -E /dev/null ... #include &quot;...&quot; search starts here: #include &lt;...&gt; search starts here: /usr/local/include /opt/homebrew/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1 /Library/Developer/CommandLineTools/usr/lib/clang/15.0.0/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include /Library/Developer/CommandLineTools/usr/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks (framework directory)   Use -I flag,  ❯ clang -x c -I/opt/homebrew/include -v -E /dev/null ... #include &quot;...&quot; search starts here: #include &lt;...&gt; search starts here: /opt/homebrew/include /usr/local/include /Library/Developer/CommandLineTools/usr/lib/clang/15.0.0/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include /Library/Developer/CommandLineTools/usr/include /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks (framework directory)   ","version":"Next","tagName":"h3"},{"title":"Find out clang library search paths​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#find-out-clang-library-search-paths","content":" ❯ clang -Xlinker -v ... Library search paths: /usr/local/lib Framework search paths: ld: Undefined symbols: _main, referenced from: &lt;initial-undefines&gt; clang: error: linker command failed with exit code 1 (use -v to see invocation)   ","version":"Next","tagName":"h3"},{"title":"Add library search path to clang​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#add-library-search-path-to-clang","content":" Use environment variables LIBRARY_PATH,  ❯ LIBRARY_PATH=$LIBRARY_PATH:/usr/lib clang -Xlinker -v ... Library search paths: . /usr/lib /usr/local/lib Framework search paths: ld: Undefined symbols: _main, referenced from: &lt;initial-undefines&gt; clang: error: linker command failed with exit code 1 (use -v to see invocation)   Use -L flag:  ❯ clang -L/opt/homebrew/lib -Xlinker -v   https://langui.sh/2015/07/24/osx-clang-include-lib-search-paths/  ","version":"Next","tagName":"h3"},{"title":"What is the difference? clang++ | clang -std=c++11​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#what-is-the-difference-clang--clang--stdc11","content":" ","version":"Next","tagName":"h2"},{"title":"CMake FAQ​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#cmake-faq","content":" ","version":"Next","tagName":"h2"},{"title":"Add library search path to CMake globally in project​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#add-library-search-path-to-cmake-globally-in-project","content":" set(CMAKE_LIBRARY_PATH ${CMAKE_LIBRARY_PATH} /opt/local/lib)LINK_DIRECTORIES(/opt/local/lib)  ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Learn CMake","url":"/docs/wiki/wiki-cmake#resources","content":" CMake hands-on workshop — CMake Workshop    Footnotes​ RPATH handling from official cmake ↩ ","version":"Next","tagName":"h2"},{"title":"Wiki Cross Compilation","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-cross-compilation","content":"","keywords":"Wiki Cross Compilation","version":"Next"},{"title":"Cross Compilation Anatomy​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#cross-compilation-anatomy","content":" Cross-Compilation ecosystem involves the following components:  host system cross-Compilation toolchain cross compilercross linkercross debuggersysroot target system library filestarget system header filestarget system other files target system  Cross-Compilation toolchain:  GCCBuildrootYocto ProjectCrosstool-NGLinaroClang/LLVM  ","version":"Next","tagName":"h2"},{"title":"GCC​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#gcc","content":" Let's explore what a toolchain is like and what are needed to build something for a aarch64 platform on x86_64 debian-like host.  ","version":"Next","tagName":"h2"},{"title":"Obtaining a cross-compilation toolchain for aarch64​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#obtaining-a-cross-compilation-toolchain-for-aarch64","content":" For simplicity and in a super fast way, we will use a prebuilt and ready-on toolchain in x86_64 Ubuntu.  apt install gcc make gcc-aarch64-linux-gnu binutils-aarch64-linux-gnu   ","version":"Next","tagName":"h3"},{"title":"Where is cross compiler​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#where-is-cross-compiler","content":" We see cross compiler binary type in host is x86-64,  $ file /usr/bin/aarch64-linux-gnu-gcc-11 /usr/bin/aarch64-linux-gnu-gcc-11: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=b1112487d0dcb759db32e15b8f40f28a05484272, for GNU/Linux 3.2.0, stripped   ","version":"Next","tagName":"h3"},{"title":"Where is sysroot​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#where-is-sysroot","content":" The sysroot locates in /usr/aarch64-linux-gnu,  $ tree --filelimit=100 /usr/aarch64-linux-gnu /usr/aarch64-linux-gnu ├── bin │ ├── ar -&gt; ../../bin/aarch64-linux-gnu-ar │ ├── as -&gt; ../../bin/aarch64-linux-gnu-as │ ├── ld -&gt; ../../bin/aarch64-linux-gnu-ld │ ├── ld.bfd -&gt; ../../bin/aarch64-linux-gnu-ld.bfd │ ├── ld.gold -&gt; ../../bin/aarch64-linux-gnu-ld.gold │ ├── nm -&gt; ../../bin/aarch64-linux-gnu-nm │ ├── objcopy -&gt; ../../bin/aarch64-linux-gnu-objcopy │ ├── objdump -&gt; ../../bin/aarch64-linux-gnu-objdump │ ├── ranlib -&gt; ../../bin/aarch64-linux-gnu-ranlib │ ├── readelf -&gt; ../../bin/aarch64-linux-gnu-readelf │ └── strip -&gt; ../../bin/aarch64-linux-gnu-strip ├── include [139 entries exceeds filelimit, not opening dir] └── lib ├── Mcrt1.o ├── Scrt1.o ├── crt1.o ├── crti.o ├── crtn.o ├── gcrt1.o ├── grcrt1.o ├── ld-linux-aarch64.so.1   As you see, the binutils-aarch64-linux-gnu will install binutils tools in /usr/aarch64-linux-gnu/bin,  These binutils are also x86_64 binaries,   file $(readlink -f /usr/aarch64-linux-gnu/bin/ar) /usr/bin/aarch64-linux-gnu-ar: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=4f75b6dc6fe5ae92c78a51e6479ca2c65bbf5335, for GNU/Linux 3.2.0, stripped   While target libraries are aarch64 type,  file /usr/aarch64-linux-gnu/lib/crt1.o /usr/aarch64-linux-gnu/lib/crt1.o: ELF 64-bit LSB relocatable, ARM aarch64, version 1 (SYSV), for GNU/Linux 3.7.0, not stripped   ","version":"Next","tagName":"h3"},{"title":"Compile hello.c​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#compile-helloc","content":" $ aarch64-linux-gnu-gcc-11 hello.c -o a.out $ file a.out a.out: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=367c436db0697f16039d9249e4a4e809ef9e68b3, for GNU/Linux 3.7.0, not stripped   ","version":"Next","tagName":"h3"},{"title":"Clang/LLVM​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#clangllvm","content":" Cross compilation with Clang and LLVM tools  Cross compiling made easy, using Clang and LLVM · mcilloni's blog  apt install lld clang llvm   $ wget https://releases.linaro.org/components/toolchain/binaries/7.5-2019.12/aarch64-linux-gnu/sysroot-glibc-linaro-2.25-2019.12-aarch64-linux-gnu.tar.xz $ tar -xvf sysroot-glibc-linaro-2.25-2019.12-aarch64-linux-gnu.tar.xz $ mv sysroot-glibc-linaro-2.25-2019.12-aarch64-linux-gnu aarch64-linux-gnu   $ ll aarch64-linux-gnu total 20K drwxr-xr-x 2 11827 9000 4.0K Dec 4 2019 etc drwxr-xr-x 3 11827 9000 4.0K Dec 4 2019 lib drwxr-xr-x 2 11827 9000 4.0K Dec 4 2019 sbin drwxr-xr-x 8 11827 9000 4.0K Dec 4 2019 usr drwxr-xr-x 3 11827 9000 4.0K Dec 4 2019 var   $ cat &gt; hello.c &lt;&lt; EOL #include &lt;stdio.h&gt; int main(int argc, char *argv[]) { printf(&quot;Hello cross-compilation world!\\n&quot;); return 0; } EOL   sysroot=~/Documents/sysroot/aarch64-linux-gnu/usr   clang --target=aarch64-linux-gnu hello.c -o hello_aarch64 -v   clang --target=aarch64-linux-gnu hello.c -o hello_aarch64 --sysroot=$sysroot -v   clang --target=aarch64-linux-gnu -fsanitize=undefined \\ -fuse-ld=lld \\ --rtlib=compiler-rt -stdlib=libc++ \\ -nostdinc++ -nostdlib \\ -I${sysroot}/usr/include/ \\ -Wl,-L${sysroot}/usr/lib \\ --sysroot=$sysroot \\ --verbose \\ hello.c -o hello   ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Wiki Cross Compilation","url":"/docs/wiki/wiki-cross-compilation#resources","content":" https://wiki.osdev.org/GCC_Cross-Compiler  https://github.com/generia/buildroot-osx  https://crosstool-ng.github.io/docs/  https://github.com/messense/homebrew-macos-cross-toolchains/blob/main/.github/workflows/aarch64.yml ","version":"Next","tagName":"h2"},{"title":"Cyber Security Wiki","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-cybersecurity","content":"","keywords":"Wiki Cybersecurity","version":"Next"},{"title":"Resources​","type":1,"pageTitle":"Cyber Security Wiki","url":"/docs/wiki/wiki-cybersecurity#resources","content":"","version":"Next","tagName":"h2"},{"title":"Diagram Wiki","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-diagram","content":"","keywords":"Wiki Diagram","version":"Next"},{"title":"Diagram tools​","type":1,"pageTitle":"Diagram Wiki","url":"/docs/wiki/wiki-diagram#diagram-tools","content":" ","version":"Next","tagName":"h2"},{"title":"Graphic-based diagram tools​","type":1,"pageTitle":"Diagram Wiki","url":"/docs/wiki/wiki-diagram#graphic-based-diagram-tools","content":" Features:  Drag-and-drop functionality.A large library of shapes and icons.Complex diagram.  Including:  Drawio Lucidchart Miro  ","version":"Next","tagName":"h3"},{"title":"Text-based diagram tools​","type":1,"pageTitle":"Diagram Wiki","url":"/docs/wiki/wiki-diagram#text-based-diagram-tools","content":" Features:  Integration with text editors.Version control.Less flexible in terms of user interface and graphical capabilities.  Including:  Mermaid Embedded in Markdown or web pages easily.Client-side rendering, based on JavaScript. Plantuml Client-side rendering, which needs Java runtime and plantuml.jar, and graphviz optionally for some diagram.Or Server-side rendering, also needs the above libraries. Asciiflow Represent diagrams in pure ASCII TEXT, without any rendering.  ","version":"Next","tagName":"h3"},{"title":"Database schema-based diagram tools​","type":1,"pageTitle":"Diagram Wiki","url":"/docs/wiki/wiki-diagram#database-schema-based-diagram-tools","content":" Features:  Generate diagrams directly from database schema.  Including:  ERD Editor | VSCode Extension bigER Modeling Tool | VSCode Extension MySQL Workbench DbVisualizer ER/Studio  ","version":"Next","tagName":"h3"},{"title":"Creating diagrams on GitHub​","type":1,"pageTitle":"Diagram Wiki","url":"/docs/wiki/wiki-diagram#creating-diagrams-on-github","content":" You can create diagrams in Markdown using three different syntaxes: mermaid, geoJSON and topoJSON, and ASCII STL, said by GitHub official. ","version":"Next","tagName":"h3"},{"title":"Wiki Development Environment","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-dev-environment","content":"","keywords":"Wiki Dev Environment","version":"Next"},{"title":"VS Code​","type":1,"pageTitle":"Wiki Development Environment","url":"/docs/wiki/wiki-dev-environment#vs-code","content":" As of now, the VS Code is the most popular IDE among developers. Absolutely, for me, it's my first choice and favorite developing tool.  ","version":"Next","tagName":"h2"},{"title":"Dev Container​","type":1,"pageTitle":"Wiki Development Environment","url":"/docs/wiki/wiki-dev-environment#dev-container","content":" Developing inside a Container using Visual Studio Code Remote Development  What is Dev Container?  A &quot;Dev Container&quot; typically refers to a development environment that is containerized. Containers are lightweight, portable, and consistent environments that encapsulate an application and its dependencies. They provide a standardized way to package and run software across different environments, ensuring that the application behaves consistently regardless of where it is deployed.  The benefits of using Dev Containers include:  Consistency: Developers work in the same environment, reducing the chances of environment-related issues.Isolation: Dev Containers are isolated from the host system, preventing conflicts with other software installed on the developer's machine.Reproducibility: The development environment can be easily recreated by anyone using the container specifications.Portability: Dev Containers can be easily shared, allowing developers to work on the same project with minimal setup.  Best ways to customize the environment in Dev containers?  Using Images, Dockerfile, and Docker Compose:love_you_gesture:Using FeaturesUsing lifecycle scripts  How to Write Dockerfile for Dev containers?  You can refer to this repo GitHub - devcontainers/images: Repository for pre-built dev container images published under mcr.microsoft.com/devcontainers  What magics does the Dev Containers extension do when starting?  Hook a default startup command while sleep 1000; do :; done to keep the container not exit. Disable this behavior by setting overrideCommand: false.  How to run a container inside of Dev containers?  Docker-in-DockerDocker-from-Docker  For instance, I use the Docker-in-Docker method to always test/run Docker containers inside of Dev containers.  ","version":"Next","tagName":"h3"},{"title":"C/C++​","type":1,"pageTitle":"Wiki Development Environment","url":"/docs/wiki/wiki-dev-environment#cc","content":" ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Wiki Development Environment","url":"/docs/wiki/wiki-dev-environment#resources","content":"","version":"Next","tagName":"h2"},{"title":"Wiki Network","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-network","content":"","keywords":"Wiki Proxy","version":"Next"},{"title":"TCP handshake​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#tcp-handshake","content":" ","version":"Next","tagName":"h2"},{"title":"TLS handshake​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#tls-handshake","content":" https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/  ","version":"Next","tagName":"h2"},{"title":"Proxy vs Reverse Proxy​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#proxy-vs-reverse-proxy","content":" Certainly! Let's explore the key differences between a proxy and a reverse proxy:  Proxy (Forward Proxy):  Acts as an intermediary between clients (users or devices) and the internet.Forwards client requests to the internet resource (e.g., a website).Provides: client anonymitycachingtraffic controlrequest/response transformation. Commonly used for bypassing content filters and accessing restricted content.Examples include: SquidProxyTorCharles ProxyHTTP ToolkitMITM ProxyFiddler Proxy  Reverse Proxy:  Sits in front of one or more web servers. Forwards server responses to clients (users or devices).Offers benefits like:server anonymityload balancingDDoS protectionURL/content rewriting. Used for improving server performance and enhancing security.Examples include: NginxTraefik  In summary, a proxy handles client traffic, while a reverse proxy shields servers by managing requests and responses.  ","version":"Next","tagName":"h2"},{"title":"Proxy​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#proxy","content":" Man-in-the-middle(MitM)  https://httptoolkit.com/docs/guides/android/  https://docs.mitmproxy.org/stable/concepts-howmitmproxyworks/  ","version":"Next","tagName":"h2"},{"title":"How the proxy intercepts the HTTP traffic?​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#how-the-proxy-intercepts-the-http-traffic","content":" ","version":"Next","tagName":"h3"},{"title":"Install SSL cert in android emulator​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#install-ssl-cert-in-android-emulator","content":" ","version":"Next","tagName":"h3"},{"title":"Socks Proxy​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#socks-proxy","content":" ","version":"Next","tagName":"h3"},{"title":"Reverse Proxy​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#reverse-proxy","content":" ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Wiki Network","url":"/docs/wiki/wiki-network#resources","content":"","version":"Next","tagName":"h2"},{"title":"Wiki NVIDIA Driver and CUDA Library","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-cuda","content":"","keywords":"wiki cuda","version":"Next"},{"title":"NVIDIA Driver on Ubuntu​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#nvidia-driver-on-ubuntu","content":" ","version":"Next","tagName":"h2"},{"title":"Find out whether the host machine have NVIDIA GPU hardware​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#find-out-whether-the-host-machine-have-nvidia-gpu-hardware","content":" $ lspci | grep VGA 0000:ac:00.0 VGA compatible controller: NVIDIA Corporation Device 2233 (rev a1)   or,  $ sudo lshw -C display *-display description: VGA compatible controller product: NVIDIA Corporation vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:ac:00.0 logical name: /dev/fb0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list rom fb configuration: depth=32 driver=nouveau latency=0 resolution=1920,1080 resources: iomemory:204f0-204ef iomemory:204f0-204ef irq:68 memory:99000000-99ffffff memory:204fe0000000-204fefffffff memory:204ff0000000-204ff1ffffff ioport:3000(size=128) memory:9a080000-9a0fffff   or,  $ hwinfo --gfxcard --short graphics card: nVidia VGA compatible controller Primary display adapter: #94   ","version":"Next","tagName":"h3"},{"title":"Check which NVIDIA driver being used​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#check-which-nvidia-driver-being-used","content":" Ubuntu is using open-source Nouveau drivers  $ lsmod | grep nouveau nouveau 2306048 1 mxm_wmi 16384 1 nouveau i2c_algo_bit 16384 1 nouveau drm_ttm_helper 16384 1 nouveau ttm 86016 2 drm_ttm_helper,nouveau drm_kms_helper 311296 1 nouveau drm 622592 5 drm_kms_helper,drm_ttm_helper,ttm,nouveau video 65536 2 dell_wmi,nouveau wmi 32768 7 dell_wmi_sysman,dell_wmi,wmi_bmof,dell_smbios,dell_wmi_descriptor,mxm_wmi,nouveau   Ubuntu is not using the proprietary NVIDIA drivers  $ lsmod | grep nvidia   ","version":"Next","tagName":"h3"},{"title":"Install the NVIDIA driver​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#install-the-nvidia-driver","content":" Ubuntu Linux Install Nvidia Driver (Latest Proprietary Driver)  Install Nvidia Beta Drivers via PPA Repository  ","version":"Next","tagName":"h3"},{"title":"Verify the NVIDIA driver​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#verify-the-nvidia-driver","content":" nvidia-smi nvidia-smi --query-gpu=driver_version --format=csv   dconfig -p | grep nvidia   ","version":"Next","tagName":"h3"},{"title":"Reload NVIDIA driver​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#reload-nvidia-driver","content":" Get related drivers,  lsmod | grep nvidia   Unload drivers,  sudo rmmod nvidia_drm sudo rmmod nvidia_modeset sudo rmmod nvidia_uvm   sudo rmmod nvidia   Load NVIDIA driver again,  nvidia-smi   cuda - Nvidia NVML Driver/library version mismatch - Stack Overflow  Prevent updating NVIDIA driver,  sudo apt-mark hold nvidia-driver-535 sudo apt-mark hold nvidia-dkms-535 sudo apt-mark hold nvidia-utils-535   updates - How to prevent updating of a specific package? - Ask Ubuntu  ","version":"Next","tagName":"h3"},{"title":"NVIDIA CUDA Toolkit on WSL​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#nvidia-cuda-toolkit-on-wsl","content":" NVIDIA CUDA software stack on WSL 2:    ","version":"Next","tagName":"h2"},{"title":"NVIDIA CUDA Toolkit on Ubuntu​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#nvidia-cuda-toolkit-on-ubuntu","content":" Official documentation: CUDA installation  How to Install CUDA on Ubuntu 22.04 LTS  ","version":"Next","tagName":"h2"},{"title":"NVIDIA Container Toolkit​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#nvidia-container-toolkit","content":" Build and run containers leveraging NVIDIA GPUs, already including CUDA Toolkit.      Prerequisites on Host Machine:  Nvidia GPU driverNvidia CUDA Container ToolkitDocker  Running a docker container ubuntu,  Specialized Configurations with Docker — container-toolkit 1.14.1 documentation  $ docker run --rm --gpus all ubuntu nvidia-smi $ docker run --rm --gpus all ubuntu ldconfig -p | grep nvidia libnvidia-ptxjitcompiler.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1 libnvidia-pkcs11-openssl3.so.535.86.05 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-pkcs11-openssl3.so.535.86.05 libnvidia-opencl.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1 libnvidia-nvvm.so.4 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so.4 libnvidia-ml.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1 libnvidia-cfg.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.1 libnvidia-allocator.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.1 $ docker run --rm -e NVIDIA_DRIVER_CAPABILITIES=video --gpus all ubuntu ldconfig -p | grep nvidia libnvidia-ptxjitcompiler.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1 libnvidia-pkcs11-openssl3.so.535.86.05 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-pkcs11-openssl3.so.535.86.05 libnvidia-opticalflow.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-opticalflow.so.1 libnvidia-opencl.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1 libnvidia-nvvm.so.4 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so.4 libnvidia-encode.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-encode.so.1 libnvidia-allocator.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.1   Running a docker container nvidia/cuda,  info Make sure the version of CUDA container nvidia/cuda:xx.x.x-base-ubuntu22.04 such as 12.2.0 in following must be compatible with the version of the Nvidia GPU driver on the host platform such as &gt;525.60.13.  docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi   docker run --rm \\ --gpus all \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,video,utility,graphics \\ nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi   More Dockerfile examples:  nvidia-cuda-ffmpeg/Dockerfile loading... View on GitHub  nvidia-cuda/docker-compose.yml loading... View on GitHub  Dockerfiledocker compose  ","version":"Next","tagName":"h2"},{"title":"FFmpeg in NVIDIA CUDA container​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#ffmpeg-in-nvidia-cuda-container","content":" https://developer.nvidia.com/ffmpeg  https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html#compiling-for-linux  https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/  Install FFmpeg on Nvidia CUDA Container  ","version":"Next","tagName":"h2"},{"title":"Known issues​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#known-issues","content":" After random long running time, in Nvidia docker the FFmpeg encoding stops and error comes out: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected. [Issue]: NVidia Docker transcoding randomly stops working after 5 minutes to 4 hours later. · Issue #9287 · jellyfin/jellyfin · GitHub  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Wiki NVIDIA Driver and CUDA Library","url":"/docs/wiki/wiki-cuda#references","content":" CUDA And Nvidia Graphics Driver  CUDA on WSL User Guide ","version":"Next","tagName":"h2"},{"title":"Wiki Skia","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-skia","content":"Wiki Skia What the difference between SkImage/SkPicture/SkCanvas/SkSurface? SkBitmap based SkCanvas very slow... How to improve draw speeds? How to move SkImage from CPU to GPU? How to control the SkImage GPU back cache size? As far as I understand when I load SkImage from file or SkBitmap the SkImage lives in CPU side memory. Then the moment I draw this SkImage on a GPU backed canvas it will make a copy of the CPU data into a GPU backed texture. So now we technically have two copies available on the SkImage. Then each time I draw that SkImage it will do it quickly cause it's already in the GPU side.","keywords":"","version":"Next"},{"title":"Wiki Samba","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-samba","content":"","keywords":"Wiki Samba","version":"Next"},{"title":"Setting up Samba​","type":1,"pageTitle":"Wiki Samba","url":"/docs/wiki/wiki-samba#setting-up-samba","content":" How to Install Samba in Ubuntu  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting Samba​","type":1,"pageTitle":"Wiki Samba","url":"/docs/wiki/wiki-samba#troubleshooting-samba","content":" Troubleshooting Access Denied on SAMBA - Ask Ubuntu  /etc/samba.conf [documents] path = /data/documents valid users = @simon guest ok = no writable = yes browsable = yes  ","version":"Next","tagName":"h2"},{"title":"Wiki Socket","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-socket","content":"","keywords":"Wiki Socket","version":"Next"},{"title":"Resources​","type":1,"pageTitle":"Wiki Socket","url":"/docs/wiki/wiki-socket#resources","content":" Transports and Protocols — Python 3.12.1 documentation  selectors — High-level I/O multiplexing — Python 3.12.1 documentation  socket — Low-level networking interface — Python 3.12.1 documentation ","version":"Next","tagName":"h2"},{"title":"Wiki vcpkg","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-vcpkg","content":"","keywords":"docs docusaurus","version":"Next"},{"title":"Classic mode​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#classic-mode","content":" Official saying: In Classic mode, vcpkg maintains a central installed tree inside the vcpkg instance built up by individual vcpkg install and vcpkg remove commands. This central set of packages can then be shared by any number of projects.  All packages are installed in a common %VCPKG_ROOT%/installed directory.  ","version":"Next","tagName":"h2"},{"title":"Classic mode how-to​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#classic-mode-how-to","content":" Just run vcpkg install %package% to use classic mode as the package will be installed into %VCPKG_ROOT%/installed/.  ","version":"Next","tagName":"h3"},{"title":"Manifest mode​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#manifest-mode","content":" Official saying: In Manifest mode, vcpkg creates separate installed trees for each project and configuration. This allows separate projects to use different versions of libraries. The vcpkg.json file and optional vcpkg-configuration.json file form a project's manifest. The manifest declares the project's direct dependencies, version constraints, and registries used.  All packages are installed in their own ${project}/vcpkg_installed directory inside the ${project} directory.  ","version":"Next","tagName":"h2"},{"title":"Manifest mode how-to​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#manifest-mode-how-to","content":" Create vcpkg.json in the project, then run vcpkg install to use manifest mode as all the packages declared in vcpkg.json will be installed into ${project}/vcpkg_installed/.  ","version":"Next","tagName":"h3"},{"title":"Useful environment variables for development​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#useful-environment-variables-for-development","content":" CURRENT_INSTALLED_DIRCURRENT_PACKAGES_DIR  set(VCPKG_RELEASE_LIBDIR &quot;${CURRENT_INSTALLED_DIR}/lib&quot;) set(VCPKG_DEBUG_LIBDIR &quot;${CURRENT_INSTALLED_DIR}/debug/lib&quot;) set(VCPKG_TOOLS_DIR &quot;${CURRENT_INSTALLED_DIR}/tools&quot;) set(VCPKG_SHARE_DIR &quot;${CURRENT_INSTALLED_DIR}/share&quot;) set(VCPKG_INCLUDE_DIR &quot;${CURRENT_INSTALLED_DIR}/include&quot;)   ","version":"Next","tagName":"h2"},{"title":"Tips and Tricks​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#tips-and-tricks","content":" ","version":"Next","tagName":"h2"},{"title":"How to specify a compiler fof vcpkg install?​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#how-to-specify-a-compiler-fof-vcpkg-install","content":" As vcpkg just use cmake toolchain to do install, how to set a specific compile is cmake things.  A fast way is use CC and CXX environment variables.  export CC=gcc-4.2 export CXX=/usr/bin/g++-4.2   See more ways at iar - How to specify a compiler in CMake? - Stack Overflow  ","version":"Next","tagName":"h3"},{"title":"Install *-osx-dynamic​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#install--osx-dynamic","content":" vcpkg install libpq --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic   ","version":"Next","tagName":"h3"},{"title":"Reinstall packages without caching​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#reinstall-packages-without-caching","content":" vcpkg remove icu --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic vcpkg install icu --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --no-binarycaching vcpkg install libpq --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --binarysource=clear vcpkg remove libpq --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic vcpkg remove &quot;qtbase[gui,widgets]&quot; --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic vcpkg install &quot;qtbase[gui,widgets]&quot; --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --no-binarycaching vcpkg install &quot;qtbase[gui,widgets]&quot; --host-triplet=arm64-osx-dynamic --triplet=arm64-osx-dynamic --binarysource=clear   ","version":"Next","tagName":"h3"},{"title":"Clean up all packages​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#clean-up-all-packages","content":" rm -rf /opt/vcpkg/installed/ rm -rf /opt/vcpkg/packages/ rm -rf /opt/vcpkg/buildtrees/   ","version":"Next","tagName":"h3"},{"title":"Clean up all caching packages​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#clean-up-all-caching-packages","content":" rm -rf ~/.cache/vcpkg/archives/   ","version":"Next","tagName":"h3"},{"title":"INSTALL_RPATH_USE_LINK_PATH different behaviours in manifest mode and classic mode​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#install_rpath_use_link_path-different-behaviours-in-manifest-mode-and-classic-mode","content":" INSTALL_RPATH_USE_LINK_PATH will not work properly when being used in the manifest mode, because CMake will don't handle libraries located in buildtree:  set_target_properties(${PROJECT_NAME} PROPERTIES INSTALL_RPATH &quot;@executable_path/../Frameworks&quot; INSTALL_RPATH_USE_LINK_PATH ON )   After ${PROJECT_NAME} installed, in the manifest mode:  ❯ otoolll /Users/frankchen/Documents/vcpkg-qt-app/install/./helloworld.app/Contents/MacOS/helloworld cmd LC_RPATH cmdsize 48 path @executable_path/../Frameworks (offset 12)   After ${PROJECT_NAME} installed, in the classic mode:  ❯ otoolll /Users/frankchen/Documents/vcpkg-qt-app/install/./helloworld.app/Contents/MacOS/helloworld cmd LC_RPATH cmdsize 56 path /opt/vcpkg/installed/arm64-osx-dynamic/lib (offset 12) Load command 27 cmd LC_FUNCTION_STARTS -- cmd LC_RPATH cmdsize 48 path @executable_path/../Frameworks (offset 12)   [wiki-cmake.mdx#RPATH in CMake](./wiki-cmake.mdx#RPATH in CMake)  ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Wiki vcpkg","url":"/docs/wiki/wiki-vcpkg#resources","content":" TODO: Fix qtbase tools/config in release/debug osx  https://github.com/microsoft/vcpkg/tree/master/ports/qtbase  https://learn.microsoft.com/en-us/vcpkg/maintainers/functions/vcpkg_cmake_config_fixup ","version":"Next","tagName":"h2"},{"title":"Wiki VPN","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-vpn","content":"","keywords":"vpn","version":"Next"},{"title":"Routers Support for VPN(OpenVPN) Client​","type":1,"pageTitle":"Wiki VPN","url":"/docs/wiki/wiki-vpn#routers-support-for-vpnopenvpn-client","content":" How to set up a router with Surfshark? – Surfshark Customer Support  Routers Supporting VPN Client - Home Network Community  ","version":"Next","tagName":"h2"},{"title":"Kill Switch​","type":1,"pageTitle":"Wiki VPN","url":"/docs/wiki/wiki-vpn#kill-switch","content":" KillSwitch could be used to block outgoing traffic when the VPN connection drops and crashes.  ","version":"Next","tagName":"h2"},{"title":"PF(packet filter) MacOS​","type":1,"pageTitle":"Wiki VPN","url":"/docs/wiki/wiki-vpn#pfpacket-filter-macos","content":" Setting up correctly Packet Filter (pf) firewall on any macOS  Prevent outgoing traffic unless OpenVPN connection is active using pf.conf on Mac OS X  Quick and easy pf (packet filter) firewall rules on macOS  A Cheat Sheet For Using pf in OS X Lion and Up  OS X PF Manual  ","version":"Next","tagName":"h2"},{"title":"Set Up Firewall to Allow Access Only via VPN(KillSwitch)​","type":1,"pageTitle":"Wiki VPN","url":"/docs/wiki/wiki-vpn#set-up-firewall-to-allow-access-only-via-vpnkillswitch","content":" ENABLING VPN-ONLY ACCESS TO THE INTERNET WITH WINDOWS FIREWALL  KillSwitch for macOS  Prevent outgoing traffic unless OpenVPN connection is active using pf.conf on Mac OS X ","version":"Next","tagName":"h2"},{"title":"Wiki QEMU","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-qemu","content":"","keywords":"Wiki QEMU","version":"Next"},{"title":"OS image Resources​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#os-image-resources","content":" Ubuntu OS ImagesDebian OS ImagesRaspberry PI OS Images  ","version":"Next","tagName":"h2"},{"title":"QEMU Keyboard shortcuts​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#qemu-keyboard-shortcuts","content":" Switch between QEMU monitor console and the guest non-graphic OS CTRL+a c Exit the guest non-graphic OS CTRL+a x Switch between QEMU monitor console and the guest graphic OS CTRL+ALT+1, CTRL+ALT+2  ","version":"Next","tagName":"h3"},{"title":"Discover the VM device tree​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#discover-the-vm-device-tree","content":" Enter the QEMU monitor console, using info qtree command,  $ info qtree dev: gpex-pcihost, id &quot;&quot; ... bus: pcie.0 type PCIE dev: virtio-scsi-pci, id &quot;&quot; ... bus: virtio-bus type virtio-pci-bus dev: virtio-scsi-device, id &quot;&quot; ... bus: scsi.0 type SCSI dev: scsi-hd, id &quot;&quot; drive = &quot;hd&quot; ... dev: nvme, id &quot;&quot; drive = &quot;drive0&quot; ... bus: nvme-bus.0 type nvme-bus dev: virtio-net-pci, id &quot;&quot; ... bus: virtio-bus type virtio-pci-bus dev: virtio-net-device, id &quot;&quot; ...   ","version":"Next","tagName":"h3"},{"title":"List supported devices​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#list-supported-devices","content":" $ qemu-system-aarch64 -device help $ qemu-system-aarch64 -device scsi-hd,help   ","version":"Next","tagName":"h3"},{"title":"Create disk image​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#create-disk-image","content":" qemu-img create -f raw ubuntu.raw 20G qemu-img create -f qcow2 ubuntu.qcow2 20G   QEMU can boot from 3 ways:  BIOS in defaultLinux kernel and initradUEFI  For UEFI boot, the -bios option should be used alongside UEFI firmware(OVMF.fd file) being provided to help QEMU do UEFI boot. For instance it is like: -bios OVMF.fd.  Get a prebuilt OVMF file from the OVMF.  ","version":"Next","tagName":"h2"},{"title":"BIOS boot​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#bios-boot","content":" Test entering BIOS,  qemu-system-x86_64 -monitor stdio -m 1G   Then QEMU will show like this,    ","version":"Next","tagName":"h2"},{"title":"Kernel boot​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#kernel-boot","content":" ","version":"Next","tagName":"h2"},{"title":"UEFI boot​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#uefi-boot","content":" ","version":"Next","tagName":"h2"},{"title":"Test UEFI boot​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#test-uefi-boot","content":" aarch64,  efi=&quot;$PWD/UEFI/aarch64/QEMU_EFI.fd&quot; qemu-system-aarch64 -monitor stdio -M virt -cpu cortex-a57 -m 1G -net none -bios $efi qemu-system-aarch64 -nographic -M virt -cpu cortex-a57 -m 1G -net none -bios $efi   x86_64,  efi=&quot;$PWD/UEFI/ovmf-x64/OVMF-pure-efi.fd&quot; qemu-system-x86_64 -monitor stdio -m 1G -net none -bios $efi   Then QEMU will drop into the UEFI shell, like this following image show,    Options in detail:  -nographic: Don't create a video for the VM, just use the terminal.  info quit QEMU: Ctrl+A X. enter QEMU monitor console: Ctrl+A C. see at How to quit the QEMU monitor when not using a GUI?  -monitor stdio: Put QEMU monitor console in the terminal, while guest OS kept in created video device.  info switch between monitor console and guest OS: Ctrl+Alt+1 or Ctrl+Alt+2.  -net none: Disable iPXE.  ","version":"Next","tagName":"h3"},{"title":"Boot x86_64 ISO image​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#boot-x86_64-iso-image","content":" Boot x86_64 image in Windows,  efi=&quot;$PWD/UEFI/ovmf-x64/OVMF-pure-efi.fd&quot; iso=ubuntu-22.04-live-server-amd64.iso   note ubuntu-**-amd64.iso support both UEFI and Legacy BIOS boot, QEMU use BIOS when the option -bios is not specified!  Create a disk image to install the ubuntu OS,  qemu-img create -f qcow2 ubuntu-image.qcow2 20G   Boot to run the Ubuntu OS  qemu-system-x86_64 \\ -monitor stdio \\ -accel whpx \\ -m 8G \\ -smp 4 \\ -drive file=ubuntu-image.qcow2 \\ -bios $efi \\ -cdrom $iso   Options in details,  -accel whpx: use hardware acceleration  [?]Boot the installed Ubuntu OS  # Install OS into a disk image qemu-system-x86_64 \\ -accel whpx \\ -m 8G \\ -smp 4 \\ -bios $efi \\ -drive file=ubuntu.qcow2,format=qcow2,if=virtio \\   ","version":"Next","tagName":"h3"},{"title":"Boot aarch64 ISO image​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#boot-aarch64-iso-image","content":" Emulate aarch64 ISO image in Windows,  efi=&quot;$PWD/UEFI/aarch64/QEMU_EFI.fd&quot; iso=&quot;ubuntu-22.04-live-server-arm64.iso&quot; qemu-system-aarch64 \\ -monitor stdio \\ -machine virt \\ -cpu cortex-a57 \\ -m 4G \\ -smp 4 \\ -drive file=ubuntu.qcow2,format=raw,if=virtio \\ -bios $efi \\ -cdrom $iso   Emulate aarch64 ISO image in mac M1,  qemu-system-aarch64 \\ -monitor stdio \\ -machine virt \\ -accel hvf \\ -cpu host \\ -m 4G \\ -smp 4 \\ -drive file=ubuntu.qcow2,format=raw,if=virtio \\ -bios $efi \\ -cdrom $iso   Options in details,  -accel hvf: use hardware acceleration in mac M1.-cpu host: use mac M1 arm CPU.  ","version":"Next","tagName":"h3"},{"title":"Boot a preinstalled image​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#boot-a-preinstalled-image","content":" # linux fdisk -l ubuntu-core-22-arm64+raspi.img # osx hdiutil imageinfo ubuntu-core-22-arm64+raspi.img   kernel=&quot;$PWD/TinyCore/boot/vmlinuz64&quot; initrd=$&quot;$PWD/TinyCore/boot/corepure64.gz&quot; img=$&quot;$PWD/TinyCorePure64-14.0.iso&quot; efi=&quot;$PWD/UEFI/ovmf-x64/OVMF-pure-efi.fd&quot; kernel=&quot;$PWD/linux_qemu/x86_64/bzImage&quot; vmlinuz=&quot;$PWD/linux_qemu/x86_64/vmlinux&quot; initrd=&quot;$PWD/linux_qemu/x86_64/rootfs.ext2&quot; img=&quot;$PWD/linux_qemu/x86_64/rootfs.ext2&quot;   qemu-system-x86_64 \\ -nographic \\ -m 4G \\ -kernel $kernel \\ -initrd $img \\ -append &quot;console=ttyS0&quot; \\ -netdev user,id=mynet,hostfwd=tcp::2222-:22 \\ -device virtio-net-pci,netdev=mynet   ","version":"Next","tagName":"h3"},{"title":"Boot linux kernel​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#boot-linux-kernel","content":" ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Wiki QEMU","url":"/docs/wiki/wiki-qemu#resources","content":" UEFI, PC boot process and UEFI with QEMU | joonas.fi  https://medium.com/@ThyCrow/compiling-the-linux-kernel-and-creating-a-bootable-iso-from-it-6afb8d23ba22  https://levelup.gitconnected.com/probably-the-simplest-way-to-install-debian-ubuntu-in-qemu-2db6afde27ef  UEFI on AARCH64 | Welcome to the Mike’s homepage!  OVMF · tianocore/tianocore.github.io Wiki · GitHub  https://wiki.debian.org/Arm64Qemu  http://cdn.kernel.org/pub/linux/kernel/people/will/docs/qemu/qemu-arm64-howto.html  https://futurewei-cloud.github.io/ARM-Datacenter/qemu/how-to-launch-aarch64-vm/  https://xryan.net/p/212 ","version":"Next","tagName":"h2"},{"title":"Wiki WPF","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-wpf","content":"","keywords":"Wiki WPF","version":"Next"},{"title":"MVVM​","type":1,"pageTitle":"Wiki WPF","url":"/docs/wiki/wiki-wpf#mvvm","content":" MVVM Pattern Made Simple - CodeProject  MVVM in Depth - CodeProject  My attempt to understand MVVM pattern and questions raised during it : csharp  Patterns - WPF Apps With The Model-View-ViewModel Design Pattern | Microsoft Docs  Introduction to the MVVM Toolkit - Windows Community Toolkit | Microsoft Docs  ","version":"Next","tagName":"h2"},{"title":"Features​","type":1,"pageTitle":"Wiki WPF","url":"/docs/wiki/wiki-wpf#features","content":" IoC, Inversion of ControlDI, Dependency InjectionNavigationViewModel-to-ViewModel Communication MVVM Light MessengerEvent Aggregator | PrismReactiveUI - Message Bus Observable Object in ViewModel Wrapping a non-observable model // https://docs.microsoft.com/en-us/windows/communitytoolkit/mvvm/observableobject#wrapping-a-non-observable-model public class ObservableUser : ObservableObject { private readonly User user;mvvm-application.png public ObservableUser(User user) =&gt; this.user = user; public string Name { get =&gt; user.Name; set =&gt; SetProperty(user.Name, value, user, (u, n) =&gt; u.Name = n); } }   ","version":"Next","tagName":"h2"},{"title":"Principles​","type":1,"pageTitle":"Wiki WPF","url":"/docs/wiki/wiki-wpf#principles","content":"   View-to-ViewModel one-to-one/many-to-one mappingViewModel-to-ViewModel communicationViewModel-to-Model one-to-one/one-to-many binding  ","version":"Next","tagName":"h2"},{"title":"Access Database​","type":1,"pageTitle":"Wiki WPF","url":"/docs/wiki/wiki-wpf#access-database","content":" DAO or Repository  Entity DB Context  ","version":"Next","tagName":"h2"},{"title":"ReactiveUI​","type":1,"pageTitle":"Wiki WPF","url":"/docs/wiki/wiki-wpf#reactiveui","content":" To property - pasoft-share/ReactiveUI  One of the core features of ReactiveUI is to be able to convert properties to Observables, via WhenAny , and to convert Observables into Properties, via a method called ToProperty . These properties are called Output Properties in ReactiveUI, and they are a huge part of using the framework effectively. ","version":"Next","tagName":"h2"},{"title":"Wiki Unicode","type":0,"sectionRef":"#","url":"/docs/wiki/wiki-unicode","content":"","keywords":"Wiki UTF8","version":"Next"},{"title":"FAQ​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#faq","content":" ","version":"Next","tagName":"h2"},{"title":"How a character is displayed on the screen?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#how-a-character-is-displayed-on-the-screen","content":" software maps each character to its glyph(a grid of pixels), draw these pixels onto the screen.  ","version":"Next","tagName":"h3"},{"title":"How to find out whether the file uses UTF-8 or ASCII or other encoding schemas?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#how-to-find-out-whether-the-file-uses-utf-8-or-ascii-or-other-encoding-schemas","content":" It's not always foolproof because there is no universal mandate or requirement that all files must specify their encoding. But it's a good practice to add BOM(Byte Order Mark) at the beginning of a UTF-8 encoded file.  ","version":"Next","tagName":"h3"},{"title":"Can I set UTF-16 as locale in Linux?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#can-i-set-utf-16-as-locale-in-linux","content":" No, you cannot. Linux use UTF-8 encoding which is compatible with ASCII.  ","version":"Next","tagName":"h3"},{"title":"What happens when printing a UTF-16 file in Linux?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#what-happens-when-printing-a-utf-16-file-in-linux","content":" # &gt;&gt;&gt; '€'.encode(&quot;utf16&quot;) -&gt; b'\\xff\\xfe\\xac ' $ echo -n -e \\\\xff\\\\xfe\\\\xac\\\\x20 &gt; a.txt $ hexdump -C a.txt 00000000 ff fe ac 20 |... | 00000004 $ file a.txt a.txt: Unicode text, UTF-16, little-endian text, with no line terminators $ cat a.txt ��� $ iconv -f UTF-16LE -t UTF-8 a.txt €   ","version":"Next","tagName":"h3"},{"title":"How can I check a UTF-8 file has a BOM?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#how-can-i-check-a-utf-8-file-has-a-bom","content":" Create a file without BOM,  &gt;&gt;&gt; f.flush() &gt;&gt;&gt; b'\\xe2\\x82\\xac'.decode() '€' &gt;&gt;&gt; '€'.encode() b'\\xe2\\x82\\xac' &gt;&gt;&gt; bom=b&quot;\\xef\\xbb\\xbf&quot; &gt;&gt;&gt; f=open(&quot;a.txt&quot;, &quot;wb+&quot;) &gt;&gt;&gt; f.write(b'\\xe2\\x82\\xac') 3 &gt;&gt;&gt; f.flush()   $ file a.txt a.txt: Unicode text, UTF-8 text, with no line terminators   Create a BOM adhere file,  &gt;&gt;&gt; f.seek(0) 0 &gt;&gt;&gt; f.truncate(0) 0 &gt;&gt;&gt; f.write(b'\\xef\\xbb\\xbf\\xe2\\x82\\xac') 6 &gt;&gt;&gt; f.flush()   $ file a.txt a.txt: Unicode text, UTF-8 (with BOM) text, with no line terminators $ hexdump -C a.txt 00000000 ef bb bf e2 82 ac |......| 00000006   ","version":"Next","tagName":"h3"},{"title":"Why we can copy and paste the unicode characters into a shell?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#why-we-can-copy-and-paste-the-unicode-characters-into-a-shell","content":" When we do copying on the screen, we're copying the character's UTF8 encoded bytes which is in the memory, not the code point.  # b'\\xe2\\x82\\xac'.decode() -&gt; '€' $ echo -e \\\\xe2\\\\x82\\\\xac | xclip -selection clipboard   Then you can use your mouse right click to copy to the shell and you will see €.  ","version":"Next","tagName":"h3"},{"title":"How a string is stored in memory when Python running?​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#how-a-string-is-stored-in-memory-when-python-running","content":" ","version":"Next","tagName":"h3"},{"title":"Unicode in JSON​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#unicode-in-json","content":" JSON(natively a text format) support the unicode character to be escaped or not. When being escaped, the character will be replaced with the unicode code point, then which will be represented in 6 or 8 ascii characters occupying 6 or 8 bytes. When not being escaped, the character will be represented as just one unicode character as itself occupying 1 to 4 bytes if using UTF-8.  Escaping will cost more storage but will be compatible in ASCII-only environments, as escaping force all characters to be ASCII characters.  Case 1: Characters escaped,  &gt;&gt;&gt; import json &gt;&gt;&gt; b=b'{&quot;text&quot;: &quot;\\u4f60\\u597d&quot;}' &gt;&gt;&gt; json.loads(b) {'text': '你好'}jsn &gt;&gt;&gt; json.dumps(json.loads(b)) '{&quot;text&quot;: &quot;\\\\u4f60\\\\u597d&quot;}' &gt;&gt;&gt; json.dumps(json.loads(b), ensure_ascii=False) '{&quot;text&quot;: &quot;你好&quot;}'   &gt;&gt;&gt; f=open(&quot;a.txt&quot;, &quot;w+&quot;) &gt;&gt;&gt; f.write(json.dumps(json.loads(b))) 24 &gt;&gt;&gt; f.flush()   $ cat a.txt {&quot;text&quot;: &quot;\\u4f60\\u597d&quot;}# $ hexdump -C a.txt 00000000 7b 22 74 65 78 74 22 3a 20 22 5c 75 34 66 36 30 |{&quot;text&quot;: &quot;\\u4f60| 00000010 5c 75 35 39 37 64 22 7d |\\u597d&quot;}| 00000018   Case 2: Characters not escaped,  &gt;&gt;&gt; f.seek(0) 0 &gt;&gt;&gt; f.truncate(0) 0 &gt;&gt;&gt; f.write(json.dumps(json.loads(b), ensure_ascii=False)) 14 &gt;&gt;&gt; f.flush()   $ cat a.txt {&quot;text&quot;: &quot;你好&quot;}# $ hexdump -C a.txt 00000000 7b 22 74 65 78 74 22 3a 20 22 e4 bd a0 e5 a5 bd |{&quot;text&quot;: &quot;......| 00000010 22 7d |&quot;}| 00000012   ","version":"Next","tagName":"h2"},{"title":"Base64​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#base64","content":" Base64 is binary-to-text encoding schema which make bytes data to be represented in ASCII characters to be human readable.  ","version":"Next","tagName":"h2"},{"title":"Python​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#python","content":" ","version":"Next","tagName":"h2"},{"title":"C application​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#c-application","content":" Let's have a look at how the Unicode is represented in a C executable file.  char  cat &gt; unicode.c &lt;&lt; EOL #include &lt;stdio.h&gt; int main(){ printf(&quot;Hello, World 你好🤨!\\n&quot;); return 0; } EOL   gcc unicode.c -o unicode.out   root@112b172acfff:/workspaces/liviaerxin.github.io/# hexdump -C unicode.out | grep -A3 &quot;Hello, World&quot; 00002000 01 00 02 00 48 65 6c 6c 6f 2c 20 57 6f 72 6c 64 |....Hello, World| 00002010 20 e4 bd a0 e5 a5 bd f0 9f a4 a8 21 00 00 00 00 | ..........!....| 00002020 01 1b 03 3b 34 00 00 00 05 00 00 00 00 f0 ff ff |...;4...........| 00002030 68 00 00 00 20 f0 ff ff 90 00 00 00 30 f0 ff ff |h... .......0...|   View each character's UTF8 encoding respectively,  # utf-8 encoding bytes &gt;&gt;&gt; &quot;你&quot;.encode() b'\\xe4\\xbd\\xa0' &gt;&gt;&gt; &quot;好&quot;.encode() b'\\xe5\\xa5\\xbd' &gt;&gt;&gt; &quot;🤨&quot;.encode() b'\\xf0\\x9f\\xa4\\xa8'   So char in the C output file stores the UTF8 encoding bytes, not the code points.  wide char,  cat &gt; unicode.c &lt;&lt; EOL #include &lt;stdio.h&gt; #include &lt;wchar.h&gt; #include &lt;locale.h&gt; int main(int argc, char *argv[]) { setlocale(LC_ALL, &quot;C.UTF-8&quot;); wchar_t* msg = L&quot;Hello, World 你好🤨!&quot;; printf(&quot;%ls\\n&quot;, msg); return 0; } EOL   root@112b172acfff:/workspaces/liviaerxin.github.io/# hexdump -C unicode.out 00002000 01 00 02 00 00 00 00 00 43 2e 55 54 46 2d 38 00 |........C.UTF-8.| 00002010 48 00 00 00 65 00 00 00 6c 00 00 00 6c 00 00 00 |H...e...l...l...| 00002020 6f 00 00 00 2c 00 00 00 20 00 00 00 57 00 00 00 |o...,... ...W...| 00002030 6f 00 00 00 72 00 00 00 6c 00 00 00 64 00 00 00 |o...r...l...d...| 00002040 20 00 00 00 60 4f 00 00 7d 59 00 00 28 f9 01 00 | ...`O..}Y..(...|   # unicode code point &gt;&gt;&gt; hex(ord(&quot;H&quot;)) '0x48' &gt;&gt;&gt; hex(ord(&quot;你&quot;)) '0x4f60' &gt;&gt;&gt; hex(ord(&quot;好&quot;)) '0x597d' &gt;&gt;&gt; hex(ord(&quot;🤨&quot;)) '0x1f928'   wchar in the C output file stores the code point(also in little endian), not the UTF8 encoding bytes.  In conclusion, char and wchar lead different encoding in C.  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Wiki Unicode","url":"/docs/wiki/wiki-unicode#resources","content":" The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) – Joel on Software  Pragmatic Unicode | Ned Batchelder ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}